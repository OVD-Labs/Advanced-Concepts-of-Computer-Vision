{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fa337daf",
      "metadata": {
        "id": "fa337daf"
      },
      "source": [
        "**<center><h1>Advanced Computer Vision for Artificial Intelligence Assignment</h1></center>**\n",
        "**<center><h2>Part 3 - Object Detection Component (RetinaNet)</h2></center>**\n",
        "**<center><h3>Matthias Bartolo, Jerome Agius, Isaac Muscat</h3></center>**\n",
        "\n",
        "RetinaNet is a single-stage object detection model that uses a focal loss function to deal with class imbalance during training. Focal loss applies a modulation term to the cross-entropy loss to focus learning on hard negative examples. Retina-Net is a single unified network consisting of a major back network and two task-specific subnetworks. The backbone is accountable for computing a convolutional characteristic map over the whole input image and is an off-the-self convolutional network. The first subnet performs convolutional object classification on the backbone output; the second subnet performs convolutional bounds regression. These two subnets have a simple design that the authors propose specifically for one-step dense detection\n",
        "\n",
        "## This Notebook is Structure in the following manner:\n",
        "\n",
        "- Required libraries\n",
        "- Downloading the Roboflow datset\n",
        "- Training the RetineNet model\n",
        "- Testing the RetinaNet model on the Testing subset\n",
        "- Testing Results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "992c29f1",
      "metadata": {},
      "source": [
        "**<h3>Libraries required to utilise GPU if one is available.</h3>**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "262dfb56",
      "metadata": {},
      "source": [
        "<font size=\"0.1\">\n",
        "\n",
        "```python\n",
        "conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0\n",
        "# Anything above 2.10 is not supported on the GPU on Windows Native\n",
        "python -m pip install \"tensorflow<2.11\"\n",
        "# Verify the installation:\n",
        "python -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n",
        "```\n",
        "</font>\n",
        "\n",
        "Reference Link: https://www.tensorflow.org/install/pip#windows-native"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77a15058",
      "metadata": {},
      "source": [
        "**<h3>Required installs.</h3>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "08583b9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "uT3IRv9s47rL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT3IRv9s47rL",
        "outputId": "b4d856fc-27ff-46f3-9b7b-278af71987ce"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "16da97f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install keras==2.15.0\n",
        "# !pip install gast==0.2.2\n",
        "# !pip install protobuf==3.17.3\n",
        "# !pip install tensorboard==2.10.0\n",
        "# !pip install tensorflow-estimator==2.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3a4b924f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-18T21:20:22.119499Z",
          "start_time": "2023-01-18T21:20:22.111498Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a4b924f",
        "outputId": "204f36d7-f923-45f0-8b71-1e330618a9e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Downloads\\RetinaNet\\keras-retinanet\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'keras-retinanet' already exists and is not an empty directory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running build_ext\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\anaconda3\\envs\\retinanet\\lib\\site-packages\\setuptools\\__init__.py:80: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Requirements should be satisfied by a PEP 517 installer.\n",
            "        If you are using pip, you can try `pip install --use-pep517`.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  dist.fetch_build_eggs(dist.setup_requires)\n"
          ]
        }
      ],
      "source": [
        "# Download RetinaNet\n",
        "!git clone https://github.com/fizyr/keras-retinanet.git\n",
        "%cd keras-retinanet/\n",
        "!python setup.py build_ext --inplace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0b951dc3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b951dc3",
        "outputId": "999324bc-c426-4ae7-b5ed-8dd9d1f70be0"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "37225671",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37225671",
        "outputId": "4e1c7905-b092-40c6-a8cc-92d26bcc2864"
      },
      "outputs": [],
      "source": [
        "# !pip install keras==2.6.0\n",
        "# !pip install tensorflow==2.10.1\n",
        "# !pip install keras-resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1ab29cf9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install tensorflow-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35956191",
      "metadata": {},
      "source": [
        "**<h3>Replacing RetinaNet files with updated variants.</h3>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f3802382",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of ../filesToReplace/modified_evaluate.py copied and replaced in keras_retinanet/bin/evaluate.py successfully.\n",
            "Contents of ../filesToReplace/modified_eval.py copied and replaced in keras_retinanet/utils/eval.py successfully.\n"
          ]
        }
      ],
      "source": [
        "evaluation_path = \"../filesToReplace/modified_evaluate.py\"\n",
        "evaluation_copyTo_path = \"keras_retinanet/bin/evaluate.py\"\n",
        "\n",
        "eval_path = \"../filesToReplace/modified_eval.py\"\n",
        "eval_copyTo_path = \"keras_retinanet/utils/eval.py\"\n",
        "\n",
        "visualise_path = \"../filesToReplace/visualization.py\"\n",
        "visualise_copyTo_path= \"keras_retinanet/utils/visualization.py\"\n",
        "\n",
        "def copy_and_replace(file_a, file_b):\n",
        "    try:\n",
        "        with open(file_a, 'r') as file_a_contents:\n",
        "            data = file_a_contents.read()\n",
        "\n",
        "        with open(file_b, 'w') as file_b_contents:\n",
        "            file_b_contents.write(data)\n",
        "\n",
        "        print(f\"Contents of {file_a} copied and replaced in {file_b} successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "copy_and_replace(evaluation_path, evaluation_copyTo_path)\n",
        "copy_and_replace(eval_path, eval_copyTo_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc7eeaaf",
      "metadata": {},
      "source": [
        "**<h3>Using GPU if one is available.</h3>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fdc73ca3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU is available\n",
            "Device name: /physical_device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.__version__\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(\"GPU is available\")\n",
        "    for gpu in gpus:\n",
        "        print(\"Device name:\", gpu.name)\n",
        "else:\n",
        "    print(\"No GPU available, using CPU instead\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "03a62e85",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WinError 2] The system cannot find the file specified: 'keras-retinanet/'\n",
            "c:\\Users\\User\\Downloads\\RetinaNet\\keras-retinanet\n"
          ]
        }
      ],
      "source": [
        "%cd keras-retinanet/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4af15b5c",
      "metadata": {},
      "source": [
        "**<h3>Required libraries.</h3>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b753a3e3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-18T21:20:18.830458Z",
          "start_time": "2023-01-18T21:20:15.562459Z"
        },
        "id": "b753a3e3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import cv2\n",
        "import time\n",
        "import keras\n",
        "import pandas\n",
        "import shutil\n",
        "import urllib\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from keras_retinanet import models\n",
        "from keras_retinanet.utils.colors import label_color\n",
        "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
        "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "Fv9H8XKc6QOs",
      "metadata": {
        "id": "Fv9H8XKc6QOs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Downloads\\RetinaNet\\keras-retinanet\n"
          ]
        }
      ],
      "source": [
        "# Retrieving the current working directory\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d90bfc4",
      "metadata": {},
      "source": [
        "**<h3>Downloading the Roboflow dataset.</h3>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c827f118",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-15T17:39:27.244582Z",
          "start_time": "2023-01-15T17:37:01.357821Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c827f118",
        "outputId": "329a9581-7ff5-43ad-c9f3-7a4851f871f6",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading Dataset Version Zip in Pizza-Object-Detector-7 to retinanet:: 100%|██████████| 583050/583050 [01:46<00:00, 5449.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Dataset Version Zip to Pizza-Object-Detector-7 in retinanet:: 100%|██████████| 3115/3115 [00:01<00:00, 2835.33it/s]\n"
          ]
        }
      ],
      "source": [
        "# Download Dataset\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"y2FfMokLpTlGQIlMvhja\")\n",
        "project = rf.workspace(\"advanced-computer-vision-assignment\").project(\"pizza-object-detector\")\n",
        "dataset = project.version(7).download(\"retinanet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "71f83b6a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-16T12:47:42.992052Z",
          "start_time": "2023-01-16T12:47:42.976020Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71f83b6a",
        "outputId": "77199d86-1ba4-4f55-e649-1318138e32b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of ../filesToReplace/_annotations.csv copied and replaced in c:\\Users\\User\\Downloads\\RetinaNet\\keras-retinanet\\Pizza-Object-Detector-7\\train\\_annotations.csv successfully.\n",
            "        1 file(s) copied.\n",
            "['Arugula', '0']\n",
            "['Bacon', '1']\n",
            "['Basil', '2']\n",
            "['Broccoli', '3']\n",
            "['Cheese', '4']\n",
            "['Chicken', '5']\n",
            "['Corn', '6']\n",
            "['Ham', '7']\n",
            "['Mushroom', '8']\n",
            "['Olives', '9']\n",
            "['Onion', '10']\n",
            "['Pepperoni', '11']\n",
            "['Peppers', '12']\n",
            "['Pineapple', '13']\n",
            "['Pizza', '14']\n",
            "['Tomatoes', '15']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        1 file(s) copied.\n",
            "c:\\Users\\User\\Downloads\\RetinaNet\\keras-retinanet\n"
          ]
        }
      ],
      "source": [
        "#Annotations and Classes csv path\n",
        "annotations_path = os.path.join(HOME, os.path.join('Pizza-Object-Detector-7','train','_annotations.csv'))\n",
        "classes_path     = os.path.join(HOME, os.path.join('_classes.csv'))\n",
        "\n",
        "# Replace the annotations csv file with the new one\n",
        "evaluation_path = \"../filesToReplace/_annotations.csv\"\n",
        "evaluation_copyTo_path = os.path.join(HOME, os.path.join('Pizza-Object-Detector-7','train','_annotations.csv'))\n",
        "copy_and_replace(evaluation_path, evaluation_copyTo_path)\n",
        "\n",
        "!copy {annotations_path} {os.path.join(HOME, os.path.join('Pizza-Object-Detector-7','train','_annotations2.csv'))}\n",
        "shutil.copy(annotations_path, os.path.join(HOME, os.path.join('Pizza-Object-Detector-7','train','_annotations2.csv')))\n",
        "annotations_path_temp = os.path.join(HOME, os.path.join('Pizza-Object-Detector-7','train','_annotations2.csv'))\n",
        "\n",
        "#Create Classes csv\n",
        "with open(classes_path, 'w', newline='') as f:\n",
        "    f.write('cranes,0')\n",
        "\n",
        "classes = [\n",
        "    'Arugula', 'Bacon', 'Basil', 'Broccoli', 'Cheese', 'Chicken', 'Corn', 'Ham',\n",
        "    'Mushroom', 'Olives', 'Onion', 'Pepperoni', 'Peppers', 'Pineapple', 'Pizza', 'Tomatoes'\n",
        "]\n",
        "\n",
        "with open(classes_path, 'w', newline='') as f:\n",
        "    csv_writer = csv.writer(f)\n",
        "    for idx, class_name in enumerate(classes):\n",
        "        csv_writer.writerow([class_name, idx])\n",
        "\n",
        "with open(classes_path, newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        # Read each row in the CSV file\n",
        "        for row in reader:\n",
        "            print(row)\n",
        "\n",
        "#Remove Empty Lines in Annotations csv\n",
        "!copy {annotations_path} {annotations_path_temp}\n",
        "print(os.getcwd())\n",
        "with open(annotations_path_temp, newline='') as in_file:\n",
        "    with open(annotations_path, 'w', newline='') as out_file:\n",
        "        writer = csv.writer(out_file)\n",
        "        for row in csv.reader(in_file):\n",
        "            if row:\n",
        "                writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ad628e0",
      "metadata": {},
      "source": [
        "**<h3>Downloading the pre-trained weights.</h3>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d9822663",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-15T17:29:34.586703Z",
          "start_time": "2023-01-15T17:28:53.033348Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9822663",
        "outputId": "8a04c794-421b-4da4-d439-0c54591d774c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('snapshots\\\\pretrained-model.h5', <http.client.HTTPMessage at 0x1af3b0fb850>)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download pre-trained weights\n",
        "url = 'https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5'\n",
        "pretrained_model_path = os.path.join('snapshots','pretrained-model.h5')\n",
        "\n",
        "urllib.request.urlretrieve(url, pretrained_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e0abb88",
      "metadata": {
        "id": "9e0abb88"
      },
      "source": [
        "**<h3>Training the RetinaNet model.</h3>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5f086ec9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-15T17:54:26.137043Z",
          "start_time": "2023-01-15T17:54:26.120043Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f086ec9",
        "outputId": "73495f4c-b0bb-4a68-f60d-3231f4742aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python keras_retinanet/bin/train.py --freeze-backbone --random-transform --weights snapshots\\pretrained-model.h5 --batch-size 8 --steps 500 --epochs 50 csv c:\\Users\\User\\Downloads\\RetinaNet\\keras-retinanet\\Pizza-Object-Detector-7\\train\\_annotations.csv c:\\Users\\User\\Downloads\\RetinaNet\\keras-retinanet\\_classes.csv\n"
          ]
        }
      ],
      "source": [
        "#ALTERNATIVE: Run command in terminal instead\n",
        "print(f'python keras_retinanet/bin/train.py --freeze-backbone --random-transform --weights {pretrained_model_path} --batch-size 8 --steps 500 --epochs 50 csv {annotations_path} {classes_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b95499ff",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-15T17:52:30.732792Z",
          "start_time": "2023-01-15T17:51:06.541926Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b95499ff",
        "outputId": "1cdd5859-c507-4845-81ec-66011e8d46ee",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating model, this may take a second...\n",
            "Model: \"retinanet\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None, None,  0           []                               \n",
            "                                 3)]                                                              \n",
            "                                                                                                  \n",
            " conv1 (Conv2D)                 (None, None, None,   9408        ['input_1[0][0]']                \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " bn_conv1 (BatchNormalization)  (None, None, None,   256         ['conv1[0][0]']                  \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)        (None, None, None,   0           ['bn_conv1[0][0]']               \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " pool1 (MaxPooling2D)           (None, None, None,   0           ['conv1_relu[0][0]']             \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " res2a_branch2a (Conv2D)        (None, None, None,   4096        ['pool1[0][0]']                  \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " bn2a_branch2a (BatchNormalizat  (None, None, None,   256        ['res2a_branch2a[0][0]']         \n",
            " ion)                           64)                                                               \n",
            "                                                                                                  \n",
            " res2a_branch2a_relu (Activatio  (None, None, None,   0          ['bn2a_branch2a[0][0]']          \n",
            " n)                             64)                                                               \n",
            "                                                                                                  \n",
            " padding2a_branch2b (ZeroPaddin  (None, None, None,   0          ['res2a_branch2a_relu[0][0]']    \n",
            " g2D)                           64)                                                               \n",
            "                                                                                                  \n",
            " res2a_branch2b (Conv2D)        (None, None, None,   36864       ['padding2a_branch2b[0][0]']     \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " bn2a_branch2b (BatchNormalizat  (None, None, None,   256        ['res2a_branch2b[0][0]']         \n",
            " ion)                           64)                                                               \n",
            "                                                                                                  \n",
            " res2a_branch2b_relu (Activatio  (None, None, None,   0          ['bn2a_branch2b[0][0]']          \n",
            " n)                             64)                                                               \n",
            "                                                                                                  \n",
            " res2a_branch2c (Conv2D)        (None, None, None,   16384       ['res2a_branch2b_relu[0][0]']    \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " res2a_branch1 (Conv2D)         (None, None, None,   16384       ['pool1[0][0]']                  \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " bn2a_branch2c (BatchNormalizat  (None, None, None,   1024       ['res2a_branch2c[0][0]']         \n",
            " ion)                           256)                                                              \n",
            "                                                                                                  \n",
            " bn2a_branch1 (BatchNormalizati  (None, None, None,   1024       ['res2a_branch1[0][0]']          \n",
            " on)                            256)                                                              \n",
            "                                                                                                  \n",
            " res2a (Add)                    (None, None, None,   0           ['bn2a_branch2c[0][0]',          \n",
            "                                256)                              'bn2a_branch1[0][0]']           \n",
            "                                                                                                  \n",
            " res2a_relu (Activation)        (None, None, None,   0           ['res2a[0][0]']                  \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " res2b_branch2a (Conv2D)        (None, None, None,   16384       ['res2a_relu[0][0]']             \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " bn2b_branch2a (BatchNormalizat  (None, None, None,   256        ['res2b_branch2a[0][0]']         \n",
            " ion)                           64)                                                               \n",
            "                                                                                                  \n",
            " res2b_branch2a_relu (Activatio  (None, None, None,   0          ['bn2b_branch2a[0][0]']          \n",
            " n)                             64)                                                               \n",
            "                                                                                                  \n",
            " padding2b_branch2b (ZeroPaddin  (None, None, None,   0          ['res2b_branch2a_relu[0][0]']    \n",
            " g2D)                           64)                                                               \n",
            "                                                                                                  \n",
            " res2b_branch2b (Conv2D)        (None, None, None,   36864       ['padding2b_branch2b[0][0]']     \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " bn2b_branch2b (BatchNormalizat  (None, None, None,   256        ['res2b_branch2b[0][0]']         \n",
            " ion)                           64)                                                               \n",
            "                                                                                                  \n",
            " res2b_branch2b_relu (Activatio  (None, None, None,   0          ['bn2b_branch2b[0][0]']          \n",
            " n)                             64)                                                               \n",
            "                                                                                                  \n",
            " res2b_branch2c (Conv2D)        (None, None, None,   16384       ['res2b_branch2b_relu[0][0]']    \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " bn2b_branch2c (BatchNormalizat  (None, None, None,   1024       ['res2b_branch2c[0][0]']         \n",
            " ion)                           256)                                                              \n",
            "                                                                                                  \n",
            " res2b (Add)                    (None, None, None,   0           ['bn2b_branch2c[0][0]',          \n",
            "                                256)                              'res2a_relu[0][0]']             \n",
            "                                                                                                  \n",
            " res2b_relu (Activation)        (None, None, None,   0           ['res2b[0][0]']                  \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " res2c_branch2a (Conv2D)        (None, None, None,   16384       ['res2b_relu[0][0]']             \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " bn2c_branch2a (BatchNormalizat  (None, None, None,   256        ['res2c_branch2a[0][0]']         \n",
            " ion)                           64)                                                               \n",
            "                                                                                                  \n",
            " res2c_branch2a_relu (Activatio  (None, None, None,   0          ['bn2c_branch2a[0][0]']          \n",
            " n)                             64)                                                               \n",
            "                                                                                                  \n",
            " padding2c_branch2b (ZeroPaddin  (None, None, None,   0          ['res2c_branch2a_relu[0][0]']    \n",
            " g2D)                           64)                                                               \n",
            "                                                                                                  \n",
            " res2c_branch2b (Conv2D)        (None, None, None,   36864       ['padding2c_branch2b[0][0]']     \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " bn2c_branch2b (BatchNormalizat  (None, None, None,   256        ['res2c_branch2b[0][0]']         \n",
            " ion)                           64)                                                               \n",
            "                                                                                                  \n",
            " res2c_branch2b_relu (Activatio  (None, None, None,   0          ['bn2c_branch2b[0][0]']          \n",
            " n)                             64)                                                               \n",
            "                                                                                                  \n",
            " res2c_branch2c (Conv2D)        (None, None, None,   16384       ['res2c_branch2b_relu[0][0]']    \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " bn2c_branch2c (BatchNormalizat  (None, None, None,   1024       ['res2c_branch2c[0][0]']         \n",
            " ion)                           256)                                                              \n",
            "                                                                                                  \n",
            " res2c (Add)                    (None, None, None,   0           ['bn2c_branch2c[0][0]',          \n",
            "                                256)                              'res2b_relu[0][0]']             \n",
            "                                                                                                  \n",
            " res2c_relu (Activation)        (None, None, None,   0           ['res2c[0][0]']                  \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " res3a_branch2a (Conv2D)        (None, None, None,   32768       ['res2c_relu[0][0]']             \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " bn3a_branch2a (BatchNormalizat  (None, None, None,   512        ['res3a_branch2a[0][0]']         \n",
            " ion)                           128)                                                              \n",
            "                                                                                                  \n",
            " res3a_branch2a_relu (Activatio  (None, None, None,   0          ['bn3a_branch2a[0][0]']          \n",
            " n)                             128)                                                              \n",
            "                                                                                                  \n",
            " padding3a_branch2b (ZeroPaddin  (None, None, None,   0          ['res3a_branch2a_relu[0][0]']    \n",
            " g2D)                           128)                                                              \n",
            "                                                                                                  \n",
            " res3a_branch2b (Conv2D)        (None, None, None,   147456      ['padding3a_branch2b[0][0]']     \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " bn3a_branch2b (BatchNormalizat  (None, None, None,   512        ['res3a_branch2b[0][0]']         \n",
            " ion)                           128)                                                              \n",
            "                                                                                                  \n",
            " res3a_branch2b_relu (Activatio  (None, None, None,   0          ['bn3a_branch2b[0][0]']          \n",
            " n)                             128)                                                              \n",
            "                                                                                                  \n",
            " res3a_branch2c (Conv2D)        (None, None, None,   65536       ['res3a_branch2b_relu[0][0]']    \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " res3a_branch1 (Conv2D)         (None, None, None,   131072      ['res2c_relu[0][0]']             \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " bn3a_branch2c (BatchNormalizat  (None, None, None,   2048       ['res3a_branch2c[0][0]']         \n",
            " ion)                           512)                                                              \n",
            "                                                                                                  \n",
            " bn3a_branch1 (BatchNormalizati  (None, None, None,   2048       ['res3a_branch1[0][0]']          \n",
            " on)                            512)                                                              \n",
            "                                                                                                  \n",
            " res3a (Add)                    (None, None, None,   0           ['bn3a_branch2c[0][0]',          \n",
            "                                512)                              'bn3a_branch1[0][0]']           \n",
            "                                                                                                  \n",
            " res3a_relu (Activation)        (None, None, None,   0           ['res3a[0][0]']                  \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " res3b_branch2a (Conv2D)        (None, None, None,   65536       ['res3a_relu[0][0]']             \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " bn3b_branch2a (BatchNormalizat  (None, None, None,   512        ['res3b_branch2a[0][0]']         \n",
            " ion)                           128)                                                              \n",
            "                                                                                                  \n",
            " res3b_branch2a_relu (Activatio  (None, None, None,   0          ['bn3b_branch2a[0][0]']          \n",
            " n)                             128)                                                              \n",
            "                                                                                                  \n",
            " padding3b_branch2b (ZeroPaddin  (None, None, None,   0          ['res3b_branch2a_relu[0][0]']    \n",
            " g2D)                           128)                                                              \n",
            "                                                                                                  \n",
            " res3b_branch2b (Conv2D)        (None, None, None,   147456      ['padding3b_branch2b[0][0]']     \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " bn3b_branch2b (BatchNormalizat  (None, None, None,   512        ['res3b_branch2b[0][0]']         \n",
            " ion)                           128)                                                              \n",
            "                                                                                                  \n",
            " res3b_branch2b_relu (Activatio  (None, None, None,   0          ['bn3b_branch2b[0][0]']          \n",
            " n)                             128)                                                              \n",
            "                                                                                                  \n",
            " res3b_branch2c (Conv2D)        (None, None, None,   65536       ['res3b_branch2b_relu[0][0]']    \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " bn3b_branch2c (BatchNormalizat  (None, None, None,   2048       ['res3b_branch2c[0][0]']         \n",
            " ion)                           512)                                                              \n",
            "                                                                                                  \n",
            " res3b (Add)                    (None, None, None,   0           ['bn3b_branch2c[0][0]',          \n",
            "                                512)                              'res3a_relu[0][0]']             \n",
            "                                                                                                  \n",
            " res3b_relu (Activation)        (None, None, None,   0           ['res3b[0][0]']                  \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " res3c_branch2a (Conv2D)        (None, None, None,   65536       ['res3b_relu[0][0]']             \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " bn3c_branch2a (BatchNormalizat  (None, None, None,   512        ['res3c_branch2a[0][0]']         \n",
            " ion)                           128)                                                              \n",
            "                                                                                                  \n",
            " res3c_branch2a_relu (Activatio  (None, None, None,   0          ['bn3c_branch2a[0][0]']          \n",
            " n)                             128)                                                              \n",
            "                                                                                                  \n",
            " padding3c_branch2b (ZeroPaddin  (None, None, None,   0          ['res3c_branch2a_relu[0][0]']    \n",
            " g2D)                           128)                                                              \n",
            "                                                                                                  \n",
            " res3c_branch2b (Conv2D)        (None, None, None,   147456      ['padding3c_branch2b[0][0]']     \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " bn3c_branch2b (BatchNormalizat  (None, None, None,   512        ['res3c_branch2b[0][0]']         \n",
            " ion)                           128)                                                              \n",
            "                                                                                                  \n",
            " res3c_branch2b_relu (Activatio  (None, None, None,   0          ['bn3c_branch2b[0][0]']          \n",
            " n)                             128)                                                              \n",
            "                                                                                                  \n",
            " res3c_branch2c (Conv2D)        (None, None, None,   65536       ['res3c_branch2b_relu[0][0]']    \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " bn3c_branch2c (BatchNormalizat  (None, None, None,   2048       ['res3c_branch2c[0][0]']         \n",
            " ion)                           512)                                                              \n",
            "                                                                                                  \n",
            " res3c (Add)                    (None, None, None,   0           ['bn3c_branch2c[0][0]',          \n",
            "                                512)                              'res3b_relu[0][0]']             \n",
            "                                                                                                  \n",
            " res3c_relu (Activation)        (None, None, None,   0           ['res3c[0][0]']                  \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " res3d_branch2a (Conv2D)        (None, None, None,   65536       ['res3c_relu[0][0]']             \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " bn3d_branch2a (BatchNormalizat  (None, None, None,   512        ['res3d_branch2a[0][0]']         \n",
            " ion)                           128)                                                              \n",
            "                                                                                                  \n",
            " res3d_branch2a_relu (Activatio  (None, None, None,   0          ['bn3d_branch2a[0][0]']          \n",
            " n)                             128)                                                              \n",
            "                                                                                                  \n",
            " padding3d_branch2b (ZeroPaddin  (None, None, None,   0          ['res3d_branch2a_relu[0][0]']    \n",
            " g2D)                           128)                                                              \n",
            "                                                                                                  \n",
            " res3d_branch2b (Conv2D)        (None, None, None,   147456      ['padding3d_branch2b[0][0]']     \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " bn3d_branch2b (BatchNormalizat  (None, None, None,   512        ['res3d_branch2b[0][0]']         \n",
            " ion)                           128)                                                              \n",
            "                                                                                                  \n",
            " res3d_branch2b_relu (Activatio  (None, None, None,   0          ['bn3d_branch2b[0][0]']          \n",
            " n)                             128)                                                              \n",
            "                                                                                                  \n",
            " res3d_branch2c (Conv2D)        (None, None, None,   65536       ['res3d_branch2b_relu[0][0]']    \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " bn3d_branch2c (BatchNormalizat  (None, None, None,   2048       ['res3d_branch2c[0][0]']         \n",
            " ion)                           512)                                                              \n",
            "                                                                                                  \n",
            " res3d (Add)                    (None, None, None,   0           ['bn3d_branch2c[0][0]',          \n",
            "                                512)                              'res3c_relu[0][0]']             \n",
            "                                                                                                  \n",
            " res3d_relu (Activation)        (None, None, None,   0           ['res3d[0][0]']                  \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " res4a_branch2a (Conv2D)        (None, None, None,   131072      ['res3d_relu[0][0]']             \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " bn4a_branch2a (BatchNormalizat  (None, None, None,   1024       ['res4a_branch2a[0][0]']         \n",
            " ion)                           256)                                                              \n",
            "                                                                                                  \n",
            " res4a_branch2a_relu (Activatio  (None, None, None,   0          ['bn4a_branch2a[0][0]']          \n",
            " n)                             256)                                                              \n",
            "                                                                                                  \n",
            " padding4a_branch2b (ZeroPaddin  (None, None, None,   0          ['res4a_branch2a_relu[0][0]']    \n",
            " g2D)                           256)                                                              \n",
            "                                                                                                  \n",
            " res4a_branch2b (Conv2D)        (None, None, None,   589824      ['padding4a_branch2b[0][0]']     \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " bn4a_branch2b (BatchNormalizat  (None, None, None,   1024       ['res4a_branch2b[0][0]']         \n",
            " ion)                           256)                                                              \n",
            "                                                                                                  \n",
            " res4a_branch2b_relu (Activatio  (None, None, None,   0          ['bn4a_branch2b[0][0]']          \n",
            " n)                             256)                                                              \n",
            "                                                                                                  \n",
            " res4a_branch2c (Conv2D)        (None, None, None,   262144      ['res4a_branch2b_relu[0][0]']    \n",
            "                                1024)                                                             \n",
            "                                                                                                  \n",
            " res4a_branch1 (Conv2D)         (None, None, None,   524288      ['res3d_relu[0][0]']             \n",
            "                                1024)                                                             \n",
            "                                                                                                  \n",
            " bn4a_branch2c (BatchNormalizat  (None, None, None,   4096       ['res4a_branch2c[0][0]']         \n",
            " ion)                           1024)                                                             \n",
            "                                                                                                  \n",
            " bn4a_branch1 (BatchNormalizati  (None, None, None,   4096       ['res4a_branch1[0][0]']          \n",
            " on)                            1024)                                                             \n",
            "                                                                                                  \n",
            " res4a (Add)                    (None, None, None,   0           ['bn4a_branch2c[0][0]',          \n",
            "                                1024)                             'bn4a_branch1[0][0]']           \n",
            "                                                                                                  \n",
            " res4a_relu (Activation)        (None, None, None,   0           ['res4a[0][0]']                  \n",
            "                                1024)                                                             \n",
            "                                                                                                  \n",
            " res4b_branch2a (Conv2D)        (None, None, None,   262144      ['res4a_relu[0][0]']             \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " bn4b_branch2a (BatchNormalizat  (None, None, None,   1024       ['res4b_branch2a[0][0]']         \n",
            " ion)                           256)                                                              \n",
            "                                                                                                  \n",
            " res4b_branch2a_relu (Activatio  (None, None, None,   0          ['bn4b_branch2a[0][0]']          \n",
            " n)                             256)                                                              \n",
            "                                                                                                  \n",
            " padding4b_branch2b (ZeroPaddin  (None, None, None,   0          ['res4b_branch2a_relu[0][0]']    \n",
            " g2D)                           256)                                                              \n",
            "                                                                                                  \n",
            " res4b_branch2b (Conv2D)        (None, None, None,   589824      ['padding4b_branch2b[0][0]']     \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " bn4b_branch2b (BatchNormalizat  (None, None, None,   1024       ['res4b_branch2b[0][0]']         \n",
            " ion)                           256)                                                              \n",
            "                                                                                                  \n",
            " res4b_branch2b_relu (Activatio  (None, None, None,   0          ['bn4b_branch2b[0][0]']          \n",
            " n)                             256)                                                              \n",
            "                                                                                                  \n",
            " res4b_branch2c (Conv2D)        (None, None, None,   262144      ['res4b_branch2b_relu[0][0]']    \n",
            "                                1024)                                                             \n",
            "                                                                                                  \n",
            " bn4b_branch2c (BatchNormalizat  (None, None, None,   4096       ['res4b_branch2c[0][0]']         \n",
            " ion)                           1024)                                                             \n",
            "                                                                                                  \n",
            " res4b (Add)                    (None, None, None,   0           ['bn4b_branch2c[0][0]',          \n",
            "                                1024)                             'res4a_relu[0][0]']             \n",
            "                                                                                                  \n",
            " res4b_relu (Activation)        (None, None, None,   0           ['res4b[0][0]']                  \n",
            "                                1024)                                                             \n",
            "                                                                                                  \n",
            " res4c_branch2a (Conv2D)        (None, None, None,   262144      ['res4b_relu[0][0]']             \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " bn4c_branch2a (BatchNormalizat  (None, None, None,   1024       ['res4c_branch2a[0][0]']         \n",
            " ion)                           256)                                                              \n",
            "                                                                                                  \n",
            " res4c_branch2a_relu (Activatio  (None, None, None,   0          ['bn4c_branch2a[0][0]']          \n",
            " n)                             256)                                                              \n",
            "                                                                                                  \n",
            " padding4c_branch2b (ZeroPaddin  (None, None, None,   0          ['res4c_branch2a_relu[0][0]']    \n",
            " g2D)                           256)                                                              \n",
            "                                                                                                  \n",
            " res4c_branch2b (Conv2D)        (None, None, None,   589824      ['padding4c_branch2b[0][0]']     \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " bn4c_branch2b (BatchNormalizat  (None, None, None,   1024       ['res4c_branch2b[0][0]']         \n",
            " ion)                           256)                                                              \n",
            "                                                                                                  \n",
            " res4c_branch2b_relu (Activatio  (None, None, None,   0          ['bn4c_branch2b[0][0]']          \n",
            " n)                             256)                                                              \n",
            "                                                                                                  \n",
            " res4c_branch2c (Conv2D)        (None, None, None,   262144      ['res4c_branch2b_relu[0][0]']    \n",
            "                                1024)                                                             \n",
            "                                                                                                  \n",
            " bn4c_branch2c (BatchNormalizat  (None, None, None,   4096       ['res4c_branch2c[0][0]']         \n",
            " ion)                           1024)                                                             \n",
            "                                                                                                  \n",
            " res4c (Add)                    (None, None, None,   0           ['bn4c_branch2c[0][0]',          \n",
            "                                1024)                             'res4b_relu[0][0]']             \n",
            "                                                                                                  \n",
            " res4c_relu (Activation)        (None, None, None,   0           ['res4c[0][0]']                  \n",
            "                                1024)                                                             \n",
            "                                                                                                  \n",
            " res4d_branch2a (Conv2D)        (None, None, None,   262144      ['res4c_relu[0][0]']             \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " bn4d_branch2a (BatchNormalizat  (None, None, None,   1024       ['res4d_branch2a[0][0]']         \n",
            " ion)                           256)                                                              \n",
            "                                                                                                  \n",
            " res4d_branch2a_relu (Activatio  (None, None, None,   0          ['bn4d_branch2a[0][0]']          \n",
            " n)                             256)                                                              \n",
            "                                                                                                  \n",
            " padding4d_branch2b (ZeroPaddin  (None, None, None,   0          ['res4d_branch2a_relu[0][0]']    \n",
            " g2D)                           256)                                                              \n",
            "                                                                                                  \n",
            " res4d_branch2b (Conv2D)        (None, None, None,   589824      ['padding4d_branch2b[0][0]']     \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " bn4d_branch2b (BatchNormalizat  (None, None, None,   1024       ['res4d_branch2b[0][0]']         \n",
            " ion)                           256)                                                              \n",
            "                                                                                                  \n",
            " res4d_branch2b_relu (Activatio  (None, None, None,   0          ['bn4d_branch2b[0][0]']          \n",
            " n)                             256)                                                              \n",
            "                                                                                                  \n",
            " res4d_branch2c (Conv2D)        (None, None, None,   262144      ['res4d_branch2b_relu[0][0]']    \n",
            "                                1024)                                                             \n",
            "                                                                                                  \n",
            " bn4d_branch2c (BatchNormalizat  (None, None, None,   4096       ['res4d_branch2c[0][0]']         \n",
            " ion)                           1024)                                                             \n",
            "                                                                                                  \n",
            " res4d (Add)                    (None, None, None,   0           ['bn4d_branch2c[0][0]',          \n",
            "                                1024)                             'res4c_relu[0][0]']             \n",
            "                                                                                                  \n",
            " res4d_relu (Activation)        (None, None, None,   0           ['res4d[0][0]']                  \n",
            "                                1024)                                                             \n",
            "                                                                                                  \n",
            " res4e_branch2a (Conv2D)        (None, None, None,   262144      ['res4d_relu[0][0]']             \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " bn4e_branch2a (BatchNormalizat  (None, None, None,   1024       ['res4e_branch2a[0][0]']         \n",
            " ion)                           256)                                                              \n",
            "                                                                                                  \n",
            " res4e_branch2a_relu (Activatio  (None, None, None,   0          ['bn4e_branch2a[0][0]']          \n",
            " n)                             256)                                                              \n",
            "                                                                                                  \n",
            " padding4e_branch2b (ZeroPaddin  (None, None, None,   0          ['res4e_branch2a_relu[0][0]']    \n",
            " g2D)                           256)                                                              \n",
            "                                                                                                  \n",
            " res4e_branch2b (Conv2D)        (None, None, None,   589824      ['padding4e_branch2b[0][0]']     \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " bn4e_branch2b (BatchNormalizat  (None, None, None,   1024       ['res4e_branch2b[0][0]']         \n",
            " ion)                           256)                                                              \n",
            "                                                                                                  \n",
            " res4e_branch2b_relu (Activatio  (None, None, None,   0          ['bn4e_branch2b[0][0]']          \n",
            " n)                             256)                                                              \n",
            "                                                                                                  \n",
            " res4e_branch2c (Conv2D)        (None, None, None,   262144      ['res4e_branch2b_relu[0][0]']    \n",
            "                                1024)                                                             \n",
            "                                                                                                  \n",
            " bn4e_branch2c (BatchNormalizat  (None, None, None,   4096       ['res4e_branch2c[0][0]']         \n",
            " ion)                           1024)                                                             \n",
            "                                                                                                  \n",
            " res4e (Add)                    (None, None, None,   0           ['bn4e_branch2c[0][0]',          \n",
            "                                1024)                             'res4d_relu[0][0]']             \n",
            "                                                                                                  \n",
            " res4e_relu (Activation)        (None, None, None,   0           ['res4e[0][0]']                  \n",
            "                                1024)                                                             \n",
            "                                                                                                  \n",
            " res4f_branch2a (Conv2D)        (None, None, None,   262144      ['res4e_relu[0][0]']             \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " bn4f_branch2a (BatchNormalizat  (None, None, None,   1024       ['res4f_branch2a[0][0]']         \n",
            " ion)                           256)                                                              \n",
            "                                                                                                  \n",
            " res4f_branch2a_relu (Activatio  (None, None, None,   0          ['bn4f_branch2a[0][0]']          \n",
            " n)                             256)                                                              \n",
            "                                                                                                  \n",
            " padding4f_branch2b (ZeroPaddin  (None, None, None,   0          ['res4f_branch2a_relu[0][0]']    \n",
            " g2D)                           256)                                                              \n",
            "                                                                                                  \n",
            " res4f_branch2b (Conv2D)        (None, None, None,   589824      ['padding4f_branch2b[0][0]']     \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " bn4f_branch2b (BatchNormalizat  (None, None, None,   1024       ['res4f_branch2b[0][0]']         \n",
            " ion)                           256)                                                              \n",
            "                                                                                                  \n",
            " res4f_branch2b_relu (Activatio  (None, None, None,   0          ['bn4f_branch2b[0][0]']          \n",
            " n)                             256)                                                              \n",
            "                                                                                                  \n",
            " res4f_branch2c (Conv2D)        (None, None, None,   262144      ['res4f_branch2b_relu[0][0]']    \n",
            "                                1024)                                                             \n",
            "                                                                                                  \n",
            " bn4f_branch2c (BatchNormalizat  (None, None, None,   4096       ['res4f_branch2c[0][0]']         \n",
            " ion)                           1024)                                                             \n",
            "                                                                                                  \n",
            " res4f (Add)                    (None, None, None,   0           ['bn4f_branch2c[0][0]',          \n",
            "                                1024)                             'res4e_relu[0][0]']             \n",
            "                                                                                                  \n",
            " res4f_relu (Activation)        (None, None, None,   0           ['res4f[0][0]']                  \n",
            "                                1024)                                                             \n",
            "                                                                                                  \n",
            " res5a_branch2a (Conv2D)        (None, None, None,   524288      ['res4f_relu[0][0]']             \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " bn5a_branch2a (BatchNormalizat  (None, None, None,   2048       ['res5a_branch2a[0][0]']         \n",
            " ion)                           512)                                                              \n",
            "                                                                                                  \n",
            " res5a_branch2a_relu (Activatio  (None, None, None,   0          ['bn5a_branch2a[0][0]']          \n",
            " n)                             512)                                                              \n",
            "                                                                                                  \n",
            " padding5a_branch2b (ZeroPaddin  (None, None, None,   0          ['res5a_branch2a_relu[0][0]']    \n",
            " g2D)                           512)                                                              \n",
            "                                                                                                  \n",
            " res5a_branch2b (Conv2D)        (None, None, None,   2359296     ['padding5a_branch2b[0][0]']     \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " bn5a_branch2b (BatchNormalizat  (None, None, None,   2048       ['res5a_branch2b[0][0]']         \n",
            " ion)                           512)                                                              \n",
            "                                                                                                  \n",
            " res5a_branch2b_relu (Activatio  (None, None, None,   0          ['bn5a_branch2b[0][0]']          \n",
            " n)                             512)                                                              \n",
            "                                                                                                  \n",
            " res5a_branch2c (Conv2D)        (None, None, None,   1048576     ['res5a_branch2b_relu[0][0]']    \n",
            "                                2048)                                                             \n",
            "                                                                                                  \n",
            " res5a_branch1 (Conv2D)         (None, None, None,   2097152     ['res4f_relu[0][0]']             \n",
            "                                2048)                                                             \n",
            "                                                                                                  \n",
            " bn5a_branch2c (BatchNormalizat  (None, None, None,   8192       ['res5a_branch2c[0][0]']         \n",
            " ion)                           2048)                                                             \n",
            "                                                                                                  \n",
            " bn5a_branch1 (BatchNormalizati  (None, None, None,   8192       ['res5a_branch1[0][0]']          \n",
            " on)                            2048)                                                             \n",
            "                                                                                                  \n",
            " res5a (Add)                    (None, None, None,   0           ['bn5a_branch2c[0][0]',          \n",
            "                                2048)                             'bn5a_branch1[0][0]']           \n",
            "                                                                                                  \n",
            " res5a_relu (Activation)        (None, None, None,   0           ['res5a[0][0]']                  \n",
            "                                2048)                                                             \n",
            "                                                                                                  \n",
            " res5b_branch2a (Conv2D)        (None, None, None,   1048576     ['res5a_relu[0][0]']             \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " bn5b_branch2a (BatchNormalizat  (None, None, None,   2048       ['res5b_branch2a[0][0]']         \n",
            " ion)                           512)                                                              \n",
            "                                                                                                  \n",
            " res5b_branch2a_relu (Activatio  (None, None, None,   0          ['bn5b_branch2a[0][0]']          \n",
            " n)                             512)                                                              \n",
            "                                                                                                  \n",
            " padding5b_branch2b (ZeroPaddin  (None, None, None,   0          ['res5b_branch2a_relu[0][0]']    \n",
            " g2D)                           512)                                                              \n",
            "                                                                                                  \n",
            " res5b_branch2b (Conv2D)        (None, None, None,   2359296     ['padding5b_branch2b[0][0]']     \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " bn5b_branch2b (BatchNormalizat  (None, None, None,   2048       ['res5b_branch2b[0][0]']         \n",
            " ion)                           512)                                                              \n",
            "                                                                                                  \n",
            " res5b_branch2b_relu (Activatio  (None, None, None,   0          ['bn5b_branch2b[0][0]']          \n",
            " n)                             512)                                                              \n",
            "                                                                                                  \n",
            " res5b_branch2c (Conv2D)        (None, None, None,   1048576     ['res5b_branch2b_relu[0][0]']    \n",
            "                                2048)                                                             \n",
            "                                                                                                  \n",
            " bn5b_branch2c (BatchNormalizat  (None, None, None,   8192       ['res5b_branch2c[0][0]']         \n",
            " ion)                           2048)                                                             \n",
            "                                                                                                  \n",
            " res5b (Add)                    (None, None, None,   0           ['bn5b_branch2c[0][0]',          \n",
            "                                2048)                             'res5a_relu[0][0]']             \n",
            "                                                                                                  \n",
            " res5b_relu (Activation)        (None, None, None,   0           ['res5b[0][0]']                  \n",
            "                                2048)                                                             \n",
            "                                                                                                  \n",
            " res5c_branch2a (Conv2D)        (None, None, None,   1048576     ['res5b_relu[0][0]']             \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " bn5c_branch2a (BatchNormalizat  (None, None, None,   2048       ['res5c_branch2a[0][0]']         \n",
            " ion)                           512)                                                              \n",
            "                                                                                                  \n",
            " res5c_branch2a_relu (Activatio  (None, None, None,   0          ['bn5c_branch2a[0][0]']          \n",
            " n)                             512)                                                              \n",
            "                                                                                                  \n",
            " padding5c_branch2b (ZeroPaddin  (None, None, None,   0          ['res5c_branch2a_relu[0][0]']    \n",
            " g2D)                           512)                                                              \n",
            "                                                                                                  \n",
            " res5c_branch2b (Conv2D)        (None, None, None,   2359296     ['padding5c_branch2b[0][0]']     \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " bn5c_branch2b (BatchNormalizat  (None, None, None,   2048       ['res5c_branch2b[0][0]']         \n",
            " ion)                           512)                                                              \n",
            "                                                                                                  \n",
            " res5c_branch2b_relu (Activatio  (None, None, None,   0          ['bn5c_branch2b[0][0]']          \n",
            " n)                             512)                                                              \n",
            "                                                                                                  \n",
            " res5c_branch2c (Conv2D)        (None, None, None,   1048576     ['res5c_branch2b_relu[0][0]']    \n",
            "                                2048)                                                             \n",
            "                                                                                                  \n",
            " bn5c_branch2c (BatchNormalizat  (None, None, None,   8192       ['res5c_branch2c[0][0]']         \n",
            " ion)                           2048)                                                             \n",
            "                                                                                                  \n",
            " res5c (Add)                    (None, None, None,   0           ['bn5c_branch2c[0][0]',          \n",
            "                                2048)                             'res5b_relu[0][0]']             \n",
            "                                                                                                  \n",
            " res5c_relu (Activation)        (None, None, None,   0           ['res5c[0][0]']                  \n",
            "                                2048)                                                             \n",
            "                                                                                                  \n",
            " C5_reduced (Conv2D)            (None, None, None,   524544      ['res5c_relu[0][0]']             \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " P5_upsampled (UpsampleLike)    (None, None, None,   0           ['C5_reduced[0][0]',             \n",
            "                                256)                              'res4f_relu[0][0]']             \n",
            "                                                                                                  \n",
            " C4_reduced (Conv2D)            (None, None, None,   262400      ['res4f_relu[0][0]']             \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " P4_merged (Add)                (None, None, None,   0           ['P5_upsampled[0][0]',           \n",
            "                                256)                              'C4_reduced[0][0]']             \n",
            "                                                                                                  \n",
            " P4_upsampled (UpsampleLike)    (None, None, None,   0           ['P4_merged[0][0]',              \n",
            "                                256)                              'res3d_relu[0][0]']             \n",
            "                                                                                                  \n",
            " C3_reduced (Conv2D)            (None, None, None,   131328      ['res3d_relu[0][0]']             \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " P6 (Conv2D)                    (None, None, None,   4718848     ['res5c_relu[0][0]']             \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " P3_merged (Add)                (None, None, None,   0           ['P4_upsampled[0][0]',           \n",
            "                                256)                              'C3_reduced[0][0]']             \n",
            "                                                                                                  \n",
            " C6_relu (Activation)           (None, None, None,   0           ['P6[0][0]']                     \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " P3 (Conv2D)                    (None, None, None,   590080      ['P3_merged[0][0]']              \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " P4 (Conv2D)                    (None, None, None,   590080      ['P4_merged[0][0]']              \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " P5 (Conv2D)                    (None, None, None,   590080      ['C5_reduced[0][0]']             \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " P7 (Conv2D)                    (None, None, None,   590080      ['C6_relu[0][0]']                \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " regression_submodel (Functiona  (None, None, 4)     2443300     ['P3[0][0]',                     \n",
            " l)                                                               'P4[0][0]',                     \n",
            "                                                                  'P5[0][0]',                     \n",
            "                                                                  'P6[0][0]',                     \n",
            "                                                                  'P7[0][0]']                     \n",
            "                                                                                                  \n",
            " classification_submodel (Funct  (None, None, 16)    2692240     ['P3[0][0]',                     \n",
            " ional)                                                           'P4[0][0]',                     \n",
            "                                                                  'P5[0][0]',                     \n",
            "                                                                  'P6[0][0]',                     \n",
            "                                                                  'P7[0][0]']                     \n",
            "                                                                                                  \n",
            " regression (Concatenate)       (None, None, 4)      0           ['regression_submodel[0][0]',    \n",
            "                                                                  'regression_submodel[1][0]',    \n",
            "                                                                  'regression_submodel[2][0]',    \n",
            "                                                                  'regression_submodel[3][0]',    \n",
            "                                                                  'regression_submodel[4][0]']    \n",
            "                                                                                                  \n",
            " classification (Concatenate)   (None, None, 16)     0           ['classification_submodel[0][0]',\n",
            "                                                                  'classification_submodel[1][0]',\n",
            "                                                                  'classification_submodel[2][0]',\n",
            "                                                                  'classification_submodel[3][0]',\n",
            "                                                                  'classification_submodel[4][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 36,694,132\n",
            "Trainable params: 13,132,980\n",
            "Non-trainable params: 23,561,152\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 13:32 - loss: 3.1138 - regression_loss: 1.9790 - classification_loss: 1.1349\n",
            " 2/86 [..............................] - ETA: 4:37 - loss: 3.0293 - regression_loss: 1.8959 - classification_loss: 1.1333 \n",
            " 3/86 [>.............................] - ETA: 5:25 - loss: 3.0967 - regression_loss: 1.9647 - classification_loss: 1.1320\n",
            " 4/86 [>.............................] - ETA: 3:48 - loss: 3.0992 - regression_loss: 1.9714 - classification_loss: 1.1278\n",
            " 5/86 [>.............................] - ETA: 3:07 - loss: 3.1069 - regression_loss: 1.9805 - classification_loss: 1.1264\n",
            " 6/86 [=>............................] - ETA: 2:44 - loss: 3.1318 - regression_loss: 2.0065 - classification_loss: 1.1253\n",
            " 7/86 [=>............................] - ETA: 2:27 - loss: 3.1317 - regression_loss: 2.0048 - classification_loss: 1.1269\n",
            " 8/86 [=>............................] - ETA: 2:17 - loss: 3.1242 - regression_loss: 1.9961 - classification_loss: 1.1281\n",
            " 9/86 [==>...........................] - ETA: 2:08 - loss: 3.1482 - regression_loss: 2.0207 - classification_loss: 1.1276\n",
            "10/86 [==>...........................] - ETA: 2:28 - loss: 3.1444 - regression_loss: 2.0186 - classification_loss: 1.1258\n",
            "11/86 [==>...........................] - ETA: 2:14 - loss: 3.1370 - regression_loss: 2.0120 - classification_loss: 1.1250\n",
            "12/86 [===>..........................] - ETA: 2:07 - loss: 3.1179 - regression_loss: 1.9931 - classification_loss: 1.1247\n",
            "13/86 [===>..........................] - ETA: 2:00 - loss: 3.1071 - regression_loss: 1.9831 - classification_loss: 1.1240\n",
            "14/86 [===>..........................] - ETA: 1:55 - loss: 3.1011 - regression_loss: 1.9778 - classification_loss: 1.1233\n",
            "15/86 [====>.........................] - ETA: 1:51 - loss: 3.1214 - regression_loss: 1.9990 - classification_loss: 1.1224\n",
            "16/86 [====>.........................] - ETA: 1:48 - loss: 3.1167 - regression_loss: 1.9941 - classification_loss: 1.1226\n",
            "17/86 [====>.........................] - ETA: 1:57 - loss: 3.1012 - regression_loss: 1.9779 - classification_loss: 1.1233\n",
            "18/86 [=====>........................] - ETA: 1:50 - loss: 3.0828 - regression_loss: 1.9603 - classification_loss: 1.1224\n",
            "19/86 [=====>........................] - ETA: 1:46 - loss: 3.0712 - regression_loss: 1.9494 - classification_loss: 1.1219\n",
            "20/86 [=====>........................] - ETA: 1:42 - loss: 3.0568 - regression_loss: 1.9361 - classification_loss: 1.1207\n",
            "21/86 [======>.......................] - ETA: 1:39 - loss: 3.0591 - regression_loss: 1.9387 - classification_loss: 1.1204\n",
            "22/86 [======>.......................] - ETA: 1:35 - loss: 3.0532 - regression_loss: 1.9340 - classification_loss: 1.1192\n",
            "23/86 [=======>......................] - ETA: 1:32 - loss: 3.0524 - regression_loss: 1.9338 - classification_loss: 1.1186\n",
            "24/86 [=======>......................] - ETA: 1:30 - loss: 3.0449 - regression_loss: 1.9274 - classification_loss: 1.1175\n",
            "25/86 [=======>......................] - ETA: 1:27 - loss: 3.0402 - regression_loss: 1.9234 - classification_loss: 1.1169\n",
            "26/86 [========>.....................] - ETA: 1:24 - loss: 3.0357 - regression_loss: 1.9198 - classification_loss: 1.1158\n",
            "27/86 [========>.....................] - ETA: 1:21 - loss: 3.0312 - regression_loss: 1.9163 - classification_loss: 1.1149\n",
            "28/86 [========>.....................] - ETA: 1:19 - loss: 3.0237 - regression_loss: 1.9093 - classification_loss: 1.1143\n",
            "29/86 [=========>....................] - ETA: 1:17 - loss: 3.0177 - regression_loss: 1.9046 - classification_loss: 1.1131\n",
            "30/86 [=========>....................] - ETA: 1:21 - loss: 3.0142 - regression_loss: 1.9018 - classification_loss: 1.1124\n",
            "31/86 [=========>....................] - ETA: 1:17 - loss: 3.0093 - regression_loss: 1.8979 - classification_loss: 1.1113\n",
            "32/86 [==========>...................] - ETA: 1:15 - loss: 2.9988 - regression_loss: 1.8884 - classification_loss: 1.1104\n",
            "33/86 [==========>...................] - ETA: 1:13 - loss: 2.9951 - regression_loss: 1.8857 - classification_loss: 1.1095\n",
            "34/86 [==========>...................] - ETA: 1:15 - loss: 2.9916 - regression_loss: 1.8834 - classification_loss: 1.1082\n",
            "35/86 [===========>..................] - ETA: 1:12 - loss: 2.9797 - regression_loss: 1.8721 - classification_loss: 1.1075\n",
            "36/86 [===========>..................] - ETA: 1:11 - loss: 2.9807 - regression_loss: 1.8739 - classification_loss: 1.1068\n",
            "37/86 [===========>..................] - ETA: 1:14 - loss: 2.9783 - regression_loss: 1.8717 - classification_loss: 1.1066\n",
            "38/86 [============>.................] - ETA: 1:11 - loss: 2.9739 - regression_loss: 1.8676 - classification_loss: 1.1063\n",
            "39/86 [============>.................] - ETA: 1:09 - loss: 2.9641 - regression_loss: 1.8588 - classification_loss: 1.1053\n",
            "40/86 [============>.................] - ETA: 1:06 - loss: 2.9616 - regression_loss: 1.8568 - classification_loss: 1.1047\n",
            "41/86 [=============>................] - ETA: 1:04 - loss: 2.9552 - regression_loss: 1.8515 - classification_loss: 1.1037\n",
            "42/86 [=============>................] - ETA: 1:02 - loss: 2.9552 - regression_loss: 1.8525 - classification_loss: 1.1028\n",
            "43/86 [==============>...............] - ETA: 1:00 - loss: 2.9481 - regression_loss: 1.8461 - classification_loss: 1.1020\n",
            "44/86 [==============>...............] - ETA: 58s - loss: 2.9448 - regression_loss: 1.8441 - classification_loss: 1.1007 \n",
            "45/86 [==============>...............] - ETA: 57s - loss: 2.9409 - regression_loss: 1.8413 - classification_loss: 1.0996\n",
            "46/86 [===============>..............] - ETA: 55s - loss: 2.9400 - regression_loss: 1.8413 - classification_loss: 1.0988\n",
            "47/86 [===============>..............] - ETA: 53s - loss: 2.9355 - regression_loss: 1.8375 - classification_loss: 1.0980\n",
            "48/86 [===============>..............] - ETA: 51s - loss: 2.9334 - regression_loss: 1.8365 - classification_loss: 1.0969\n",
            "49/86 [================>.............] - ETA: 50s - loss: 2.9253 - regression_loss: 1.8302 - classification_loss: 1.0951\n",
            "50/86 [================>.............] - ETA: 48s - loss: 2.9272 - regression_loss: 1.8333 - classification_loss: 1.0939\n",
            "51/86 [================>.............] - ETA: 46s - loss: 2.9288 - regression_loss: 1.8359 - classification_loss: 1.0930\n",
            "52/86 [=================>............] - ETA: 45s - loss: 2.9246 - regression_loss: 1.8327 - classification_loss: 1.0919\n",
            "53/86 [=================>............] - ETA: 46s - loss: 2.9182 - regression_loss: 1.8279 - classification_loss: 1.0903\n",
            "54/86 [=================>............] - ETA: 44s - loss: 2.9141 - regression_loss: 1.8252 - classification_loss: 1.0890\n",
            "55/86 [==================>...........] - ETA: 44s - loss: 2.9112 - regression_loss: 1.8231 - classification_loss: 1.0881\n",
            "56/86 [==================>...........] - ETA: 44s - loss: 2.9135 - regression_loss: 1.8269 - classification_loss: 1.0866\n",
            "57/86 [==================>...........] - ETA: 42s - loss: 2.9151 - regression_loss: 1.8294 - classification_loss: 1.0857\n",
            "58/86 [===================>..........] - ETA: 40s - loss: 2.9190 - regression_loss: 1.8339 - classification_loss: 1.0851\n",
            "59/86 [===================>..........] - ETA: 40s - loss: 2.9200 - regression_loss: 1.8363 - classification_loss: 1.0836\n",
            "60/86 [===================>..........] - ETA: 38s - loss: 2.9162 - regression_loss: 1.8336 - classification_loss: 1.0825\n",
            "61/86 [====================>.........] - ETA: 39s - loss: 2.9135 - regression_loss: 1.8316 - classification_loss: 1.0820\n",
            "62/86 [====================>.........] - ETA: 36s - loss: 2.9132 - regression_loss: 1.8324 - classification_loss: 1.0808\n",
            "63/86 [====================>.........] - ETA: 36s - loss: 2.9083 - regression_loss: 1.8289 - classification_loss: 1.0794\n",
            "64/86 [=====================>........] - ETA: 35s - loss: 2.9084 - regression_loss: 1.8301 - classification_loss: 1.0783\n",
            "65/86 [=====================>........] - ETA: 33s - loss: 2.9019 - regression_loss: 1.8254 - classification_loss: 1.0765\n",
            "66/86 [======================>.......] - ETA: 31s - loss: 2.8999 - regression_loss: 1.8245 - classification_loss: 1.0754\n",
            "67/86 [======================>.......] - ETA: 30s - loss: 2.8981 - regression_loss: 1.8242 - classification_loss: 1.0739\n",
            "68/86 [======================>.......] - ETA: 28s - loss: 2.8977 - regression_loss: 1.8251 - classification_loss: 1.0726\n",
            "69/86 [=======================>......] - ETA: 27s - loss: 2.8900 - regression_loss: 1.8194 - classification_loss: 1.0706\n",
            "70/86 [=======================>......] - ETA: 25s - loss: 2.8872 - regression_loss: 1.8179 - classification_loss: 1.0693\n",
            "71/86 [=======================>......] - ETA: 24s - loss: 2.8846 - regression_loss: 1.8167 - classification_loss: 1.0679\n",
            "72/86 [========================>.....] - ETA: 22s - loss: 2.8833 - regression_loss: 1.8168 - classification_loss: 1.0665\n",
            "73/86 [========================>.....] - ETA: 21s - loss: 2.8817 - regression_loss: 1.8162 - classification_loss: 1.0654\n",
            "74/86 [========================>.....] - ETA: 19s - loss: 2.8776 - regression_loss: 1.8146 - classification_loss: 1.0631\n",
            "75/86 [=========================>....] - ETA: 17s - loss: 2.8740 - regression_loss: 1.8130 - classification_loss: 1.0610\n",
            "76/86 [=========================>....] - ETA: 16s - loss: 2.8739 - regression_loss: 1.8141 - classification_loss: 1.0598\n",
            "77/86 [=========================>....] - ETA: 14s - loss: 2.8697 - regression_loss: 1.8122 - classification_loss: 1.0575\n",
            "78/86 [==========================>...] - ETA: 12s - loss: 2.8689 - regression_loss: 1.8131 - classification_loss: 1.0558\n",
            "79/86 [==========================>...] - ETA: 11s - loss: 2.8711 - regression_loss: 1.8168 - classification_loss: 1.0543\n",
            "80/86 [==========================>...] - ETA: 9s - loss: 2.8688 - regression_loss: 1.8162 - classification_loss: 1.0526 \n",
            "81/86 [===========================>..] - ETA: 8s - loss: 2.8687 - regression_loss: 1.8183 - classification_loss: 1.0504\n",
            "82/86 [===========================>..] - ETA: 6s - loss: 2.8643 - regression_loss: 1.8160 - classification_loss: 1.0483\n",
            "83/86 [===========================>..] - ETA: 5s - loss: 2.8612 - regression_loss: 1.8152 - classification_loss: 1.0461\n",
            "84/86 [============================>.] - ETA: 3s - loss: 2.8592 - regression_loss: 1.8147 - classification_loss: 1.0445\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.8496 - regression_loss: 1.8083 - classification_loss: 1.0413\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.8411 - regression_loss: 1.8023 - classification_loss: 1.0388\n",
            "Epoch 1: saving model to ./snapshots\\resnet50_csv_01.h5\n",
            "\n",
            "86/86 [==============================] - 158s 2s/step - loss: 2.8411 - regression_loss: 1.8023 - classification_loss: 1.0388 - lr: 1.0000e-05\n",
            "Epoch 2/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:41 - loss: 2.5452 - regression_loss: 1.6020 - classification_loss: 0.9432\n",
            " 2/86 [..............................] - ETA: 5:09 - loss: 2.5789 - regression_loss: 1.6923 - classification_loss: 0.8866\n",
            " 3/86 [>.............................] - ETA: 2:52 - loss: 2.5912 - regression_loss: 1.7013 - classification_loss: 0.8899\n",
            " 4/86 [>.............................] - ETA: 2:19 - loss: 2.6369 - regression_loss: 1.7410 - classification_loss: 0.8959\n",
            " 5/86 [>.............................] - ETA: 2:01 - loss: 2.6317 - regression_loss: 1.7407 - classification_loss: 0.8910\n",
            " 6/86 [=>............................] - ETA: 1:49 - loss: 2.6612 - regression_loss: 1.7581 - classification_loss: 0.9031\n",
            " 7/86 [=>............................] - ETA: 1:40 - loss: 2.6407 - regression_loss: 1.7434 - classification_loss: 0.8974\n",
            " 8/86 [=>............................] - ETA: 2:14 - loss: 2.6350 - regression_loss: 1.7288 - classification_loss: 0.9062\n",
            " 9/86 [==>...........................] - ETA: 2:26 - loss: 2.6616 - regression_loss: 1.7608 - classification_loss: 0.9008\n",
            "10/86 [==>...........................] - ETA: 2:11 - loss: 2.6567 - regression_loss: 1.7585 - classification_loss: 0.8982\n",
            "11/86 [==>...........................] - ETA: 2:06 - loss: 2.6676 - regression_loss: 1.7725 - classification_loss: 0.8951\n",
            "12/86 [===>..........................] - ETA: 1:58 - loss: 2.6737 - regression_loss: 1.7868 - classification_loss: 0.8869\n",
            "13/86 [===>..........................] - ETA: 1:52 - loss: 2.6676 - regression_loss: 1.7872 - classification_loss: 0.8804\n",
            "14/86 [===>..........................] - ETA: 2:05 - loss: 2.6706 - regression_loss: 1.7974 - classification_loss: 0.8732\n",
            "15/86 [====>.........................] - ETA: 1:56 - loss: 2.6667 - regression_loss: 1.7955 - classification_loss: 0.8712\n",
            "16/86 [====>.........................] - ETA: 1:51 - loss: 2.6553 - regression_loss: 1.7881 - classification_loss: 0.8673\n",
            "17/86 [====>.........................] - ETA: 1:47 - loss: 2.6471 - regression_loss: 1.7809 - classification_loss: 0.8662\n",
            "18/86 [=====>........................] - ETA: 1:44 - loss: 2.6412 - regression_loss: 1.7760 - classification_loss: 0.8653\n",
            "19/86 [=====>........................] - ETA: 1:39 - loss: 2.6295 - regression_loss: 1.7687 - classification_loss: 0.8608\n",
            "20/86 [=====>........................] - ETA: 1:54 - loss: 2.6177 - regression_loss: 1.7602 - classification_loss: 0.8575\n",
            "21/86 [======>.......................] - ETA: 1:49 - loss: 2.6150 - regression_loss: 1.7583 - classification_loss: 0.8567\n",
            "22/86 [======>.......................] - ETA: 1:52 - loss: 2.6091 - regression_loss: 1.7536 - classification_loss: 0.8555\n",
            "23/86 [=======>......................] - ETA: 1:47 - loss: 2.6119 - regression_loss: 1.7534 - classification_loss: 0.8585\n",
            "24/86 [=======>......................] - ETA: 1:43 - loss: 2.6064 - regression_loss: 1.7495 - classification_loss: 0.8569\n",
            "25/86 [=======>......................] - ETA: 1:39 - loss: 2.6127 - regression_loss: 1.7558 - classification_loss: 0.8568\n",
            "26/86 [========>.....................] - ETA: 1:36 - loss: 2.6212 - regression_loss: 1.7628 - classification_loss: 0.8584\n",
            "27/86 [========>.....................] - ETA: 1:33 - loss: 2.6118 - regression_loss: 1.7553 - classification_loss: 0.8565\n",
            "28/86 [========>.....................] - ETA: 1:30 - loss: 2.6165 - regression_loss: 1.7595 - classification_loss: 0.8570\n",
            "29/86 [=========>....................] - ETA: 1:27 - loss: 2.6290 - regression_loss: 1.7705 - classification_loss: 0.8584\n",
            "30/86 [=========>....................] - ETA: 1:24 - loss: 2.6193 - regression_loss: 1.7633 - classification_loss: 0.8559\n",
            "31/86 [=========>....................] - ETA: 1:21 - loss: 2.6228 - regression_loss: 1.7680 - classification_loss: 0.8548\n",
            "32/86 [==========>...................] - ETA: 1:19 - loss: 2.6203 - regression_loss: 1.7686 - classification_loss: 0.8518\n",
            "33/86 [==========>...................] - ETA: 1:16 - loss: 2.6220 - regression_loss: 1.7719 - classification_loss: 0.8501\n",
            "34/86 [==========>...................] - ETA: 1:14 - loss: 2.6151 - regression_loss: 1.7669 - classification_loss: 0.8482\n",
            "35/86 [===========>..................] - ETA: 1:12 - loss: 2.6162 - regression_loss: 1.7695 - classification_loss: 0.8466\n",
            "36/86 [===========>..................] - ETA: 1:10 - loss: 2.6081 - regression_loss: 1.7623 - classification_loss: 0.8458\n",
            "37/86 [===========>..................] - ETA: 1:12 - loss: 2.6137 - regression_loss: 1.7704 - classification_loss: 0.8434\n",
            "38/86 [============>.................] - ETA: 1:09 - loss: 2.6117 - regression_loss: 1.7699 - classification_loss: 0.8418\n",
            "39/86 [============>.................] - ETA: 1:07 - loss: 2.6110 - regression_loss: 1.7719 - classification_loss: 0.8391\n",
            "40/86 [============>.................] - ETA: 1:05 - loss: 2.6060 - regression_loss: 1.7678 - classification_loss: 0.8382\n",
            "41/86 [=============>................] - ETA: 1:03 - loss: 2.6035 - regression_loss: 1.7666 - classification_loss: 0.8369\n",
            "42/86 [=============>................] - ETA: 1:05 - loss: 2.6079 - regression_loss: 1.7701 - classification_loss: 0.8378\n",
            "43/86 [==============>...............] - ETA: 1:02 - loss: 2.5999 - regression_loss: 1.7666 - classification_loss: 0.8333\n",
            "44/86 [==============>...............] - ETA: 1:00 - loss: 2.5952 - regression_loss: 1.7643 - classification_loss: 0.8309\n",
            "45/86 [==============>...............] - ETA: 58s - loss: 2.5934 - regression_loss: 1.7644 - classification_loss: 0.8290 \n",
            "46/86 [===============>..............] - ETA: 59s - loss: 2.5856 - regression_loss: 1.7591 - classification_loss: 0.8264\n",
            "47/86 [===============>..............] - ETA: 56s - loss: 2.5805 - regression_loss: 1.7571 - classification_loss: 0.8234\n",
            "48/86 [===============>..............] - ETA: 54s - loss: 2.5775 - regression_loss: 1.7560 - classification_loss: 0.8216\n",
            "49/86 [================>.............] - ETA: 52s - loss: 2.5733 - regression_loss: 1.7536 - classification_loss: 0.8198\n",
            "50/86 [================>.............] - ETA: 51s - loss: 2.5688 - regression_loss: 1.7502 - classification_loss: 0.8187\n",
            "51/86 [================>.............] - ETA: 49s - loss: 2.5649 - regression_loss: 1.7475 - classification_loss: 0.8175\n",
            "52/86 [=================>............] - ETA: 47s - loss: 2.5645 - regression_loss: 1.7485 - classification_loss: 0.8160\n",
            "53/86 [=================>............] - ETA: 45s - loss: 2.5621 - regression_loss: 1.7472 - classification_loss: 0.8149\n",
            "54/86 [=================>............] - ETA: 44s - loss: 2.5579 - regression_loss: 1.7443 - classification_loss: 0.8136\n",
            "55/86 [==================>...........] - ETA: 42s - loss: 2.5560 - regression_loss: 1.7432 - classification_loss: 0.8127\n",
            "56/86 [==================>...........] - ETA: 40s - loss: 2.5551 - regression_loss: 1.7420 - classification_loss: 0.8132\n",
            "57/86 [==================>...........] - ETA: 39s - loss: 2.5481 - regression_loss: 1.7378 - classification_loss: 0.8102\n",
            "58/86 [===================>..........] - ETA: 37s - loss: 2.5443 - regression_loss: 1.7351 - classification_loss: 0.8092\n",
            "59/86 [===================>..........] - ETA: 36s - loss: 2.5410 - regression_loss: 1.7334 - classification_loss: 0.8076\n",
            "60/86 [===================>..........] - ETA: 34s - loss: 2.5442 - regression_loss: 1.7370 - classification_loss: 0.8072\n",
            "61/86 [====================>.........] - ETA: 33s - loss: 2.5460 - regression_loss: 1.7393 - classification_loss: 0.8067\n",
            "62/86 [====================>.........] - ETA: 33s - loss: 2.5417 - regression_loss: 1.7383 - classification_loss: 0.8034\n",
            "63/86 [====================>.........] - ETA: 33s - loss: 2.5400 - regression_loss: 1.7378 - classification_loss: 0.8022\n",
            "64/86 [=====================>........] - ETA: 31s - loss: 2.5433 - regression_loss: 1.7408 - classification_loss: 0.8026\n",
            "65/86 [=====================>........] - ETA: 29s - loss: 2.5426 - regression_loss: 1.7411 - classification_loss: 0.8016\n",
            "66/86 [======================>.......] - ETA: 28s - loss: 2.5368 - regression_loss: 1.7364 - classification_loss: 0.8005\n",
            "67/86 [======================>.......] - ETA: 26s - loss: 2.5332 - regression_loss: 1.7344 - classification_loss: 0.7988\n",
            "68/86 [======================>.......] - ETA: 25s - loss: 2.5322 - regression_loss: 1.7340 - classification_loss: 0.7981\n",
            "69/86 [=======================>......] - ETA: 23s - loss: 2.5305 - regression_loss: 1.7344 - classification_loss: 0.7961\n",
            "70/86 [=======================>......] - ETA: 22s - loss: 2.5291 - regression_loss: 1.7354 - classification_loss: 0.7937\n",
            "71/86 [=======================>......] - ETA: 21s - loss: 2.5197 - regression_loss: 1.7293 - classification_loss: 0.7904\n",
            "72/86 [========================>.....] - ETA: 19s - loss: 2.5199 - regression_loss: 1.7310 - classification_loss: 0.7889\n",
            "73/86 [========================>.....] - ETA: 18s - loss: 2.5202 - regression_loss: 1.7316 - classification_loss: 0.7886\n",
            "74/86 [========================>.....] - ETA: 17s - loss: 2.5174 - regression_loss: 1.7304 - classification_loss: 0.7870\n",
            "75/86 [=========================>....] - ETA: 15s - loss: 2.5163 - regression_loss: 1.7300 - classification_loss: 0.7863\n",
            "76/86 [=========================>....] - ETA: 14s - loss: 2.5096 - regression_loss: 1.7259 - classification_loss: 0.7836\n",
            "77/86 [=========================>....] - ETA: 13s - loss: 2.5095 - regression_loss: 1.7275 - classification_loss: 0.7820\n",
            "78/86 [==========================>...] - ETA: 11s - loss: 2.5071 - regression_loss: 1.7232 - classification_loss: 0.7839\n",
            "79/86 [==========================>...] - ETA: 9s - loss: 2.5036 - regression_loss: 1.7206 - classification_loss: 0.7831 \n",
            "80/86 [==========================>...] - ETA: 8s - loss: 2.5022 - regression_loss: 1.7199 - classification_loss: 0.7823\n",
            "81/86 [===========================>..] - ETA: 7s - loss: 2.4973 - regression_loss: 1.7169 - classification_loss: 0.7805\n",
            "82/86 [===========================>..] - ETA: 5s - loss: 2.4961 - regression_loss: 1.7172 - classification_loss: 0.7789\n",
            "83/86 [===========================>..] - ETA: 4s - loss: 2.4918 - regression_loss: 1.7143 - classification_loss: 0.7776\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.4906 - regression_loss: 1.7139 - classification_loss: 0.7767\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.4946 - regression_loss: 1.7167 - classification_loss: 0.7779\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.4914 - regression_loss: 1.7144 - classification_loss: 0.7770\n",
            "Epoch 2: saving model to ./snapshots\\resnet50_csv_02.h5\n",
            "\n",
            "86/86 [==============================] - 119s 1s/step - loss: 2.4914 - regression_loss: 1.7144 - classification_loss: 0.7770 - lr: 1.0000e-05\n",
            "Epoch 3/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:41 - loss: 2.3919 - regression_loss: 1.7068 - classification_loss: 0.6850\n",
            " 2/86 [..............................] - ETA: 1:12 - loss: 2.3726 - regression_loss: 1.6722 - classification_loss: 0.7005\n",
            " 3/86 [>.............................] - ETA: 1:25 - loss: 2.2897 - regression_loss: 1.6234 - classification_loss: 0.6663\n",
            " 4/86 [>.............................] - ETA: 1:19 - loss: 2.2870 - regression_loss: 1.6146 - classification_loss: 0.6725\n",
            " 5/86 [>.............................] - ETA: 1:23 - loss: 2.3222 - regression_loss: 1.6396 - classification_loss: 0.6826\n",
            " 6/86 [=>............................] - ETA: 1:20 - loss: 2.2364 - regression_loss: 1.5758 - classification_loss: 0.6606\n",
            " 7/86 [=>............................] - ETA: 1:17 - loss: 2.2448 - regression_loss: 1.5779 - classification_loss: 0.6669\n",
            " 8/86 [=>............................] - ETA: 1:18 - loss: 2.2555 - regression_loss: 1.5830 - classification_loss: 0.6725\n",
            " 9/86 [==>...........................] - ETA: 1:14 - loss: 2.2656 - regression_loss: 1.5950 - classification_loss: 0.6705\n",
            "10/86 [==>...........................] - ETA: 1:51 - loss: 2.3247 - regression_loss: 1.6472 - classification_loss: 0.6775\n",
            "11/86 [==>...........................] - ETA: 1:41 - loss: 2.3203 - regression_loss: 1.6433 - classification_loss: 0.6770\n",
            "12/86 [===>..........................] - ETA: 1:37 - loss: 2.3218 - regression_loss: 1.6456 - classification_loss: 0.6762\n",
            "13/86 [===>..........................] - ETA: 1:33 - loss: 2.3594 - regression_loss: 1.6737 - classification_loss: 0.6856\n",
            "14/86 [===>..........................] - ETA: 1:29 - loss: 2.3722 - regression_loss: 1.6803 - classification_loss: 0.6919\n",
            "15/86 [====>.........................] - ETA: 1:26 - loss: 2.3732 - regression_loss: 1.6802 - classification_loss: 0.6929\n",
            "16/86 [====>.........................] - ETA: 1:24 - loss: 2.3739 - regression_loss: 1.6825 - classification_loss: 0.6915\n",
            "17/86 [====>.........................] - ETA: 1:21 - loss: 2.3679 - regression_loss: 1.6796 - classification_loss: 0.6883\n",
            "18/86 [=====>........................] - ETA: 1:18 - loss: 2.3763 - regression_loss: 1.6870 - classification_loss: 0.6893\n",
            "19/86 [=====>........................] - ETA: 1:16 - loss: 2.3748 - regression_loss: 1.6855 - classification_loss: 0.6894\n",
            "20/86 [=====>........................] - ETA: 1:16 - loss: 2.3751 - regression_loss: 1.6888 - classification_loss: 0.6863\n",
            "21/86 [======>.......................] - ETA: 1:13 - loss: 2.3752 - regression_loss: 1.6895 - classification_loss: 0.6857\n",
            "22/86 [======>.......................] - ETA: 1:12 - loss: 2.3854 - regression_loss: 1.6978 - classification_loss: 0.6876\n",
            "23/86 [=======>......................] - ETA: 1:10 - loss: 2.3811 - regression_loss: 1.6962 - classification_loss: 0.6849\n",
            "24/86 [=======>......................] - ETA: 1:08 - loss: 2.3819 - regression_loss: 1.6958 - classification_loss: 0.6861\n",
            "25/86 [=======>......................] - ETA: 1:07 - loss: 2.3847 - regression_loss: 1.6977 - classification_loss: 0.6870\n",
            "26/86 [========>.....................] - ETA: 1:05 - loss: 2.3876 - regression_loss: 1.7004 - classification_loss: 0.6871\n",
            "27/86 [========>.....................] - ETA: 1:04 - loss: 2.3846 - regression_loss: 1.6971 - classification_loss: 0.6875\n",
            "28/86 [========>.....................] - ETA: 1:02 - loss: 2.3795 - regression_loss: 1.6937 - classification_loss: 0.6859\n",
            "29/86 [=========>....................] - ETA: 1:01 - loss: 2.3878 - regression_loss: 1.6995 - classification_loss: 0.6883\n",
            "30/86 [=========>....................] - ETA: 1:00 - loss: 2.3891 - regression_loss: 1.7003 - classification_loss: 0.6888\n",
            "31/86 [=========>....................] - ETA: 58s - loss: 2.3841 - regression_loss: 1.6970 - classification_loss: 0.6872 \n",
            "32/86 [==========>...................] - ETA: 57s - loss: 2.3878 - regression_loss: 1.7024 - classification_loss: 0.6854\n",
            "33/86 [==========>...................] - ETA: 55s - loss: 2.3867 - regression_loss: 1.7012 - classification_loss: 0.6855\n",
            "34/86 [==========>...................] - ETA: 54s - loss: 2.3824 - regression_loss: 1.6986 - classification_loss: 0.6838\n",
            "35/86 [===========>..................] - ETA: 53s - loss: 2.3757 - regression_loss: 1.6940 - classification_loss: 0.6817\n",
            "36/86 [===========>..................] - ETA: 52s - loss: 2.3720 - regression_loss: 1.6910 - classification_loss: 0.6810\n",
            "37/86 [===========>..................] - ETA: 50s - loss: 2.3669 - regression_loss: 1.6861 - classification_loss: 0.6808\n",
            "38/86 [============>.................] - ETA: 49s - loss: 2.3682 - regression_loss: 1.6894 - classification_loss: 0.6788\n",
            "39/86 [============>.................] - ETA: 48s - loss: 2.3715 - regression_loss: 1.6940 - classification_loss: 0.6774\n",
            "40/86 [============>.................] - ETA: 47s - loss: 2.3728 - regression_loss: 1.6957 - classification_loss: 0.6771\n",
            "41/86 [=============>................] - ETA: 46s - loss: 2.3719 - regression_loss: 1.6964 - classification_loss: 0.6755\n",
            "42/86 [=============>................] - ETA: 44s - loss: 2.3727 - regression_loss: 1.6971 - classification_loss: 0.6756\n",
            "43/86 [==============>...............] - ETA: 43s - loss: 2.3666 - regression_loss: 1.6940 - classification_loss: 0.6726\n",
            "44/86 [==============>...............] - ETA: 44s - loss: 2.3761 - regression_loss: 1.7027 - classification_loss: 0.6735\n",
            "45/86 [==============>...............] - ETA: 43s - loss: 2.3692 - regression_loss: 1.6972 - classification_loss: 0.6720\n",
            "46/86 [===============>..............] - ETA: 42s - loss: 2.3640 - regression_loss: 1.6939 - classification_loss: 0.6700\n",
            "47/86 [===============>..............] - ETA: 40s - loss: 2.3700 - regression_loss: 1.6985 - classification_loss: 0.6714\n",
            "48/86 [===============>..............] - ETA: 39s - loss: 2.3731 - regression_loss: 1.7015 - classification_loss: 0.6717\n",
            "49/86 [================>.............] - ETA: 38s - loss: 2.3665 - regression_loss: 1.6965 - classification_loss: 0.6700\n",
            "50/86 [================>.............] - ETA: 39s - loss: 2.3660 - regression_loss: 1.6965 - classification_loss: 0.6695\n",
            "51/86 [================>.............] - ETA: 37s - loss: 2.3594 - regression_loss: 1.6914 - classification_loss: 0.6680\n",
            "52/86 [=================>............] - ETA: 36s - loss: 2.3608 - regression_loss: 1.6929 - classification_loss: 0.6679\n",
            "53/86 [=================>............] - ETA: 35s - loss: 2.3564 - regression_loss: 1.6888 - classification_loss: 0.6677\n",
            "54/86 [=================>............] - ETA: 34s - loss: 2.3592 - regression_loss: 1.6916 - classification_loss: 0.6675\n",
            "55/86 [==================>...........] - ETA: 32s - loss: 2.3552 - regression_loss: 1.6889 - classification_loss: 0.6663\n",
            "56/86 [==================>...........] - ETA: 31s - loss: 2.3544 - regression_loss: 1.6881 - classification_loss: 0.6663\n",
            "57/86 [==================>...........] - ETA: 30s - loss: 2.3541 - regression_loss: 1.6888 - classification_loss: 0.6654\n",
            "58/86 [===================>..........] - ETA: 29s - loss: 2.3492 - regression_loss: 1.6849 - classification_loss: 0.6643\n",
            "59/86 [===================>..........] - ETA: 28s - loss: 2.3459 - regression_loss: 1.6825 - classification_loss: 0.6634\n",
            "60/86 [===================>..........] - ETA: 27s - loss: 2.3439 - regression_loss: 1.6811 - classification_loss: 0.6628\n",
            "61/86 [====================>.........] - ETA: 26s - loss: 2.3475 - regression_loss: 1.6851 - classification_loss: 0.6624\n",
            "62/86 [====================>.........] - ETA: 25s - loss: 2.3488 - regression_loss: 1.6864 - classification_loss: 0.6624\n",
            "63/86 [====================>.........] - ETA: 24s - loss: 2.3517 - regression_loss: 1.6898 - classification_loss: 0.6620\n",
            "64/86 [=====================>........] - ETA: 23s - loss: 2.3517 - regression_loss: 1.6902 - classification_loss: 0.6615\n",
            "65/86 [=====================>........] - ETA: 22s - loss: 2.3472 - regression_loss: 1.6871 - classification_loss: 0.6601\n",
            "66/86 [======================>.......] - ETA: 20s - loss: 2.3455 - regression_loss: 1.6852 - classification_loss: 0.6602\n",
            "67/86 [======================>.......] - ETA: 19s - loss: 2.3454 - regression_loss: 1.6852 - classification_loss: 0.6602\n",
            "68/86 [======================>.......] - ETA: 20s - loss: 2.3469 - regression_loss: 1.6872 - classification_loss: 0.6597\n",
            "69/86 [=======================>......] - ETA: 19s - loss: 2.3441 - regression_loss: 1.6853 - classification_loss: 0.6588\n",
            "70/86 [=======================>......] - ETA: 18s - loss: 2.3447 - regression_loss: 1.6868 - classification_loss: 0.6578\n",
            "71/86 [=======================>......] - ETA: 17s - loss: 2.3431 - regression_loss: 1.6854 - classification_loss: 0.6577\n",
            "72/86 [========================>.....] - ETA: 15s - loss: 2.3393 - regression_loss: 1.6829 - classification_loss: 0.6564\n",
            "73/86 [========================>.....] - ETA: 15s - loss: 2.3418 - regression_loss: 1.6846 - classification_loss: 0.6572\n",
            "74/86 [========================>.....] - ETA: 13s - loss: 2.3420 - regression_loss: 1.6843 - classification_loss: 0.6577\n",
            "75/86 [=========================>....] - ETA: 12s - loss: 2.3419 - regression_loss: 1.6847 - classification_loss: 0.6572\n",
            "76/86 [=========================>....] - ETA: 11s - loss: 2.3404 - regression_loss: 1.6841 - classification_loss: 0.6563\n",
            "77/86 [=========================>....] - ETA: 10s - loss: 2.3394 - regression_loss: 1.6835 - classification_loss: 0.6559\n",
            "78/86 [==========================>...] - ETA: 9s - loss: 2.3404 - regression_loss: 1.6843 - classification_loss: 0.6562 \n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.3375 - regression_loss: 1.6824 - classification_loss: 0.6551\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.3389 - regression_loss: 1.6841 - classification_loss: 0.6548\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.3346 - regression_loss: 1.6814 - classification_loss: 0.6532\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.3331 - regression_loss: 1.6802 - classification_loss: 0.6529\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.3353 - regression_loss: 1.6827 - classification_loss: 0.6525\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.3348 - regression_loss: 1.6822 - classification_loss: 0.6526\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.3345 - regression_loss: 1.6821 - classification_loss: 0.6525\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.3354 - regression_loss: 1.6835 - classification_loss: 0.6520\n",
            "Epoch 3: saving model to ./snapshots\\resnet50_csv_03.h5\n",
            "\n",
            "86/86 [==============================] - 97s 1s/step - loss: 2.3354 - regression_loss: 1.6835 - classification_loss: 0.6520 - lr: 1.0000e-05\n",
            "Epoch 4/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:38 - loss: 2.2270 - regression_loss: 1.6048 - classification_loss: 0.6223\n",
            " 2/86 [..............................] - ETA: 1:11 - loss: 2.1757 - regression_loss: 1.5494 - classification_loss: 0.6262\n",
            " 3/86 [>.............................] - ETA: 1:15 - loss: 2.2195 - regression_loss: 1.6151 - classification_loss: 0.6044\n",
            " 4/86 [>.............................] - ETA: 1:10 - loss: 2.2214 - regression_loss: 1.6218 - classification_loss: 0.5996\n",
            " 5/86 [>.............................] - ETA: 1:12 - loss: 2.2629 - regression_loss: 1.6453 - classification_loss: 0.6175\n",
            " 6/86 [=>............................] - ETA: 1:11 - loss: 2.2538 - regression_loss: 1.6404 - classification_loss: 0.6134\n",
            " 7/86 [=>............................] - ETA: 1:14 - loss: 2.2984 - regression_loss: 1.6607 - classification_loss: 0.6377\n",
            " 8/86 [=>............................] - ETA: 1:13 - loss: 2.2787 - regression_loss: 1.6439 - classification_loss: 0.6348\n",
            " 9/86 [==>...........................] - ETA: 1:11 - loss: 2.2843 - regression_loss: 1.6465 - classification_loss: 0.6378\n",
            "10/86 [==>...........................] - ETA: 1:11 - loss: 2.2988 - regression_loss: 1.6664 - classification_loss: 0.6324\n",
            "11/86 [==>...........................] - ETA: 1:10 - loss: 2.2870 - regression_loss: 1.6543 - classification_loss: 0.6327\n",
            "12/86 [===>..........................] - ETA: 1:09 - loss: 2.2855 - regression_loss: 1.6543 - classification_loss: 0.6312\n",
            "13/86 [===>..........................] - ETA: 1:08 - loss: 2.3022 - regression_loss: 1.6678 - classification_loss: 0.6345\n",
            "14/86 [===>..........................] - ETA: 1:06 - loss: 2.2981 - regression_loss: 1.6637 - classification_loss: 0.6344\n",
            "15/86 [====>.........................] - ETA: 1:05 - loss: 2.2814 - regression_loss: 1.6501 - classification_loss: 0.6313\n",
            "16/86 [====>.........................] - ETA: 1:05 - loss: 2.2807 - regression_loss: 1.6504 - classification_loss: 0.6303\n",
            "17/86 [====>.........................] - ETA: 1:05 - loss: 2.2737 - regression_loss: 1.6456 - classification_loss: 0.6280\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.2833 - regression_loss: 1.6553 - classification_loss: 0.6280\n",
            "19/86 [=====>........................] - ETA: 1:05 - loss: 2.2738 - regression_loss: 1.6496 - classification_loss: 0.6242\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.2674 - regression_loss: 1.6455 - classification_loss: 0.6218\n",
            "21/86 [======>.......................] - ETA: 1:03 - loss: 2.2790 - regression_loss: 1.6555 - classification_loss: 0.6234\n",
            "22/86 [======>.......................] - ETA: 1:02 - loss: 2.2819 - regression_loss: 1.6603 - classification_loss: 0.6216\n",
            "23/86 [=======>......................] - ETA: 1:11 - loss: 2.2748 - regression_loss: 1.6542 - classification_loss: 0.6206\n",
            "24/86 [=======>......................] - ETA: 1:08 - loss: 2.2643 - regression_loss: 1.6459 - classification_loss: 0.6184\n",
            "25/86 [=======>......................] - ETA: 1:08 - loss: 2.2522 - regression_loss: 1.6353 - classification_loss: 0.6169\n",
            "26/86 [========>.....................] - ETA: 1:06 - loss: 2.2490 - regression_loss: 1.6347 - classification_loss: 0.6143\n",
            "27/86 [========>.....................] - ETA: 1:05 - loss: 2.2524 - regression_loss: 1.6355 - classification_loss: 0.6169\n",
            "28/86 [========>.....................] - ETA: 1:03 - loss: 2.2606 - regression_loss: 1.6418 - classification_loss: 0.6189\n",
            "29/86 [=========>....................] - ETA: 1:01 - loss: 2.2705 - regression_loss: 1.6523 - classification_loss: 0.6182\n",
            "30/86 [=========>....................] - ETA: 1:01 - loss: 2.2594 - regression_loss: 1.6445 - classification_loss: 0.6149\n",
            "31/86 [=========>....................] - ETA: 59s - loss: 2.2676 - regression_loss: 1.6495 - classification_loss: 0.6182 \n",
            "32/86 [==========>...................] - ETA: 1:03 - loss: 2.2699 - regression_loss: 1.6519 - classification_loss: 0.6180\n",
            "33/86 [==========>...................] - ETA: 1:01 - loss: 2.2805 - regression_loss: 1.6609 - classification_loss: 0.6196\n",
            "34/86 [==========>...................] - ETA: 59s - loss: 2.2930 - regression_loss: 1.6695 - classification_loss: 0.6234 \n",
            "35/86 [===========>..................] - ETA: 58s - loss: 2.2930 - regression_loss: 1.6692 - classification_loss: 0.6238\n",
            "36/86 [===========>..................] - ETA: 57s - loss: 2.2973 - regression_loss: 1.6707 - classification_loss: 0.6265\n",
            "37/86 [===========>..................] - ETA: 55s - loss: 2.2869 - regression_loss: 1.6631 - classification_loss: 0.6237\n",
            "38/86 [============>.................] - ETA: 54s - loss: 2.2872 - regression_loss: 1.6636 - classification_loss: 0.6236\n",
            "39/86 [============>.................] - ETA: 52s - loss: 2.2993 - regression_loss: 1.6739 - classification_loss: 0.6255\n",
            "40/86 [============>.................] - ETA: 51s - loss: 2.3062 - regression_loss: 1.6805 - classification_loss: 0.6258\n",
            "41/86 [=============>................] - ETA: 49s - loss: 2.2987 - regression_loss: 1.6755 - classification_loss: 0.6232\n",
            "42/86 [=============>................] - ETA: 48s - loss: 2.2976 - regression_loss: 1.6760 - classification_loss: 0.6216\n",
            "43/86 [==============>...............] - ETA: 47s - loss: 2.2906 - regression_loss: 1.6715 - classification_loss: 0.6191\n",
            "44/86 [==============>...............] - ETA: 45s - loss: 2.2925 - regression_loss: 1.6719 - classification_loss: 0.6206\n",
            "45/86 [==============>...............] - ETA: 44s - loss: 2.2948 - regression_loss: 1.6747 - classification_loss: 0.6201\n",
            "46/86 [===============>..............] - ETA: 43s - loss: 2.2928 - regression_loss: 1.6733 - classification_loss: 0.6195\n",
            "47/86 [===============>..............] - ETA: 42s - loss: 2.2937 - regression_loss: 1.6737 - classification_loss: 0.6201\n",
            "48/86 [===============>..............] - ETA: 41s - loss: 2.2857 - regression_loss: 1.6675 - classification_loss: 0.6182\n",
            "49/86 [================>.............] - ETA: 39s - loss: 2.2833 - regression_loss: 1.6651 - classification_loss: 0.6182\n",
            "50/86 [================>.............] - ETA: 38s - loss: 2.2842 - regression_loss: 1.6662 - classification_loss: 0.6181\n",
            "51/86 [================>.............] - ETA: 37s - loss: 2.2865 - regression_loss: 1.6675 - classification_loss: 0.6190\n",
            "52/86 [=================>............] - ETA: 36s - loss: 2.2958 - regression_loss: 1.6745 - classification_loss: 0.6213\n",
            "53/86 [=================>............] - ETA: 35s - loss: 2.2948 - regression_loss: 1.6730 - classification_loss: 0.6219\n",
            "54/86 [=================>............] - ETA: 34s - loss: 2.2918 - regression_loss: 1.6705 - classification_loss: 0.6213\n",
            "55/86 [==================>...........] - ETA: 32s - loss: 2.2871 - regression_loss: 1.6669 - classification_loss: 0.6203\n",
            "56/86 [==================>...........] - ETA: 31s - loss: 2.2847 - regression_loss: 1.6660 - classification_loss: 0.6187\n",
            "57/86 [==================>...........] - ETA: 30s - loss: 2.2852 - regression_loss: 1.6661 - classification_loss: 0.6191\n",
            "58/86 [===================>..........] - ETA: 29s - loss: 2.2797 - regression_loss: 1.6623 - classification_loss: 0.6174\n",
            "59/86 [===================>..........] - ETA: 28s - loss: 2.2782 - regression_loss: 1.6615 - classification_loss: 0.6167\n",
            "60/86 [===================>..........] - ETA: 27s - loss: 2.2793 - regression_loss: 1.6629 - classification_loss: 0.6164\n",
            "61/86 [====================>.........] - ETA: 26s - loss: 2.2750 - regression_loss: 1.6573 - classification_loss: 0.6177\n",
            "62/86 [====================>.........] - ETA: 25s - loss: 2.2707 - regression_loss: 1.6532 - classification_loss: 0.6175\n",
            "63/86 [====================>.........] - ETA: 24s - loss: 2.2732 - regression_loss: 1.6558 - classification_loss: 0.6174\n",
            "64/86 [=====================>........] - ETA: 23s - loss: 2.2728 - regression_loss: 1.6551 - classification_loss: 0.6178\n",
            "65/86 [=====================>........] - ETA: 22s - loss: 2.2761 - regression_loss: 1.6571 - classification_loss: 0.6190\n",
            "66/86 [======================>.......] - ETA: 21s - loss: 2.2801 - regression_loss: 1.6606 - classification_loss: 0.6195\n",
            "67/86 [======================>.......] - ETA: 19s - loss: 2.2800 - regression_loss: 1.6614 - classification_loss: 0.6186\n",
            "68/86 [======================>.......] - ETA: 18s - loss: 2.2823 - regression_loss: 1.6630 - classification_loss: 0.6192\n",
            "69/86 [=======================>......] - ETA: 17s - loss: 2.2846 - regression_loss: 1.6649 - classification_loss: 0.6196\n",
            "70/86 [=======================>......] - ETA: 16s - loss: 2.2841 - regression_loss: 1.6653 - classification_loss: 0.6188\n",
            "71/86 [=======================>......] - ETA: 15s - loss: 2.2849 - regression_loss: 1.6662 - classification_loss: 0.6187\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.2871 - regression_loss: 1.6683 - classification_loss: 0.6188\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.2870 - regression_loss: 1.6683 - classification_loss: 0.6187\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.2878 - regression_loss: 1.6697 - classification_loss: 0.6181\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.2873 - regression_loss: 1.6698 - classification_loss: 0.6175\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.2896 - regression_loss: 1.6712 - classification_loss: 0.6184\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.2869 - regression_loss: 1.6688 - classification_loss: 0.6181 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.2834 - regression_loss: 1.6656 - classification_loss: 0.6178\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.2831 - regression_loss: 1.6647 - classification_loss: 0.6184\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.2785 - regression_loss: 1.6614 - classification_loss: 0.6171\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.2810 - regression_loss: 1.6647 - classification_loss: 0.6163\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.2798 - regression_loss: 1.6636 - classification_loss: 0.6161\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.2790 - regression_loss: 1.6630 - classification_loss: 0.6159\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.2804 - regression_loss: 1.6647 - classification_loss: 0.6157\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.2810 - regression_loss: 1.6652 - classification_loss: 0.6158\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.2770 - regression_loss: 1.6622 - classification_loss: 0.6148\n",
            "Epoch 4: saving model to ./snapshots\\resnet50_csv_04.h5\n",
            "\n",
            "86/86 [==============================] - 89s 1s/step - loss: 2.2770 - regression_loss: 1.6622 - classification_loss: 0.6148 - lr: 1.0000e-05\n",
            "Epoch 5/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:06 - loss: 2.0881 - regression_loss: 1.5270 - classification_loss: 0.5611\n",
            " 2/86 [..............................] - ETA: 1:04 - loss: 2.0691 - regression_loss: 1.4952 - classification_loss: 0.5739\n",
            " 3/86 [>.............................] - ETA: 1:11 - loss: 2.1004 - regression_loss: 1.5298 - classification_loss: 0.5707\n",
            " 4/86 [>.............................] - ETA: 1:11 - loss: 2.2028 - regression_loss: 1.6090 - classification_loss: 0.5937\n",
            " 5/86 [>.............................] - ETA: 1:09 - loss: 2.2561 - regression_loss: 1.6621 - classification_loss: 0.5940\n",
            " 6/86 [=>............................] - ETA: 1:09 - loss: 2.2950 - regression_loss: 1.6908 - classification_loss: 0.6042\n",
            " 7/86 [=>............................] - ETA: 1:11 - loss: 2.3183 - regression_loss: 1.7048 - classification_loss: 0.6135\n",
            " 8/86 [=>............................] - ETA: 1:12 - loss: 2.2722 - regression_loss: 1.6708 - classification_loss: 0.6014\n",
            " 9/86 [==>...........................] - ETA: 1:12 - loss: 2.2877 - regression_loss: 1.6868 - classification_loss: 0.6009\n",
            "10/86 [==>...........................] - ETA: 1:09 - loss: 2.2790 - regression_loss: 1.6763 - classification_loss: 0.6026\n",
            "11/86 [==>...........................] - ETA: 1:08 - loss: 2.2758 - regression_loss: 1.6742 - classification_loss: 0.6015\n",
            "12/86 [===>..........................] - ETA: 1:07 - loss: 2.2555 - regression_loss: 1.6580 - classification_loss: 0.5975\n",
            "13/86 [===>..........................] - ETA: 1:08 - loss: 2.2728 - regression_loss: 1.6760 - classification_loss: 0.5968\n",
            "14/86 [===>..........................] - ETA: 1:23 - loss: 2.2849 - regression_loss: 1.6893 - classification_loss: 0.5956\n",
            "15/86 [====>.........................] - ETA: 1:18 - loss: 2.2678 - regression_loss: 1.6755 - classification_loss: 0.5923\n",
            "16/86 [====>.........................] - ETA: 1:16 - loss: 2.2564 - regression_loss: 1.6638 - classification_loss: 0.5926\n",
            "17/86 [====>.........................] - ETA: 1:14 - loss: 2.2585 - regression_loss: 1.6621 - classification_loss: 0.5964\n",
            "18/86 [=====>........................] - ETA: 1:13 - loss: 2.2497 - regression_loss: 1.6549 - classification_loss: 0.5949\n",
            "19/86 [=====>........................] - ETA: 1:11 - loss: 2.2457 - regression_loss: 1.6541 - classification_loss: 0.5916\n",
            "20/86 [=====>........................] - ETA: 1:10 - loss: 2.2476 - regression_loss: 1.6545 - classification_loss: 0.5931\n",
            "21/86 [======>.......................] - ETA: 1:09 - loss: 2.2511 - regression_loss: 1.6561 - classification_loss: 0.5950\n",
            "22/86 [======>.......................] - ETA: 1:07 - loss: 2.2549 - regression_loss: 1.6586 - classification_loss: 0.5962\n",
            "23/86 [=======>......................] - ETA: 1:08 - loss: 2.2636 - regression_loss: 1.6679 - classification_loss: 0.5957\n",
            "24/86 [=======>......................] - ETA: 1:06 - loss: 2.2574 - regression_loss: 1.6609 - classification_loss: 0.5965\n",
            "25/86 [=======>......................] - ETA: 1:04 - loss: 2.2573 - regression_loss: 1.6613 - classification_loss: 0.5960\n",
            "26/86 [========>.....................] - ETA: 1:02 - loss: 2.2569 - regression_loss: 1.6586 - classification_loss: 0.5982\n",
            "27/86 [========>.....................] - ETA: 1:01 - loss: 2.2578 - regression_loss: 1.6583 - classification_loss: 0.5995\n",
            "28/86 [========>.....................] - ETA: 1:00 - loss: 2.2537 - regression_loss: 1.6566 - classification_loss: 0.5972\n",
            "29/86 [=========>....................] - ETA: 58s - loss: 2.2452 - regression_loss: 1.6513 - classification_loss: 0.5939 \n",
            "30/86 [=========>....................] - ETA: 58s - loss: 2.2450 - regression_loss: 1.6520 - classification_loss: 0.5931\n",
            "31/86 [=========>....................] - ETA: 1:01 - loss: 2.2490 - regression_loss: 1.6523 - classification_loss: 0.5966\n",
            "32/86 [==========>...................] - ETA: 59s - loss: 2.2441 - regression_loss: 1.6489 - classification_loss: 0.5951 \n",
            "33/86 [==========>...................] - ETA: 57s - loss: 2.2506 - regression_loss: 1.6548 - classification_loss: 0.5959\n",
            "34/86 [==========>...................] - ETA: 56s - loss: 2.2586 - regression_loss: 1.6590 - classification_loss: 0.5996\n",
            "35/86 [===========>..................] - ETA: 55s - loss: 2.2558 - regression_loss: 1.6554 - classification_loss: 0.6004\n",
            "36/86 [===========>..................] - ETA: 54s - loss: 2.2537 - regression_loss: 1.6530 - classification_loss: 0.6007\n",
            "37/86 [===========>..................] - ETA: 52s - loss: 2.2450 - regression_loss: 1.6456 - classification_loss: 0.5995\n",
            "38/86 [============>.................] - ETA: 51s - loss: 2.2477 - regression_loss: 1.6465 - classification_loss: 0.6013\n",
            "39/86 [============>.................] - ETA: 50s - loss: 2.2539 - regression_loss: 1.6509 - classification_loss: 0.6030\n",
            "40/86 [============>.................] - ETA: 49s - loss: 2.2540 - regression_loss: 1.6501 - classification_loss: 0.6039\n",
            "41/86 [=============>................] - ETA: 48s - loss: 2.2548 - regression_loss: 1.6511 - classification_loss: 0.6037\n",
            "42/86 [=============>................] - ETA: 47s - loss: 2.2478 - regression_loss: 1.6451 - classification_loss: 0.6027\n",
            "43/86 [==============>...............] - ETA: 45s - loss: 2.2397 - regression_loss: 1.6391 - classification_loss: 0.6007\n",
            "44/86 [==============>...............] - ETA: 44s - loss: 2.2429 - regression_loss: 1.6418 - classification_loss: 0.6011\n",
            "45/86 [==============>...............] - ETA: 43s - loss: 2.2432 - regression_loss: 1.6427 - classification_loss: 0.6004\n",
            "46/86 [===============>..............] - ETA: 42s - loss: 2.2456 - regression_loss: 1.6439 - classification_loss: 0.6018\n",
            "47/86 [===============>..............] - ETA: 41s - loss: 2.2467 - regression_loss: 1.6450 - classification_loss: 0.6017\n",
            "48/86 [===============>..............] - ETA: 39s - loss: 2.2449 - regression_loss: 1.6430 - classification_loss: 0.6019\n",
            "49/86 [================>.............] - ETA: 38s - loss: 2.2460 - regression_loss: 1.6447 - classification_loss: 0.6013\n",
            "50/86 [================>.............] - ETA: 37s - loss: 2.2471 - regression_loss: 1.6459 - classification_loss: 0.6012\n",
            "51/86 [================>.............] - ETA: 36s - loss: 2.2470 - regression_loss: 1.6459 - classification_loss: 0.6011\n",
            "52/86 [=================>............] - ETA: 35s - loss: 2.2417 - regression_loss: 1.6405 - classification_loss: 0.6012\n",
            "53/86 [=================>............] - ETA: 34s - loss: 2.2435 - regression_loss: 1.6419 - classification_loss: 0.6016\n",
            "54/86 [=================>............] - ETA: 33s - loss: 2.2487 - regression_loss: 1.6466 - classification_loss: 0.6021\n",
            "55/86 [==================>...........] - ETA: 32s - loss: 2.2462 - regression_loss: 1.6430 - classification_loss: 0.6032\n",
            "56/86 [==================>...........] - ETA: 32s - loss: 2.2381 - regression_loss: 1.6363 - classification_loss: 0.6018\n",
            "57/86 [==================>...........] - ETA: 30s - loss: 2.2463 - regression_loss: 1.6413 - classification_loss: 0.6050\n",
            "58/86 [===================>..........] - ETA: 29s - loss: 2.2461 - regression_loss: 1.6420 - classification_loss: 0.6041\n",
            "59/86 [===================>..........] - ETA: 28s - loss: 2.2530 - regression_loss: 1.6472 - classification_loss: 0.6058\n",
            "60/86 [===================>..........] - ETA: 27s - loss: 2.2517 - regression_loss: 1.6463 - classification_loss: 0.6054\n",
            "61/86 [====================>.........] - ETA: 26s - loss: 2.2527 - regression_loss: 1.6463 - classification_loss: 0.6064\n",
            "62/86 [====================>.........] - ETA: 25s - loss: 2.2509 - regression_loss: 1.6454 - classification_loss: 0.6055\n",
            "63/86 [====================>.........] - ETA: 24s - loss: 2.2465 - regression_loss: 1.6425 - classification_loss: 0.6040\n",
            "64/86 [=====================>........] - ETA: 23s - loss: 2.2458 - regression_loss: 1.6418 - classification_loss: 0.6040\n",
            "65/86 [=====================>........] - ETA: 21s - loss: 2.2457 - regression_loss: 1.6426 - classification_loss: 0.6031\n",
            "66/86 [======================>.......] - ETA: 20s - loss: 2.2457 - regression_loss: 1.6429 - classification_loss: 0.6028\n",
            "67/86 [======================>.......] - ETA: 19s - loss: 2.2490 - regression_loss: 1.6471 - classification_loss: 0.6020\n",
            "68/86 [======================>.......] - ETA: 18s - loss: 2.2490 - regression_loss: 1.6471 - classification_loss: 0.6019\n",
            "69/86 [=======================>......] - ETA: 17s - loss: 2.2491 - regression_loss: 1.6469 - classification_loss: 0.6021\n",
            "70/86 [=======================>......] - ETA: 16s - loss: 2.2478 - regression_loss: 1.6461 - classification_loss: 0.6017\n",
            "71/86 [=======================>......] - ETA: 15s - loss: 2.2453 - regression_loss: 1.6448 - classification_loss: 0.6006\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.2446 - regression_loss: 1.6442 - classification_loss: 0.6004\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.2452 - regression_loss: 1.6442 - classification_loss: 0.6010\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.2451 - regression_loss: 1.6445 - classification_loss: 0.6006\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.2463 - regression_loss: 1.6450 - classification_loss: 0.6013\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.2434 - regression_loss: 1.6429 - classification_loss: 0.6005\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.2424 - regression_loss: 1.6419 - classification_loss: 0.6005 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.2394 - regression_loss: 1.6394 - classification_loss: 0.5999\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.2370 - regression_loss: 1.6375 - classification_loss: 0.5996\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.2358 - regression_loss: 1.6366 - classification_loss: 0.5992\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.2361 - regression_loss: 1.6371 - classification_loss: 0.5990\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.2307 - regression_loss: 1.6332 - classification_loss: 0.5975\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.2312 - regression_loss: 1.6337 - classification_loss: 0.5975\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.2325 - regression_loss: 1.6352 - classification_loss: 0.5973\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.2339 - regression_loss: 1.6363 - classification_loss: 0.5975\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.2336 - regression_loss: 1.6365 - classification_loss: 0.5971\n",
            "Epoch 5: saving model to ./snapshots\\resnet50_csv_05.h5\n",
            "\n",
            "86/86 [==============================] - 91s 1s/step - loss: 2.2336 - regression_loss: 1.6365 - classification_loss: 0.5971 - lr: 1.0000e-05\n",
            "Epoch 6/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:52 - loss: 2.0121 - regression_loss: 1.4333 - classification_loss: 0.5788\n",
            " 2/86 [..............................] - ETA: 1:23 - loss: 2.1198 - regression_loss: 1.5284 - classification_loss: 0.5914\n",
            " 3/86 [>.............................] - ETA: 1:15 - loss: 2.1567 - regression_loss: 1.5501 - classification_loss: 0.6067\n",
            " 4/86 [>.............................] - ETA: 1:26 - loss: 2.2056 - regression_loss: 1.5845 - classification_loss: 0.6212\n",
            " 5/86 [>.............................] - ETA: 1:29 - loss: 2.1478 - regression_loss: 1.5463 - classification_loss: 0.6015\n",
            " 6/86 [=>............................] - ETA: 1:26 - loss: 2.1817 - regression_loss: 1.5715 - classification_loss: 0.6102\n",
            " 7/86 [=>............................] - ETA: 1:27 - loss: 2.2047 - regression_loss: 1.5967 - classification_loss: 0.6080\n",
            " 8/86 [=>............................] - ETA: 1:24 - loss: 2.2158 - regression_loss: 1.6096 - classification_loss: 0.6062\n",
            " 9/86 [==>...........................] - ETA: 1:24 - loss: 2.2178 - regression_loss: 1.6109 - classification_loss: 0.6069\n",
            "10/86 [==>...........................] - ETA: 1:21 - loss: 2.2369 - regression_loss: 1.6346 - classification_loss: 0.6023\n",
            "11/86 [==>...........................] - ETA: 1:18 - loss: 2.2140 - regression_loss: 1.6211 - classification_loss: 0.5929\n",
            "12/86 [===>..........................] - ETA: 1:16 - loss: 2.2259 - regression_loss: 1.6356 - classification_loss: 0.5903\n",
            "13/86 [===>..........................] - ETA: 1:15 - loss: 2.2316 - regression_loss: 1.6432 - classification_loss: 0.5885\n",
            "14/86 [===>..........................] - ETA: 1:15 - loss: 2.2416 - regression_loss: 1.6523 - classification_loss: 0.5894\n",
            "15/86 [====>.........................] - ETA: 1:14 - loss: 2.2822 - regression_loss: 1.6859 - classification_loss: 0.5963\n",
            "16/86 [====>.........................] - ETA: 1:11 - loss: 2.2670 - regression_loss: 1.6720 - classification_loss: 0.5950\n",
            "17/86 [====>.........................] - ETA: 1:11 - loss: 2.2677 - regression_loss: 1.6752 - classification_loss: 0.5924\n",
            "18/86 [=====>........................] - ETA: 1:09 - loss: 2.2667 - regression_loss: 1.6728 - classification_loss: 0.5939\n",
            "19/86 [=====>........................] - ETA: 1:09 - loss: 2.2686 - regression_loss: 1.6758 - classification_loss: 0.5928\n",
            "20/86 [=====>........................] - ETA: 1:08 - loss: 2.2820 - regression_loss: 1.6878 - classification_loss: 0.5942\n",
            "21/86 [======>.......................] - ETA: 1:05 - loss: 2.2855 - regression_loss: 1.6931 - classification_loss: 0.5924\n",
            "22/86 [======>.......................] - ETA: 1:05 - loss: 2.2821 - regression_loss: 1.6901 - classification_loss: 0.5920\n",
            "23/86 [=======>......................] - ETA: 1:04 - loss: 2.2911 - regression_loss: 1.6972 - classification_loss: 0.5939\n",
            "24/86 [=======>......................] - ETA: 1:03 - loss: 2.2819 - regression_loss: 1.6898 - classification_loss: 0.5921\n",
            "25/86 [=======>......................] - ETA: 1:02 - loss: 2.2843 - regression_loss: 1.6918 - classification_loss: 0.5925\n",
            "26/86 [========>.....................] - ETA: 1:00 - loss: 2.2952 - regression_loss: 1.6979 - classification_loss: 0.5973\n",
            "27/86 [========>.....................] - ETA: 59s - loss: 2.2910 - regression_loss: 1.6935 - classification_loss: 0.5975 \n",
            "28/86 [========>.....................] - ETA: 59s - loss: 2.2881 - regression_loss: 1.6900 - classification_loss: 0.5980\n",
            "29/86 [=========>....................] - ETA: 1:03 - loss: 2.2963 - regression_loss: 1.6988 - classification_loss: 0.5975\n",
            "30/86 [=========>....................] - ETA: 1:01 - loss: 2.2969 - regression_loss: 1.6981 - classification_loss: 0.5988\n",
            "31/86 [=========>....................] - ETA: 59s - loss: 2.2950 - regression_loss: 1.6961 - classification_loss: 0.5990 \n",
            "32/86 [==========>...................] - ETA: 58s - loss: 2.2930 - regression_loss: 1.6959 - classification_loss: 0.5971\n",
            "33/86 [==========>...................] - ETA: 57s - loss: 2.2885 - regression_loss: 1.6915 - classification_loss: 0.5970\n",
            "34/86 [==========>...................] - ETA: 56s - loss: 2.2904 - regression_loss: 1.6948 - classification_loss: 0.5956\n",
            "35/86 [===========>..................] - ETA: 55s - loss: 2.2944 - regression_loss: 1.6966 - classification_loss: 0.5978\n",
            "36/86 [===========>..................] - ETA: 53s - loss: 2.2857 - regression_loss: 1.6890 - classification_loss: 0.5967\n",
            "37/86 [===========>..................] - ETA: 52s - loss: 2.2789 - regression_loss: 1.6836 - classification_loss: 0.5952\n",
            "38/86 [============>.................] - ETA: 51s - loss: 2.2791 - regression_loss: 1.6842 - classification_loss: 0.5950\n",
            "39/86 [============>.................] - ETA: 50s - loss: 2.2722 - regression_loss: 1.6777 - classification_loss: 0.5945\n",
            "40/86 [============>.................] - ETA: 48s - loss: 2.2776 - regression_loss: 1.6823 - classification_loss: 0.5953\n",
            "41/86 [=============>................] - ETA: 47s - loss: 2.2743 - regression_loss: 1.6789 - classification_loss: 0.5954\n",
            "42/86 [=============>................] - ETA: 46s - loss: 2.2754 - regression_loss: 1.6779 - classification_loss: 0.5975\n",
            "43/86 [==============>...............] - ETA: 48s - loss: 2.2814 - regression_loss: 1.6799 - classification_loss: 0.6015\n",
            "44/86 [==============>...............] - ETA: 46s - loss: 2.2804 - regression_loss: 1.6794 - classification_loss: 0.6010\n",
            "45/86 [==============>...............] - ETA: 45s - loss: 2.2745 - regression_loss: 1.6753 - classification_loss: 0.5991\n",
            "46/86 [===============>..............] - ETA: 43s - loss: 2.2738 - regression_loss: 1.6751 - classification_loss: 0.5987\n",
            "47/86 [===============>..............] - ETA: 42s - loss: 2.2696 - regression_loss: 1.6710 - classification_loss: 0.5985\n",
            "48/86 [===============>..............] - ETA: 41s - loss: 2.2729 - regression_loss: 1.6749 - classification_loss: 0.5980\n",
            "49/86 [================>.............] - ETA: 40s - loss: 2.2723 - regression_loss: 1.6739 - classification_loss: 0.5983\n",
            "50/86 [================>.............] - ETA: 39s - loss: 2.2728 - regression_loss: 1.6736 - classification_loss: 0.5992\n",
            "51/86 [================>.............] - ETA: 37s - loss: 2.2693 - regression_loss: 1.6695 - classification_loss: 0.5998\n",
            "52/86 [=================>............] - ETA: 36s - loss: 2.2633 - regression_loss: 1.6658 - classification_loss: 0.5975\n",
            "53/86 [=================>............] - ETA: 35s - loss: 2.2610 - regression_loss: 1.6636 - classification_loss: 0.5974\n",
            "54/86 [=================>............] - ETA: 34s - loss: 2.2620 - regression_loss: 1.6649 - classification_loss: 0.5971\n",
            "55/86 [==================>...........] - ETA: 32s - loss: 2.2622 - regression_loss: 1.6651 - classification_loss: 0.5971\n",
            "56/86 [==================>...........] - ETA: 31s - loss: 2.2605 - regression_loss: 1.6633 - classification_loss: 0.5972\n",
            "57/86 [==================>...........] - ETA: 30s - loss: 2.2575 - regression_loss: 1.6612 - classification_loss: 0.5963\n",
            "58/86 [===================>..........] - ETA: 29s - loss: 2.2552 - regression_loss: 1.6594 - classification_loss: 0.5958\n",
            "59/86 [===================>..........] - ETA: 28s - loss: 2.2538 - regression_loss: 1.6579 - classification_loss: 0.5960\n",
            "60/86 [===================>..........] - ETA: 27s - loss: 2.2555 - regression_loss: 1.6596 - classification_loss: 0.5960\n",
            "61/86 [====================>.........] - ETA: 26s - loss: 2.2603 - regression_loss: 1.6639 - classification_loss: 0.5964\n",
            "62/86 [====================>.........] - ETA: 25s - loss: 2.2548 - regression_loss: 1.6602 - classification_loss: 0.5946\n",
            "63/86 [====================>.........] - ETA: 23s - loss: 2.2532 - regression_loss: 1.6600 - classification_loss: 0.5932\n",
            "64/86 [=====================>........] - ETA: 22s - loss: 2.2523 - regression_loss: 1.6603 - classification_loss: 0.5920\n",
            "65/86 [=====================>........] - ETA: 21s - loss: 2.2515 - regression_loss: 1.6597 - classification_loss: 0.5918\n",
            "66/86 [======================>.......] - ETA: 20s - loss: 2.2506 - regression_loss: 1.6590 - classification_loss: 0.5916\n",
            "67/86 [======================>.......] - ETA: 19s - loss: 2.2495 - regression_loss: 1.6582 - classification_loss: 0.5913\n",
            "68/86 [======================>.......] - ETA: 18s - loss: 2.2470 - regression_loss: 1.6570 - classification_loss: 0.5900\n",
            "69/86 [=======================>......] - ETA: 17s - loss: 2.2481 - regression_loss: 1.6573 - classification_loss: 0.5908\n",
            "70/86 [=======================>......] - ETA: 16s - loss: 2.2503 - regression_loss: 1.6591 - classification_loss: 0.5912\n",
            "71/86 [=======================>......] - ETA: 15s - loss: 2.2468 - regression_loss: 1.6567 - classification_loss: 0.5901\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.2481 - regression_loss: 1.6562 - classification_loss: 0.5919\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.2457 - regression_loss: 1.6549 - classification_loss: 0.5909\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.2506 - regression_loss: 1.6591 - classification_loss: 0.5916\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.2473 - regression_loss: 1.6567 - classification_loss: 0.5906\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.2488 - regression_loss: 1.6582 - classification_loss: 0.5907\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.2528 - regression_loss: 1.6617 - classification_loss: 0.5911 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.2509 - regression_loss: 1.6609 - classification_loss: 0.5899\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.2470 - regression_loss: 1.6579 - classification_loss: 0.5891\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.2477 - regression_loss: 1.6579 - classification_loss: 0.5898\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.2480 - regression_loss: 1.6579 - classification_loss: 0.5902\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.2483 - regression_loss: 1.6584 - classification_loss: 0.5899\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.2539 - regression_loss: 1.6632 - classification_loss: 0.5908\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.2501 - regression_loss: 1.6606 - classification_loss: 0.5894\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.2517 - regression_loss: 1.6623 - classification_loss: 0.5894\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.2522 - regression_loss: 1.6624 - classification_loss: 0.5898\n",
            "Epoch 6: saving model to ./snapshots\\resnet50_csv_06.h5\n",
            "\n",
            "86/86 [==============================] - 88s 1s/step - loss: 2.2522 - regression_loss: 1.6624 - classification_loss: 0.5898 - lr: 1.0000e-05\n",
            "Epoch 7/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:37 - loss: 2.1960 - regression_loss: 1.6667 - classification_loss: 0.5293\n",
            " 2/86 [..............................] - ETA: 52s - loss: 2.1881 - regression_loss: 1.6405 - classification_loss: 0.5476 \n",
            " 3/86 [>.............................] - ETA: 1:00 - loss: 2.1861 - regression_loss: 1.6069 - classification_loss: 0.5793\n",
            " 4/86 [>.............................] - ETA: 1:06 - loss: 2.1268 - regression_loss: 1.5595 - classification_loss: 0.5673\n",
            " 5/86 [>.............................] - ETA: 1:08 - loss: 2.1865 - regression_loss: 1.6101 - classification_loss: 0.5763\n",
            " 6/86 [=>............................] - ETA: 1:06 - loss: 2.1829 - regression_loss: 1.6044 - classification_loss: 0.5786\n",
            " 7/86 [=>............................] - ETA: 1:06 - loss: 2.1660 - regression_loss: 1.5850 - classification_loss: 0.5810\n",
            " 8/86 [=>............................] - ETA: 1:06 - loss: 2.1474 - regression_loss: 1.5780 - classification_loss: 0.5694\n",
            " 9/86 [==>...........................] - ETA: 1:06 - loss: 2.1479 - regression_loss: 1.5737 - classification_loss: 0.5742\n",
            "10/86 [==>...........................] - ETA: 1:07 - loss: 2.1463 - regression_loss: 1.5783 - classification_loss: 0.5680\n",
            "11/86 [==>...........................] - ETA: 1:05 - loss: 2.1568 - regression_loss: 1.5839 - classification_loss: 0.5729\n",
            "12/86 [===>..........................] - ETA: 1:07 - loss: 2.1615 - regression_loss: 1.5847 - classification_loss: 0.5768\n",
            "13/86 [===>..........................] - ETA: 1:06 - loss: 2.1611 - regression_loss: 1.5894 - classification_loss: 0.5717\n",
            "14/86 [===>..........................] - ETA: 1:04 - loss: 2.1663 - regression_loss: 1.5939 - classification_loss: 0.5724\n",
            "15/86 [====>.........................] - ETA: 1:03 - loss: 2.1722 - regression_loss: 1.5995 - classification_loss: 0.5728\n",
            "16/86 [====>.........................] - ETA: 1:03 - loss: 2.1708 - regression_loss: 1.5962 - classification_loss: 0.5746\n",
            "17/86 [====>.........................] - ETA: 1:02 - loss: 2.1551 - regression_loss: 1.5821 - classification_loss: 0.5730\n",
            "18/86 [=====>........................] - ETA: 1:01 - loss: 2.1679 - regression_loss: 1.5922 - classification_loss: 0.5757\n",
            "19/86 [=====>........................] - ETA: 59s - loss: 2.1566 - regression_loss: 1.5836 - classification_loss: 0.5730 \n",
            "20/86 [=====>........................] - ETA: 58s - loss: 2.1689 - regression_loss: 1.5942 - classification_loss: 0.5748\n",
            "21/86 [======>.......................] - ETA: 58s - loss: 2.1673 - regression_loss: 1.5929 - classification_loss: 0.5744\n",
            "22/86 [======>.......................] - ETA: 57s - loss: 2.1640 - regression_loss: 1.5897 - classification_loss: 0.5743\n",
            "23/86 [=======>......................] - ETA: 56s - loss: 2.1850 - regression_loss: 1.6058 - classification_loss: 0.5792\n",
            "24/86 [=======>......................] - ETA: 55s - loss: 2.1798 - regression_loss: 1.6012 - classification_loss: 0.5787\n",
            "25/86 [=======>......................] - ETA: 54s - loss: 2.1845 - regression_loss: 1.6036 - classification_loss: 0.5808\n",
            "26/86 [========>.....................] - ETA: 53s - loss: 2.1853 - regression_loss: 1.6049 - classification_loss: 0.5804\n",
            "27/86 [========>.....................] - ETA: 53s - loss: 2.1648 - regression_loss: 1.5895 - classification_loss: 0.5752\n",
            "28/86 [========>.....................] - ETA: 51s - loss: 2.1672 - regression_loss: 1.5911 - classification_loss: 0.5761\n",
            "29/86 [=========>....................] - ETA: 51s - loss: 2.1720 - regression_loss: 1.5953 - classification_loss: 0.5766\n",
            "30/86 [=========>....................] - ETA: 50s - loss: 2.1687 - regression_loss: 1.5941 - classification_loss: 0.5746\n",
            "31/86 [=========>....................] - ETA: 49s - loss: 2.1728 - regression_loss: 1.5976 - classification_loss: 0.5752\n",
            "32/86 [==========>...................] - ETA: 48s - loss: 2.1763 - regression_loss: 1.6006 - classification_loss: 0.5757\n",
            "33/86 [==========>...................] - ETA: 47s - loss: 2.1781 - regression_loss: 1.6009 - classification_loss: 0.5773\n",
            "34/86 [==========>...................] - ETA: 46s - loss: 2.1701 - regression_loss: 1.5952 - classification_loss: 0.5749\n",
            "35/86 [===========>..................] - ETA: 46s - loss: 2.1748 - regression_loss: 1.6002 - classification_loss: 0.5746\n",
            "36/86 [===========>..................] - ETA: 45s - loss: 2.1765 - regression_loss: 1.6019 - classification_loss: 0.5747\n",
            "37/86 [===========>..................] - ETA: 44s - loss: 2.1851 - regression_loss: 1.6087 - classification_loss: 0.5764\n",
            "38/86 [============>.................] - ETA: 43s - loss: 2.1783 - regression_loss: 1.6034 - classification_loss: 0.5749\n",
            "39/86 [============>.................] - ETA: 43s - loss: 2.1786 - regression_loss: 1.6035 - classification_loss: 0.5751\n",
            "40/86 [============>.................] - ETA: 42s - loss: 2.1809 - regression_loss: 1.6057 - classification_loss: 0.5751\n",
            "41/86 [=============>................] - ETA: 41s - loss: 2.1810 - regression_loss: 1.6055 - classification_loss: 0.5755\n",
            "42/86 [=============>................] - ETA: 40s - loss: 2.1827 - regression_loss: 1.6068 - classification_loss: 0.5759\n",
            "43/86 [==============>...............] - ETA: 39s - loss: 2.1755 - regression_loss: 1.6005 - classification_loss: 0.5749\n",
            "44/86 [==============>...............] - ETA: 38s - loss: 2.1781 - regression_loss: 1.6039 - classification_loss: 0.5742\n",
            "45/86 [==============>...............] - ETA: 37s - loss: 2.1759 - regression_loss: 1.6014 - classification_loss: 0.5745\n",
            "46/86 [===============>..............] - ETA: 36s - loss: 2.1797 - regression_loss: 1.6056 - classification_loss: 0.5741\n",
            "47/86 [===============>..............] - ETA: 35s - loss: 2.1764 - regression_loss: 1.6027 - classification_loss: 0.5737\n",
            "48/86 [===============>..............] - ETA: 34s - loss: 2.1780 - regression_loss: 1.6037 - classification_loss: 0.5743\n",
            "49/86 [================>.............] - ETA: 33s - loss: 2.1828 - regression_loss: 1.6077 - classification_loss: 0.5750\n",
            "50/86 [================>.............] - ETA: 33s - loss: 2.1835 - regression_loss: 1.6094 - classification_loss: 0.5741\n",
            "51/86 [================>.............] - ETA: 32s - loss: 2.1878 - regression_loss: 1.6126 - classification_loss: 0.5752\n",
            "52/86 [=================>............] - ETA: 31s - loss: 2.1822 - regression_loss: 1.6087 - classification_loss: 0.5735\n",
            "53/86 [=================>............] - ETA: 30s - loss: 2.1787 - regression_loss: 1.6055 - classification_loss: 0.5732\n",
            "54/86 [=================>............] - ETA: 29s - loss: 2.1754 - regression_loss: 1.6031 - classification_loss: 0.5723\n",
            "55/86 [==================>...........] - ETA: 28s - loss: 2.1771 - regression_loss: 1.6053 - classification_loss: 0.5718\n",
            "56/86 [==================>...........] - ETA: 27s - loss: 2.1735 - regression_loss: 1.6029 - classification_loss: 0.5706\n",
            "57/86 [==================>...........] - ETA: 26s - loss: 2.1762 - regression_loss: 1.6053 - classification_loss: 0.5709\n",
            "58/86 [===================>..........] - ETA: 25s - loss: 2.1825 - regression_loss: 1.6108 - classification_loss: 0.5717\n",
            "59/86 [===================>..........] - ETA: 24s - loss: 2.1835 - regression_loss: 1.6119 - classification_loss: 0.5716\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1854 - regression_loss: 1.6131 - classification_loss: 0.5724\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1887 - regression_loss: 1.6154 - classification_loss: 0.5733\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1879 - regression_loss: 1.6138 - classification_loss: 0.5741\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1860 - regression_loss: 1.6121 - classification_loss: 0.5740\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1868 - regression_loss: 1.6133 - classification_loss: 0.5735\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1865 - regression_loss: 1.6129 - classification_loss: 0.5737\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.1892 - regression_loss: 1.6158 - classification_loss: 0.5734\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.1922 - regression_loss: 1.6190 - classification_loss: 0.5732\n",
            "68/86 [======================>.......] - ETA: 16s - loss: 2.1914 - regression_loss: 1.6181 - classification_loss: 0.5733\n",
            "69/86 [=======================>......] - ETA: 15s - loss: 2.1916 - regression_loss: 1.6181 - classification_loss: 0.5735\n",
            "70/86 [=======================>......] - ETA: 14s - loss: 2.1920 - regression_loss: 1.6183 - classification_loss: 0.5737\n",
            "71/86 [=======================>......] - ETA: 13s - loss: 2.1912 - regression_loss: 1.6173 - classification_loss: 0.5739\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1951 - regression_loss: 1.6195 - classification_loss: 0.5756\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1962 - regression_loss: 1.6205 - classification_loss: 0.5757\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.2023 - regression_loss: 1.6257 - classification_loss: 0.5766\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.2047 - regression_loss: 1.6279 - classification_loss: 0.5768\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.2070 - regression_loss: 1.6301 - classification_loss: 0.5769 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.2045 - regression_loss: 1.6280 - classification_loss: 0.5764\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.2045 - regression_loss: 1.6283 - classification_loss: 0.5762\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.2017 - regression_loss: 1.6264 - classification_loss: 0.5753\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.2019 - regression_loss: 1.6266 - classification_loss: 0.5753\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1960 - regression_loss: 1.6223 - classification_loss: 0.5737\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1940 - regression_loss: 1.6203 - classification_loss: 0.5736\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1926 - regression_loss: 1.6194 - classification_loss: 0.5732\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1934 - regression_loss: 1.6201 - classification_loss: 0.5733\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1925 - regression_loss: 1.6195 - classification_loss: 0.5730\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1924 - regression_loss: 1.6188 - classification_loss: 0.5735\n",
            "Epoch 7: saving model to ./snapshots\\resnet50_csv_07.h5\n",
            "\n",
            "86/86 [==============================] - 82s 941ms/step - loss: 2.1924 - regression_loss: 1.6188 - classification_loss: 0.5735 - lr: 1.0000e-05\n",
            "Epoch 8/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:50 - loss: 2.1229 - regression_loss: 1.5938 - classification_loss: 0.5291\n",
            " 2/86 [..............................] - ETA: 1:54 - loss: 2.1488 - regression_loss: 1.6213 - classification_loss: 0.5274\n",
            " 3/86 [>.............................] - ETA: 1:36 - loss: 2.1282 - regression_loss: 1.6036 - classification_loss: 0.5246\n",
            " 4/86 [>.............................] - ETA: 1:25 - loss: 2.1020 - regression_loss: 1.5634 - classification_loss: 0.5386\n",
            " 5/86 [>.............................] - ETA: 1:22 - loss: 2.0986 - regression_loss: 1.5583 - classification_loss: 0.5402\n",
            " 6/86 [=>............................] - ETA: 1:18 - loss: 2.0928 - regression_loss: 1.5580 - classification_loss: 0.5348\n",
            " 7/86 [=>............................] - ETA: 1:15 - loss: 2.1147 - regression_loss: 1.5792 - classification_loss: 0.5356\n",
            " 8/86 [=>............................] - ETA: 1:15 - loss: 2.0874 - regression_loss: 1.5589 - classification_loss: 0.5286\n",
            " 9/86 [==>...........................] - ETA: 1:12 - loss: 2.0956 - regression_loss: 1.5623 - classification_loss: 0.5333\n",
            "10/86 [==>...........................] - ETA: 1:11 - loss: 2.0670 - regression_loss: 1.5386 - classification_loss: 0.5283\n",
            "11/86 [==>...........................] - ETA: 1:12 - loss: 2.0903 - regression_loss: 1.5593 - classification_loss: 0.5309\n",
            "12/86 [===>..........................] - ETA: 1:10 - loss: 2.0721 - regression_loss: 1.5388 - classification_loss: 0.5333\n",
            "13/86 [===>..........................] - ETA: 1:08 - loss: 2.0824 - regression_loss: 1.5468 - classification_loss: 0.5356\n",
            "14/86 [===>..........................] - ETA: 1:08 - loss: 2.1006 - regression_loss: 1.5633 - classification_loss: 0.5373\n",
            "15/86 [====>.........................] - ETA: 1:08 - loss: 2.1259 - regression_loss: 1.5836 - classification_loss: 0.5424\n",
            "16/86 [====>.........................] - ETA: 1:06 - loss: 2.1307 - regression_loss: 1.5876 - classification_loss: 0.5431\n",
            "17/86 [====>.........................] - ETA: 1:06 - loss: 2.1424 - regression_loss: 1.5961 - classification_loss: 0.5464\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.1451 - regression_loss: 1.5983 - classification_loss: 0.5468\n",
            "19/86 [=====>........................] - ETA: 1:03 - loss: 2.1358 - regression_loss: 1.5927 - classification_loss: 0.5430\n",
            "20/86 [=====>........................] - ETA: 1:02 - loss: 2.1309 - regression_loss: 1.5877 - classification_loss: 0.5433\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.1290 - regression_loss: 1.5871 - classification_loss: 0.5419\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.1230 - regression_loss: 1.5818 - classification_loss: 0.5412\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.1401 - regression_loss: 1.5923 - classification_loss: 0.5478\n",
            "24/86 [=======>......................] - ETA: 1:00 - loss: 2.1467 - regression_loss: 1.5982 - classification_loss: 0.5485\n",
            "25/86 [=======>......................] - ETA: 59s - loss: 2.1386 - regression_loss: 1.5926 - classification_loss: 0.5461 \n",
            "26/86 [========>.....................] - ETA: 58s - loss: 2.1458 - regression_loss: 1.5991 - classification_loss: 0.5467\n",
            "27/86 [========>.....................] - ETA: 57s - loss: 2.1559 - regression_loss: 1.6063 - classification_loss: 0.5496\n",
            "28/86 [========>.....................] - ETA: 56s - loss: 2.1605 - regression_loss: 1.6080 - classification_loss: 0.5526\n",
            "29/86 [=========>....................] - ETA: 55s - loss: 2.1639 - regression_loss: 1.6116 - classification_loss: 0.5524\n",
            "30/86 [=========>....................] - ETA: 55s - loss: 2.1677 - regression_loss: 1.6142 - classification_loss: 0.5535\n",
            "31/86 [=========>....................] - ETA: 54s - loss: 2.1694 - regression_loss: 1.6150 - classification_loss: 0.5544\n",
            "32/86 [==========>...................] - ETA: 53s - loss: 2.1677 - regression_loss: 1.6156 - classification_loss: 0.5521\n",
            "33/86 [==========>...................] - ETA: 52s - loss: 2.1689 - regression_loss: 1.6161 - classification_loss: 0.5529\n",
            "34/86 [==========>...................] - ETA: 51s - loss: 2.1750 - regression_loss: 1.6203 - classification_loss: 0.5548\n",
            "35/86 [===========>..................] - ETA: 50s - loss: 2.1703 - regression_loss: 1.6169 - classification_loss: 0.5535\n",
            "36/86 [===========>..................] - ETA: 49s - loss: 2.1722 - regression_loss: 1.6188 - classification_loss: 0.5535\n",
            "37/86 [===========>..................] - ETA: 48s - loss: 2.1804 - regression_loss: 1.6248 - classification_loss: 0.5556\n",
            "38/86 [============>.................] - ETA: 47s - loss: 2.1783 - regression_loss: 1.6234 - classification_loss: 0.5549\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.1801 - regression_loss: 1.6242 - classification_loss: 0.5559\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.1770 - regression_loss: 1.6233 - classification_loss: 0.5538\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.1789 - regression_loss: 1.6241 - classification_loss: 0.5548\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1799 - regression_loss: 1.6245 - classification_loss: 0.5554\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1771 - regression_loss: 1.6221 - classification_loss: 0.5550\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1716 - regression_loss: 1.6158 - classification_loss: 0.5558\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.1714 - regression_loss: 1.6151 - classification_loss: 0.5563\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1751 - regression_loss: 1.6168 - classification_loss: 0.5583\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1677 - regression_loss: 1.6104 - classification_loss: 0.5573\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1661 - regression_loss: 1.6096 - classification_loss: 0.5565\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1724 - regression_loss: 1.6144 - classification_loss: 0.5580\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1779 - regression_loss: 1.6195 - classification_loss: 0.5584\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1768 - regression_loss: 1.6187 - classification_loss: 0.5581\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1770 - regression_loss: 1.6188 - classification_loss: 0.5582\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1760 - regression_loss: 1.6187 - classification_loss: 0.5573\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1769 - regression_loss: 1.6176 - classification_loss: 0.5592\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1784 - regression_loss: 1.6192 - classification_loss: 0.5591\n",
            "56/86 [==================>...........] - ETA: 31s - loss: 2.1834 - regression_loss: 1.6216 - classification_loss: 0.5618\n",
            "57/86 [==================>...........] - ETA: 29s - loss: 2.1894 - regression_loss: 1.6266 - classification_loss: 0.5628\n",
            "58/86 [===================>..........] - ETA: 28s - loss: 2.1849 - regression_loss: 1.6222 - classification_loss: 0.5628\n",
            "59/86 [===================>..........] - ETA: 27s - loss: 2.1902 - regression_loss: 1.6255 - classification_loss: 0.5647\n",
            "60/86 [===================>..........] - ETA: 26s - loss: 2.1824 - regression_loss: 1.6196 - classification_loss: 0.5628\n",
            "61/86 [====================>.........] - ETA: 25s - loss: 2.1766 - regression_loss: 1.6153 - classification_loss: 0.5614\n",
            "62/86 [====================>.........] - ETA: 24s - loss: 2.1765 - regression_loss: 1.6148 - classification_loss: 0.5617\n",
            "63/86 [====================>.........] - ETA: 23s - loss: 2.1743 - regression_loss: 1.6122 - classification_loss: 0.5621\n",
            "64/86 [=====================>........] - ETA: 22s - loss: 2.1746 - regression_loss: 1.6118 - classification_loss: 0.5627\n",
            "65/86 [=====================>........] - ETA: 21s - loss: 2.1699 - regression_loss: 1.6086 - classification_loss: 0.5614\n",
            "66/86 [======================>.......] - ETA: 20s - loss: 2.1681 - regression_loss: 1.6071 - classification_loss: 0.5609\n",
            "67/86 [======================>.......] - ETA: 19s - loss: 2.1728 - regression_loss: 1.6099 - classification_loss: 0.5630\n",
            "68/86 [======================>.......] - ETA: 18s - loss: 2.1792 - regression_loss: 1.6132 - classification_loss: 0.5660\n",
            "69/86 [=======================>......] - ETA: 17s - loss: 2.1776 - regression_loss: 1.6122 - classification_loss: 0.5654\n",
            "70/86 [=======================>......] - ETA: 16s - loss: 2.1791 - regression_loss: 1.6134 - classification_loss: 0.5656\n",
            "71/86 [=======================>......] - ETA: 15s - loss: 2.1756 - regression_loss: 1.6104 - classification_loss: 0.5652\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.1763 - regression_loss: 1.6109 - classification_loss: 0.5654\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.1743 - regression_loss: 1.6088 - classification_loss: 0.5654\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.1756 - regression_loss: 1.6102 - classification_loss: 0.5654\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.1755 - regression_loss: 1.6101 - classification_loss: 0.5654\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.1790 - regression_loss: 1.6123 - classification_loss: 0.5667\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.1870 - regression_loss: 1.6187 - classification_loss: 0.5683 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.1877 - regression_loss: 1.6196 - classification_loss: 0.5681\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.1873 - regression_loss: 1.6191 - classification_loss: 0.5682\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.1880 - regression_loss: 1.6195 - classification_loss: 0.5686\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.1869 - regression_loss: 1.6194 - classification_loss: 0.5674\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.1852 - regression_loss: 1.6186 - classification_loss: 0.5666\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.1864 - regression_loss: 1.6203 - classification_loss: 0.5661\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.1867 - regression_loss: 1.6211 - classification_loss: 0.5656\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.1871 - regression_loss: 1.6215 - classification_loss: 0.5656\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1877 - regression_loss: 1.6221 - classification_loss: 0.5656\n",
            "Epoch 8: saving model to ./snapshots\\resnet50_csv_08.h5\n",
            "\n",
            "86/86 [==============================] - 87s 1s/step - loss: 2.1877 - regression_loss: 1.6221 - classification_loss: 0.5656 - lr: 1.0000e-05\n",
            "Epoch 9/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:37 - loss: 2.0199 - regression_loss: 1.4686 - classification_loss: 0.5513\n",
            " 2/86 [..............................] - ETA: 1:00 - loss: 2.0702 - regression_loss: 1.5197 - classification_loss: 0.5505\n",
            " 3/86 [>.............................] - ETA: 1:04 - loss: 2.0876 - regression_loss: 1.5374 - classification_loss: 0.5502\n",
            " 4/86 [>.............................] - ETA: 1:07 - loss: 2.1146 - regression_loss: 1.5635 - classification_loss: 0.5510\n",
            " 5/86 [>.............................] - ETA: 1:06 - loss: 2.1169 - regression_loss: 1.5649 - classification_loss: 0.5520\n",
            " 6/86 [=>............................] - ETA: 1:07 - loss: 2.1217 - regression_loss: 1.5658 - classification_loss: 0.5558\n",
            " 7/86 [=>............................] - ETA: 1:09 - loss: 2.1713 - regression_loss: 1.6106 - classification_loss: 0.5607\n",
            " 8/86 [=>............................] - ETA: 1:08 - loss: 2.1688 - regression_loss: 1.6043 - classification_loss: 0.5645\n",
            " 9/86 [==>...........................] - ETA: 1:08 - loss: 2.1479 - regression_loss: 1.5828 - classification_loss: 0.5651\n",
            "10/86 [==>...........................] - ETA: 1:07 - loss: 2.1756 - regression_loss: 1.6082 - classification_loss: 0.5674\n",
            "11/86 [==>...........................] - ETA: 1:06 - loss: 2.1706 - regression_loss: 1.6054 - classification_loss: 0.5652\n",
            "12/86 [===>..........................] - ETA: 1:06 - loss: 2.1739 - regression_loss: 1.6123 - classification_loss: 0.5616\n",
            "13/86 [===>..........................] - ETA: 1:05 - loss: 2.1700 - regression_loss: 1.6047 - classification_loss: 0.5654\n",
            "14/86 [===>..........................] - ETA: 1:03 - loss: 2.1753 - regression_loss: 1.6104 - classification_loss: 0.5649\n",
            "15/86 [====>.........................] - ETA: 1:03 - loss: 2.1513 - regression_loss: 1.5935 - classification_loss: 0.5578\n",
            "16/86 [====>.........................] - ETA: 1:02 - loss: 2.1220 - regression_loss: 1.5700 - classification_loss: 0.5519\n",
            "17/86 [====>.........................] - ETA: 1:01 - loss: 2.1204 - regression_loss: 1.5701 - classification_loss: 0.5504\n",
            "18/86 [=====>........................] - ETA: 1:00 - loss: 2.1248 - regression_loss: 1.5691 - classification_loss: 0.5558\n",
            "19/86 [=====>........................] - ETA: 1:00 - loss: 2.1302 - regression_loss: 1.5744 - classification_loss: 0.5558\n",
            "20/86 [=====>........................] - ETA: 59s - loss: 2.1376 - regression_loss: 1.5831 - classification_loss: 0.5545 \n",
            "21/86 [======>.......................] - ETA: 59s - loss: 2.1383 - regression_loss: 1.5841 - classification_loss: 0.5542\n",
            "22/86 [======>.......................] - ETA: 58s - loss: 2.1527 - regression_loss: 1.5942 - classification_loss: 0.5585\n",
            "23/86 [=======>......................] - ETA: 57s - loss: 2.1662 - regression_loss: 1.6057 - classification_loss: 0.5605\n",
            "24/86 [=======>......................] - ETA: 56s - loss: 2.1623 - regression_loss: 1.6033 - classification_loss: 0.5590\n",
            "25/86 [=======>......................] - ETA: 55s - loss: 2.1528 - regression_loss: 1.5949 - classification_loss: 0.5579\n",
            "26/86 [========>.....................] - ETA: 54s - loss: 2.1450 - regression_loss: 1.5874 - classification_loss: 0.5577\n",
            "27/86 [========>.....................] - ETA: 53s - loss: 2.1539 - regression_loss: 1.5968 - classification_loss: 0.5571\n",
            "28/86 [========>.....................] - ETA: 52s - loss: 2.1482 - regression_loss: 1.5940 - classification_loss: 0.5542\n",
            "29/86 [=========>....................] - ETA: 52s - loss: 2.1514 - regression_loss: 1.5963 - classification_loss: 0.5551\n",
            "30/86 [=========>....................] - ETA: 50s - loss: 2.1532 - regression_loss: 1.5965 - classification_loss: 0.5566\n",
            "31/86 [=========>....................] - ETA: 50s - loss: 2.1591 - regression_loss: 1.6011 - classification_loss: 0.5580\n",
            "32/86 [==========>...................] - ETA: 49s - loss: 2.1542 - regression_loss: 1.5979 - classification_loss: 0.5562\n",
            "33/86 [==========>...................] - ETA: 48s - loss: 2.1532 - regression_loss: 1.5962 - classification_loss: 0.5570\n",
            "34/86 [==========>...................] - ETA: 48s - loss: 2.1507 - regression_loss: 1.5948 - classification_loss: 0.5559\n",
            "35/86 [===========>..................] - ETA: 46s - loss: 2.1587 - regression_loss: 1.6009 - classification_loss: 0.5578\n",
            "36/86 [===========>..................] - ETA: 46s - loss: 2.1688 - regression_loss: 1.6104 - classification_loss: 0.5584\n",
            "37/86 [===========>..................] - ETA: 45s - loss: 2.1734 - regression_loss: 1.6157 - classification_loss: 0.5576\n",
            "38/86 [============>.................] - ETA: 44s - loss: 2.1773 - regression_loss: 1.6179 - classification_loss: 0.5594\n",
            "39/86 [============>.................] - ETA: 43s - loss: 2.1751 - regression_loss: 1.6155 - classification_loss: 0.5596\n",
            "40/86 [============>.................] - ETA: 42s - loss: 2.1717 - regression_loss: 1.6135 - classification_loss: 0.5583\n",
            "41/86 [=============>................] - ETA: 41s - loss: 2.1734 - regression_loss: 1.6151 - classification_loss: 0.5584\n",
            "42/86 [=============>................] - ETA: 40s - loss: 2.1777 - regression_loss: 1.6181 - classification_loss: 0.5597\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.1799 - regression_loss: 1.6202 - classification_loss: 0.5597\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.1802 - regression_loss: 1.6199 - classification_loss: 0.5603\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.1798 - regression_loss: 1.6189 - classification_loss: 0.5609\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.1848 - regression_loss: 1.6228 - classification_loss: 0.5620\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.1945 - regression_loss: 1.6283 - classification_loss: 0.5662\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.1883 - regression_loss: 1.6229 - classification_loss: 0.5654\n",
            "49/86 [================>.............] - ETA: 34s - loss: 2.1907 - regression_loss: 1.6245 - classification_loss: 0.5662\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1818 - regression_loss: 1.6182 - classification_loss: 0.5637\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1854 - regression_loss: 1.6221 - classification_loss: 0.5633\n",
            "52/86 [=================>............] - ETA: 31s - loss: 2.1839 - regression_loss: 1.6217 - classification_loss: 0.5622\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1839 - regression_loss: 1.6226 - classification_loss: 0.5612\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1876 - regression_loss: 1.6267 - classification_loss: 0.5609\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1839 - regression_loss: 1.6243 - classification_loss: 0.5597\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1796 - regression_loss: 1.6206 - classification_loss: 0.5590\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1740 - regression_loss: 1.6170 - classification_loss: 0.5571\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1805 - regression_loss: 1.6228 - classification_loss: 0.5578\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1786 - regression_loss: 1.6213 - classification_loss: 0.5573\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1793 - regression_loss: 1.6207 - classification_loss: 0.5585\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1756 - regression_loss: 1.6182 - classification_loss: 0.5573\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1770 - regression_loss: 1.6199 - classification_loss: 0.5571\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1767 - regression_loss: 1.6197 - classification_loss: 0.5570\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1729 - regression_loss: 1.6166 - classification_loss: 0.5563\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1756 - regression_loss: 1.6185 - classification_loss: 0.5572\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.1733 - regression_loss: 1.6176 - classification_loss: 0.5558\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.1746 - regression_loss: 1.6186 - classification_loss: 0.5559\n",
            "68/86 [======================>.......] - ETA: 16s - loss: 2.1743 - regression_loss: 1.6183 - classification_loss: 0.5560\n",
            "69/86 [=======================>......] - ETA: 15s - loss: 2.1724 - regression_loss: 1.6173 - classification_loss: 0.5551\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1756 - regression_loss: 1.6197 - classification_loss: 0.5559\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1717 - regression_loss: 1.6164 - classification_loss: 0.5553\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1733 - regression_loss: 1.6182 - classification_loss: 0.5551\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1765 - regression_loss: 1.6203 - classification_loss: 0.5562\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1751 - regression_loss: 1.6197 - classification_loss: 0.5554\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1737 - regression_loss: 1.6185 - classification_loss: 0.5552\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1727 - regression_loss: 1.6181 - classification_loss: 0.5545 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1747 - regression_loss: 1.6194 - classification_loss: 0.5553\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1725 - regression_loss: 1.6179 - classification_loss: 0.5546\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1702 - regression_loss: 1.6163 - classification_loss: 0.5539\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1685 - regression_loss: 1.6148 - classification_loss: 0.5537\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1722 - regression_loss: 1.6179 - classification_loss: 0.5543\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1739 - regression_loss: 1.6191 - classification_loss: 0.5549\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1748 - regression_loss: 1.6198 - classification_loss: 0.5550\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1736 - regression_loss: 1.6187 - classification_loss: 0.5549\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1760 - regression_loss: 1.6203 - classification_loss: 0.5557\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1787 - regression_loss: 1.6224 - classification_loss: 0.5563\n",
            "Epoch 9: saving model to ./snapshots\\resnet50_csv_09.h5\n",
            "\n",
            "86/86 [==============================] - 83s 959ms/step - loss: 2.1787 - regression_loss: 1.6224 - classification_loss: 0.5563 - lr: 1.0000e-05\n",
            "Epoch 10/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:03 - loss: 2.0455 - regression_loss: 1.5253 - classification_loss: 0.5203\n",
            " 2/86 [..............................] - ETA: 1:02 - loss: 2.2348 - regression_loss: 1.6620 - classification_loss: 0.5728\n",
            " 3/86 [>.............................] - ETA: 1:29 - loss: 2.1252 - regression_loss: 1.5724 - classification_loss: 0.5529\n",
            " 4/86 [>.............................] - ETA: 1:31 - loss: 2.1383 - regression_loss: 1.5886 - classification_loss: 0.5497\n",
            " 5/86 [>.............................] - ETA: 1:22 - loss: 2.1359 - regression_loss: 1.5797 - classification_loss: 0.5562\n",
            " 6/86 [=>............................] - ETA: 1:21 - loss: 2.1088 - regression_loss: 1.5553 - classification_loss: 0.5535\n",
            " 7/86 [=>............................] - ETA: 1:20 - loss: 2.0876 - regression_loss: 1.5498 - classification_loss: 0.5379\n",
            " 8/86 [=>............................] - ETA: 1:22 - loss: 2.0890 - regression_loss: 1.5540 - classification_loss: 0.5349\n",
            " 9/86 [==>...........................] - ETA: 1:19 - loss: 2.1140 - regression_loss: 1.5744 - classification_loss: 0.5397\n",
            "10/86 [==>...........................] - ETA: 1:16 - loss: 2.1332 - regression_loss: 1.5908 - classification_loss: 0.5424\n",
            "11/86 [==>...........................] - ETA: 1:15 - loss: 2.1340 - regression_loss: 1.5900 - classification_loss: 0.5441\n",
            "12/86 [===>..........................] - ETA: 1:13 - loss: 2.1236 - regression_loss: 1.5764 - classification_loss: 0.5473\n",
            "13/86 [===>..........................] - ETA: 1:11 - loss: 2.1446 - regression_loss: 1.5983 - classification_loss: 0.5463\n",
            "14/86 [===>..........................] - ETA: 1:10 - loss: 2.1323 - regression_loss: 1.5892 - classification_loss: 0.5431\n",
            "15/86 [====>.........................] - ETA: 1:08 - loss: 2.1500 - regression_loss: 1.5955 - classification_loss: 0.5545\n",
            "16/86 [====>.........................] - ETA: 1:06 - loss: 2.1500 - regression_loss: 1.5966 - classification_loss: 0.5535\n",
            "17/86 [====>.........................] - ETA: 1:07 - loss: 2.1277 - regression_loss: 1.5805 - classification_loss: 0.5472\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.1346 - regression_loss: 1.5900 - classification_loss: 0.5445\n",
            "19/86 [=====>........................] - ETA: 1:04 - loss: 2.1230 - regression_loss: 1.5807 - classification_loss: 0.5423\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.1336 - regression_loss: 1.5909 - classification_loss: 0.5427\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.1503 - regression_loss: 1.6026 - classification_loss: 0.5478\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.1496 - regression_loss: 1.5995 - classification_loss: 0.5501\n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.1489 - regression_loss: 1.5994 - classification_loss: 0.5494 \n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.1456 - regression_loss: 1.5957 - classification_loss: 0.5499\n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.1261 - regression_loss: 1.5819 - classification_loss: 0.5442\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.1237 - regression_loss: 1.5811 - classification_loss: 0.5426\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.1190 - regression_loss: 1.5754 - classification_loss: 0.5436\n",
            "28/86 [========>.....................] - ETA: 56s - loss: 2.1231 - regression_loss: 1.5789 - classification_loss: 0.5442\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.1197 - regression_loss: 1.5772 - classification_loss: 0.5425\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.1258 - regression_loss: 1.5821 - classification_loss: 0.5437\n",
            "31/86 [=========>....................] - ETA: 53s - loss: 2.1329 - regression_loss: 1.5885 - classification_loss: 0.5443\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.1489 - regression_loss: 1.6023 - classification_loss: 0.5467\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.1531 - regression_loss: 1.6054 - classification_loss: 0.5477\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.1558 - regression_loss: 1.6087 - classification_loss: 0.5471\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.1634 - regression_loss: 1.6119 - classification_loss: 0.5515\n",
            "36/86 [===========>..................] - ETA: 48s - loss: 2.1596 - regression_loss: 1.6099 - classification_loss: 0.5497\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.1556 - regression_loss: 1.6069 - classification_loss: 0.5487\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.1614 - regression_loss: 1.6134 - classification_loss: 0.5480\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.1561 - regression_loss: 1.6088 - classification_loss: 0.5473\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.1594 - regression_loss: 1.6118 - classification_loss: 0.5476\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.1645 - regression_loss: 1.6156 - classification_loss: 0.5488\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1573 - regression_loss: 1.6101 - classification_loss: 0.5471\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1628 - regression_loss: 1.6159 - classification_loss: 0.5470\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1655 - regression_loss: 1.6184 - classification_loss: 0.5471\n",
            "45/86 [==============>...............] - ETA: 40s - loss: 2.1628 - regression_loss: 1.6159 - classification_loss: 0.5469\n",
            "46/86 [===============>..............] - ETA: 39s - loss: 2.1634 - regression_loss: 1.6168 - classification_loss: 0.5465\n",
            "47/86 [===============>..............] - ETA: 38s - loss: 2.1682 - regression_loss: 1.6207 - classification_loss: 0.5475\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1682 - regression_loss: 1.6206 - classification_loss: 0.5475\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1773 - regression_loss: 1.6282 - classification_loss: 0.5491\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1773 - regression_loss: 1.6266 - classification_loss: 0.5507\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1837 - regression_loss: 1.6312 - classification_loss: 0.5525\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1880 - regression_loss: 1.6347 - classification_loss: 0.5533\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1844 - regression_loss: 1.6329 - classification_loss: 0.5515\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1784 - regression_loss: 1.6280 - classification_loss: 0.5504\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1815 - regression_loss: 1.6318 - classification_loss: 0.5497\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1836 - regression_loss: 1.6327 - classification_loss: 0.5508\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1895 - regression_loss: 1.6369 - classification_loss: 0.5525\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1898 - regression_loss: 1.6372 - classification_loss: 0.5526\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1858 - regression_loss: 1.6346 - classification_loss: 0.5512\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1848 - regression_loss: 1.6332 - classification_loss: 0.5516\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1833 - regression_loss: 1.6287 - classification_loss: 0.5547\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1821 - regression_loss: 1.6282 - classification_loss: 0.5539\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1793 - regression_loss: 1.6262 - classification_loss: 0.5531\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1863 - regression_loss: 1.6316 - classification_loss: 0.5547\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1865 - regression_loss: 1.6323 - classification_loss: 0.5542\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.1893 - regression_loss: 1.6334 - classification_loss: 0.5559\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1854 - regression_loss: 1.6304 - classification_loss: 0.5550\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1850 - regression_loss: 1.6293 - classification_loss: 0.5557\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1886 - regression_loss: 1.6321 - classification_loss: 0.5565\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1926 - regression_loss: 1.6360 - classification_loss: 0.5566\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1883 - regression_loss: 1.6331 - classification_loss: 0.5552\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1872 - regression_loss: 1.6320 - classification_loss: 0.5552\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1859 - regression_loss: 1.6309 - classification_loss: 0.5550\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1884 - regression_loss: 1.6334 - classification_loss: 0.5551\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1877 - regression_loss: 1.6333 - classification_loss: 0.5544\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1907 - regression_loss: 1.6357 - classification_loss: 0.5549 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1892 - regression_loss: 1.6345 - classification_loss: 0.5548\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1913 - regression_loss: 1.6358 - classification_loss: 0.5555\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.1875 - regression_loss: 1.6332 - classification_loss: 0.5543\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.1865 - regression_loss: 1.6323 - classification_loss: 0.5542\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.1835 - regression_loss: 1.6294 - classification_loss: 0.5541\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.1839 - regression_loss: 1.6293 - classification_loss: 0.5546\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.1843 - regression_loss: 1.6296 - classification_loss: 0.5547\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.1901 - regression_loss: 1.6344 - classification_loss: 0.5557\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.1947 - regression_loss: 1.6387 - classification_loss: 0.5560\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1954 - regression_loss: 1.6387 - classification_loss: 0.5567\n",
            "Epoch 10: saving model to ./snapshots\\resnet50_csv_10.h5\n",
            "\n",
            "86/86 [==============================] - 88s 1s/step - loss: 2.1954 - regression_loss: 1.6387 - classification_loss: 0.5567 - lr: 1.0000e-05\n",
            "Epoch 11/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:55 - loss: 2.1336 - regression_loss: 1.6053 - classification_loss: 0.5283\n",
            " 2/86 [..............................] - ETA: 1:23 - loss: 2.0541 - regression_loss: 1.5459 - classification_loss: 0.5083\n",
            " 3/86 [>.............................] - ETA: 1:37 - loss: 1.9656 - regression_loss: 1.4797 - classification_loss: 0.4859\n",
            " 4/86 [>.............................] - ETA: 1:28 - loss: 2.0243 - regression_loss: 1.5273 - classification_loss: 0.4970\n",
            " 5/86 [>.............................] - ETA: 1:23 - loss: 2.0306 - regression_loss: 1.5324 - classification_loss: 0.4982\n",
            " 6/86 [=>............................] - ETA: 1:26 - loss: 2.0554 - regression_loss: 1.5570 - classification_loss: 0.4984\n",
            " 7/86 [=>............................] - ETA: 1:21 - loss: 2.0696 - regression_loss: 1.5654 - classification_loss: 0.5042\n",
            " 8/86 [=>............................] - ETA: 1:23 - loss: 2.0728 - regression_loss: 1.5709 - classification_loss: 0.5020\n",
            " 9/86 [==>...........................] - ETA: 1:19 - loss: 2.0873 - regression_loss: 1.5808 - classification_loss: 0.5065\n",
            "10/86 [==>...........................] - ETA: 1:18 - loss: 2.1119 - regression_loss: 1.5998 - classification_loss: 0.5121\n",
            "11/86 [==>...........................] - ETA: 1:17 - loss: 2.1050 - regression_loss: 1.5885 - classification_loss: 0.5165\n",
            "12/86 [===>..........................] - ETA: 1:16 - loss: 2.1075 - regression_loss: 1.5848 - classification_loss: 0.5227\n",
            "13/86 [===>..........................] - ETA: 1:14 - loss: 2.1175 - regression_loss: 1.5924 - classification_loss: 0.5252\n",
            "14/86 [===>..........................] - ETA: 1:14 - loss: 2.1076 - regression_loss: 1.5849 - classification_loss: 0.5227\n",
            "15/86 [====>.........................] - ETA: 1:11 - loss: 2.0998 - regression_loss: 1.5810 - classification_loss: 0.5187\n",
            "16/86 [====>.........................] - ETA: 1:11 - loss: 2.0929 - regression_loss: 1.5795 - classification_loss: 0.5134\n",
            "17/86 [====>.........................] - ETA: 1:08 - loss: 2.0806 - regression_loss: 1.5698 - classification_loss: 0.5108\n",
            "18/86 [=====>........................] - ETA: 1:07 - loss: 2.0652 - regression_loss: 1.5544 - classification_loss: 0.5109\n",
            "19/86 [=====>........................] - ETA: 1:07 - loss: 2.0749 - regression_loss: 1.5596 - classification_loss: 0.5152\n",
            "20/86 [=====>........................] - ETA: 1:08 - loss: 2.0860 - regression_loss: 1.5700 - classification_loss: 0.5160\n",
            "21/86 [======>.......................] - ETA: 1:06 - loss: 2.0805 - regression_loss: 1.5632 - classification_loss: 0.5172\n",
            "22/86 [======>.......................] - ETA: 1:05 - loss: 2.0931 - regression_loss: 1.5724 - classification_loss: 0.5208\n",
            "23/86 [=======>......................] - ETA: 1:03 - loss: 2.1110 - regression_loss: 1.5865 - classification_loss: 0.5245\n",
            "24/86 [=======>......................] - ETA: 1:03 - loss: 2.1249 - regression_loss: 1.5972 - classification_loss: 0.5277\n",
            "25/86 [=======>......................] - ETA: 1:01 - loss: 2.1159 - regression_loss: 1.5893 - classification_loss: 0.5266\n",
            "26/86 [========>.....................] - ETA: 1:00 - loss: 2.1249 - regression_loss: 1.5953 - classification_loss: 0.5296\n",
            "27/86 [========>.....................] - ETA: 1:00 - loss: 2.1262 - regression_loss: 1.5955 - classification_loss: 0.5307\n",
            "28/86 [========>.....................] - ETA: 58s - loss: 2.1332 - regression_loss: 1.6018 - classification_loss: 0.5314 \n",
            "29/86 [=========>....................] - ETA: 57s - loss: 2.1366 - regression_loss: 1.6055 - classification_loss: 0.5311\n",
            "30/86 [=========>....................] - ETA: 56s - loss: 2.1481 - regression_loss: 1.6144 - classification_loss: 0.5337\n",
            "31/86 [=========>....................] - ETA: 55s - loss: 2.1534 - regression_loss: 1.6170 - classification_loss: 0.5364\n",
            "32/86 [==========>...................] - ETA: 54s - loss: 2.1589 - regression_loss: 1.6214 - classification_loss: 0.5374\n",
            "33/86 [==========>...................] - ETA: 53s - loss: 2.1600 - regression_loss: 1.6227 - classification_loss: 0.5373\n",
            "34/86 [==========>...................] - ETA: 52s - loss: 2.1570 - regression_loss: 1.6203 - classification_loss: 0.5368\n",
            "35/86 [===========>..................] - ETA: 50s - loss: 2.1477 - regression_loss: 1.6139 - classification_loss: 0.5338\n",
            "36/86 [===========>..................] - ETA: 54s - loss: 2.1487 - regression_loss: 1.6133 - classification_loss: 0.5355\n",
            "37/86 [===========>..................] - ETA: 52s - loss: 2.1615 - regression_loss: 1.6237 - classification_loss: 0.5377\n",
            "38/86 [============>.................] - ETA: 51s - loss: 2.1618 - regression_loss: 1.6244 - classification_loss: 0.5374\n",
            "39/86 [============>.................] - ETA: 50s - loss: 2.1701 - regression_loss: 1.6316 - classification_loss: 0.5385\n",
            "40/86 [============>.................] - ETA: 48s - loss: 2.1780 - regression_loss: 1.6378 - classification_loss: 0.5402\n",
            "41/86 [=============>................] - ETA: 47s - loss: 2.1741 - regression_loss: 1.6361 - classification_loss: 0.5380\n",
            "42/86 [=============>................] - ETA: 46s - loss: 2.1713 - regression_loss: 1.6326 - classification_loss: 0.5387\n",
            "43/86 [==============>...............] - ETA: 45s - loss: 2.1671 - regression_loss: 1.6284 - classification_loss: 0.5387\n",
            "44/86 [==============>...............] - ETA: 43s - loss: 2.1589 - regression_loss: 1.6224 - classification_loss: 0.5365\n",
            "45/86 [==============>...............] - ETA: 42s - loss: 2.1540 - regression_loss: 1.6199 - classification_loss: 0.5341\n",
            "46/86 [===============>..............] - ETA: 41s - loss: 2.1532 - regression_loss: 1.6204 - classification_loss: 0.5328\n",
            "47/86 [===============>..............] - ETA: 40s - loss: 2.1571 - regression_loss: 1.6237 - classification_loss: 0.5334\n",
            "48/86 [===============>..............] - ETA: 39s - loss: 2.1593 - regression_loss: 1.6261 - classification_loss: 0.5332\n",
            "49/86 [================>.............] - ETA: 38s - loss: 2.1617 - regression_loss: 1.6277 - classification_loss: 0.5340\n",
            "50/86 [================>.............] - ETA: 37s - loss: 2.1595 - regression_loss: 1.6253 - classification_loss: 0.5342\n",
            "51/86 [================>.............] - ETA: 36s - loss: 2.1532 - regression_loss: 1.6196 - classification_loss: 0.5337\n",
            "52/86 [=================>............] - ETA: 35s - loss: 2.1486 - regression_loss: 1.6163 - classification_loss: 0.5322\n",
            "53/86 [=================>............] - ETA: 33s - loss: 2.1490 - regression_loss: 1.6160 - classification_loss: 0.5329\n",
            "54/86 [=================>............] - ETA: 33s - loss: 2.1519 - regression_loss: 1.6177 - classification_loss: 0.5342\n",
            "55/86 [==================>...........] - ETA: 31s - loss: 2.1510 - regression_loss: 1.6168 - classification_loss: 0.5342\n",
            "56/86 [==================>...........] - ETA: 30s - loss: 2.1496 - regression_loss: 1.6156 - classification_loss: 0.5340\n",
            "57/86 [==================>...........] - ETA: 29s - loss: 2.1461 - regression_loss: 1.6115 - classification_loss: 0.5347\n",
            "58/86 [===================>..........] - ETA: 28s - loss: 2.1455 - regression_loss: 1.6102 - classification_loss: 0.5352\n",
            "59/86 [===================>..........] - ETA: 27s - loss: 2.1451 - regression_loss: 1.6088 - classification_loss: 0.5363\n",
            "60/86 [===================>..........] - ETA: 26s - loss: 2.1466 - regression_loss: 1.6098 - classification_loss: 0.5368\n",
            "61/86 [====================>.........] - ETA: 25s - loss: 2.1413 - regression_loss: 1.6052 - classification_loss: 0.5361\n",
            "62/86 [====================>.........] - ETA: 24s - loss: 2.1431 - regression_loss: 1.6060 - classification_loss: 0.5371\n",
            "63/86 [====================>.........] - ETA: 23s - loss: 2.1459 - regression_loss: 1.6083 - classification_loss: 0.5376\n",
            "64/86 [=====================>........] - ETA: 22s - loss: 2.1465 - regression_loss: 1.6082 - classification_loss: 0.5383\n",
            "65/86 [=====================>........] - ETA: 21s - loss: 2.1410 - regression_loss: 1.6044 - classification_loss: 0.5366\n",
            "66/86 [======================>.......] - ETA: 20s - loss: 2.1443 - regression_loss: 1.6076 - classification_loss: 0.5367\n",
            "67/86 [======================>.......] - ETA: 19s - loss: 2.1455 - regression_loss: 1.6086 - classification_loss: 0.5368\n",
            "68/86 [======================>.......] - ETA: 18s - loss: 2.1426 - regression_loss: 1.6060 - classification_loss: 0.5366\n",
            "69/86 [=======================>......] - ETA: 17s - loss: 2.1411 - regression_loss: 1.6042 - classification_loss: 0.5369\n",
            "70/86 [=======================>......] - ETA: 16s - loss: 2.1421 - regression_loss: 1.6053 - classification_loss: 0.5368\n",
            "71/86 [=======================>......] - ETA: 15s - loss: 2.1452 - regression_loss: 1.6081 - classification_loss: 0.5371\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.1442 - regression_loss: 1.6075 - classification_loss: 0.5367\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.1508 - regression_loss: 1.6128 - classification_loss: 0.5380\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.1524 - regression_loss: 1.6134 - classification_loss: 0.5391\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.1534 - regression_loss: 1.6146 - classification_loss: 0.5387\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.1538 - regression_loss: 1.6154 - classification_loss: 0.5384\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.1551 - regression_loss: 1.6164 - classification_loss: 0.5387 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.1476 - regression_loss: 1.6109 - classification_loss: 0.5367\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.1455 - regression_loss: 1.6095 - classification_loss: 0.5360\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.1418 - regression_loss: 1.6066 - classification_loss: 0.5353\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.1414 - regression_loss: 1.6063 - classification_loss: 0.5351\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.1400 - regression_loss: 1.6054 - classification_loss: 0.5346\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.1406 - regression_loss: 1.6060 - classification_loss: 0.5346\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1390 - regression_loss: 1.6044 - classification_loss: 0.5346\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.1389 - regression_loss: 1.6038 - classification_loss: 0.5351\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1415 - regression_loss: 1.6058 - classification_loss: 0.5358\n",
            "Epoch 11: saving model to ./snapshots\\resnet50_csv_11.h5\n",
            "\n",
            "86/86 [==============================] - 87s 1s/step - loss: 2.1415 - regression_loss: 1.6058 - classification_loss: 0.5358 - lr: 1.0000e-05\n",
            "Epoch 12/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:06 - loss: 1.9815 - regression_loss: 1.4497 - classification_loss: 0.5319\n",
            " 2/86 [..............................] - ETA: 1:09 - loss: 2.0490 - regression_loss: 1.5259 - classification_loss: 0.5231\n",
            " 3/86 [>.............................] - ETA: 1:12 - loss: 2.1347 - regression_loss: 1.5858 - classification_loss: 0.5490\n",
            " 4/86 [>.............................] - ETA: 1:17 - loss: 2.1204 - regression_loss: 1.5737 - classification_loss: 0.5467\n",
            " 5/86 [>.............................] - ETA: 1:18 - loss: 2.1306 - regression_loss: 1.5985 - classification_loss: 0.5320\n",
            " 6/86 [=>............................] - ETA: 1:16 - loss: 2.0849 - regression_loss: 1.5574 - classification_loss: 0.5274\n",
            " 7/86 [=>............................] - ETA: 1:14 - loss: 2.1175 - regression_loss: 1.5815 - classification_loss: 0.5360\n",
            " 8/86 [=>............................] - ETA: 1:16 - loss: 2.1613 - regression_loss: 1.6066 - classification_loss: 0.5547\n",
            " 9/86 [==>...........................] - ETA: 1:12 - loss: 2.1379 - regression_loss: 1.5883 - classification_loss: 0.5496\n",
            "10/86 [==>...........................] - ETA: 1:12 - loss: 2.1584 - regression_loss: 1.6044 - classification_loss: 0.5539\n",
            "11/86 [==>...........................] - ETA: 1:11 - loss: 2.1571 - regression_loss: 1.6049 - classification_loss: 0.5522\n",
            "12/86 [===>..........................] - ETA: 1:10 - loss: 2.1709 - regression_loss: 1.6126 - classification_loss: 0.5583\n",
            "13/86 [===>..........................] - ETA: 1:09 - loss: 2.1730 - regression_loss: 1.6150 - classification_loss: 0.5579\n",
            "14/86 [===>..........................] - ETA: 1:08 - loss: 2.1676 - regression_loss: 1.6104 - classification_loss: 0.5573\n",
            "15/86 [====>.........................] - ETA: 1:07 - loss: 2.1404 - regression_loss: 1.5918 - classification_loss: 0.5486\n",
            "16/86 [====>.........................] - ETA: 1:06 - loss: 2.1737 - regression_loss: 1.6191 - classification_loss: 0.5545\n",
            "17/86 [====>.........................] - ETA: 1:04 - loss: 2.1699 - regression_loss: 1.6166 - classification_loss: 0.5533\n",
            "18/86 [=====>........................] - ETA: 1:03 - loss: 2.1737 - regression_loss: 1.6182 - classification_loss: 0.5555\n",
            "19/86 [=====>........................] - ETA: 1:02 - loss: 2.1669 - regression_loss: 1.6117 - classification_loss: 0.5552\n",
            "20/86 [=====>........................] - ETA: 1:01 - loss: 2.1714 - regression_loss: 1.6178 - classification_loss: 0.5536\n",
            "21/86 [======>.......................] - ETA: 1:00 - loss: 2.1728 - regression_loss: 1.6181 - classification_loss: 0.5547\n",
            "22/86 [======>.......................] - ETA: 59s - loss: 2.1692 - regression_loss: 1.6169 - classification_loss: 0.5523 \n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.1746 - regression_loss: 1.6218 - classification_loss: 0.5529\n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.1752 - regression_loss: 1.6228 - classification_loss: 0.5524\n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.1640 - regression_loss: 1.6154 - classification_loss: 0.5487\n",
            "26/86 [========>.....................] - ETA: 56s - loss: 2.1568 - regression_loss: 1.6097 - classification_loss: 0.5470\n",
            "27/86 [========>.....................] - ETA: 55s - loss: 2.1623 - regression_loss: 1.6165 - classification_loss: 0.5458\n",
            "28/86 [========>.....................] - ETA: 54s - loss: 2.1669 - regression_loss: 1.6186 - classification_loss: 0.5484\n",
            "29/86 [=========>....................] - ETA: 53s - loss: 2.1589 - regression_loss: 1.6121 - classification_loss: 0.5468\n",
            "30/86 [=========>....................] - ETA: 52s - loss: 2.1603 - regression_loss: 1.6130 - classification_loss: 0.5473\n",
            "31/86 [=========>....................] - ETA: 51s - loss: 2.1616 - regression_loss: 1.6140 - classification_loss: 0.5476\n",
            "32/86 [==========>...................] - ETA: 50s - loss: 2.1699 - regression_loss: 1.6183 - classification_loss: 0.5516\n",
            "33/86 [==========>...................] - ETA: 49s - loss: 2.1722 - regression_loss: 1.6181 - classification_loss: 0.5541\n",
            "34/86 [==========>...................] - ETA: 48s - loss: 2.1627 - regression_loss: 1.6106 - classification_loss: 0.5521\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.1577 - regression_loss: 1.6077 - classification_loss: 0.5501\n",
            "36/86 [===========>..................] - ETA: 46s - loss: 2.1631 - regression_loss: 1.6134 - classification_loss: 0.5498\n",
            "37/86 [===========>..................] - ETA: 45s - loss: 2.1714 - regression_loss: 1.6210 - classification_loss: 0.5504\n",
            "38/86 [============>.................] - ETA: 44s - loss: 2.1685 - regression_loss: 1.6189 - classification_loss: 0.5496\n",
            "39/86 [============>.................] - ETA: 43s - loss: 2.1680 - regression_loss: 1.6185 - classification_loss: 0.5495\n",
            "40/86 [============>.................] - ETA: 42s - loss: 2.1756 - regression_loss: 1.6233 - classification_loss: 0.5523\n",
            "41/86 [=============>................] - ETA: 41s - loss: 2.1826 - regression_loss: 1.6289 - classification_loss: 0.5537\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.1840 - regression_loss: 1.6297 - classification_loss: 0.5543\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.1779 - regression_loss: 1.6263 - classification_loss: 0.5516\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.1790 - regression_loss: 1.6278 - classification_loss: 0.5512\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.1840 - regression_loss: 1.6324 - classification_loss: 0.5516\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.1881 - regression_loss: 1.6335 - classification_loss: 0.5545\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.1921 - regression_loss: 1.6361 - classification_loss: 0.5560\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1889 - regression_loss: 1.6339 - classification_loss: 0.5550\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1850 - regression_loss: 1.6319 - classification_loss: 0.5532\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1866 - regression_loss: 1.6337 - classification_loss: 0.5529\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1831 - regression_loss: 1.6327 - classification_loss: 0.5504\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1873 - regression_loss: 1.6336 - classification_loss: 0.5537\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1867 - regression_loss: 1.6344 - classification_loss: 0.5524\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1797 - regression_loss: 1.6296 - classification_loss: 0.5501\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1777 - regression_loss: 1.6282 - classification_loss: 0.5495\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1842 - regression_loss: 1.6318 - classification_loss: 0.5524\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1856 - regression_loss: 1.6329 - classification_loss: 0.5527\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1826 - regression_loss: 1.6303 - classification_loss: 0.5522\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1822 - regression_loss: 1.6305 - classification_loss: 0.5517\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1826 - regression_loss: 1.6306 - classification_loss: 0.5519\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.1788 - regression_loss: 1.6276 - classification_loss: 0.5513\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.1791 - regression_loss: 1.6279 - classification_loss: 0.5512\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.1774 - regression_loss: 1.6258 - classification_loss: 0.5516\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.1767 - regression_loss: 1.6254 - classification_loss: 0.5512\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1774 - regression_loss: 1.6266 - classification_loss: 0.5508\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1777 - regression_loss: 1.6269 - classification_loss: 0.5508\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1825 - regression_loss: 1.6316 - classification_loss: 0.5510\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1822 - regression_loss: 1.6314 - classification_loss: 0.5508\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1797 - regression_loss: 1.6297 - classification_loss: 0.5500\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1756 - regression_loss: 1.6271 - classification_loss: 0.5485\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1771 - regression_loss: 1.6276 - classification_loss: 0.5495\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1698 - regression_loss: 1.6223 - classification_loss: 0.5475\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1701 - regression_loss: 1.6222 - classification_loss: 0.5479\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1673 - regression_loss: 1.6203 - classification_loss: 0.5470\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1699 - regression_loss: 1.6228 - classification_loss: 0.5472\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1664 - regression_loss: 1.6194 - classification_loss: 0.5470 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1693 - regression_loss: 1.6213 - classification_loss: 0.5480\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1673 - regression_loss: 1.6198 - classification_loss: 0.5475\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1605 - regression_loss: 1.6148 - classification_loss: 0.5457\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1640 - regression_loss: 1.6182 - classification_loss: 0.5458\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1682 - regression_loss: 1.6209 - classification_loss: 0.5474\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1696 - regression_loss: 1.6226 - classification_loss: 0.5470\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1677 - regression_loss: 1.6218 - classification_loss: 0.5459\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1673 - regression_loss: 1.6213 - classification_loss: 0.5460\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1669 - regression_loss: 1.6204 - classification_loss: 0.5465\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1720 - regression_loss: 1.6246 - classification_loss: 0.5474\n",
            "Epoch 12: saving model to ./snapshots\\resnet50_csv_12.h5\n",
            "\n",
            "86/86 [==============================] - 84s 967ms/step - loss: 2.1720 - regression_loss: 1.6246 - classification_loss: 0.5474 - lr: 1.0000e-05\n",
            "Epoch 13/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:58 - loss: 1.8664 - regression_loss: 1.4141 - classification_loss: 0.4523\n",
            " 2/86 [..............................] - ETA: 1:21 - loss: 2.0614 - regression_loss: 1.5470 - classification_loss: 0.5144\n",
            " 3/86 [>.............................] - ETA: 1:21 - loss: 2.0529 - regression_loss: 1.5471 - classification_loss: 0.5058\n",
            " 4/86 [>.............................] - ETA: 1:17 - loss: 2.0664 - regression_loss: 1.5645 - classification_loss: 0.5019\n",
            " 5/86 [>.............................] - ETA: 1:13 - loss: 2.0772 - regression_loss: 1.5702 - classification_loss: 0.5070\n",
            " 6/86 [=>............................] - ETA: 1:15 - loss: 2.1123 - regression_loss: 1.6053 - classification_loss: 0.5070\n",
            " 7/86 [=>............................] - ETA: 1:18 - loss: 2.1303 - regression_loss: 1.6134 - classification_loss: 0.5169\n",
            " 8/86 [=>............................] - ETA: 1:17 - loss: 2.1797 - regression_loss: 1.6509 - classification_loss: 0.5288\n",
            " 9/86 [==>...........................] - ETA: 1:15 - loss: 2.1931 - regression_loss: 1.6591 - classification_loss: 0.5340\n",
            "10/86 [==>...........................] - ETA: 1:15 - loss: 2.1941 - regression_loss: 1.6606 - classification_loss: 0.5335\n",
            "11/86 [==>...........................] - ETA: 1:13 - loss: 2.1613 - regression_loss: 1.6329 - classification_loss: 0.5284\n",
            "12/86 [===>..........................] - ETA: 1:12 - loss: 2.1497 - regression_loss: 1.6290 - classification_loss: 0.5207\n",
            "13/86 [===>..........................] - ETA: 1:11 - loss: 2.1903 - regression_loss: 1.6597 - classification_loss: 0.5306\n",
            "14/86 [===>..........................] - ETA: 1:10 - loss: 2.2002 - regression_loss: 1.6683 - classification_loss: 0.5318\n",
            "15/86 [====>.........................] - ETA: 1:09 - loss: 2.2074 - regression_loss: 1.6726 - classification_loss: 0.5349\n",
            "16/86 [====>.........................] - ETA: 1:08 - loss: 2.2066 - regression_loss: 1.6722 - classification_loss: 0.5344\n",
            "17/86 [====>.........................] - ETA: 1:07 - loss: 2.2258 - regression_loss: 1.6877 - classification_loss: 0.5380\n",
            "18/86 [=====>........................] - ETA: 1:06 - loss: 2.2312 - regression_loss: 1.6948 - classification_loss: 0.5363\n",
            "19/86 [=====>........................] - ETA: 1:05 - loss: 2.2406 - regression_loss: 1.7006 - classification_loss: 0.5400\n",
            "20/86 [=====>........................] - ETA: 1:04 - loss: 2.2300 - regression_loss: 1.6916 - classification_loss: 0.5384\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.2260 - regression_loss: 1.6856 - classification_loss: 0.5404\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.2257 - regression_loss: 1.6849 - classification_loss: 0.5409\n",
            "23/86 [=======>......................] - ETA: 1:03 - loss: 2.2271 - regression_loss: 1.6866 - classification_loss: 0.5405\n",
            "24/86 [=======>......................] - ETA: 1:02 - loss: 2.2200 - regression_loss: 1.6812 - classification_loss: 0.5388\n",
            "25/86 [=======>......................] - ETA: 1:02 - loss: 2.2171 - regression_loss: 1.6777 - classification_loss: 0.5394\n",
            "26/86 [========>.....................] - ETA: 1:00 - loss: 2.2155 - regression_loss: 1.6746 - classification_loss: 0.5409\n",
            "27/86 [========>.....................] - ETA: 59s - loss: 2.2057 - regression_loss: 1.6666 - classification_loss: 0.5391 \n",
            "28/86 [========>.....................] - ETA: 58s - loss: 2.2050 - regression_loss: 1.6667 - classification_loss: 0.5383\n",
            "29/86 [=========>....................] - ETA: 57s - loss: 2.1929 - regression_loss: 1.6548 - classification_loss: 0.5381\n",
            "30/86 [=========>....................] - ETA: 55s - loss: 2.1798 - regression_loss: 1.6439 - classification_loss: 0.5358\n",
            "31/86 [=========>....................] - ETA: 54s - loss: 2.1804 - regression_loss: 1.6420 - classification_loss: 0.5385\n",
            "32/86 [==========>...................] - ETA: 53s - loss: 2.1748 - regression_loss: 1.6375 - classification_loss: 0.5373\n",
            "33/86 [==========>...................] - ETA: 52s - loss: 2.1738 - regression_loss: 1.6386 - classification_loss: 0.5352\n",
            "34/86 [==========>...................] - ETA: 51s - loss: 2.1707 - regression_loss: 1.6352 - classification_loss: 0.5356\n",
            "35/86 [===========>..................] - ETA: 50s - loss: 2.1545 - regression_loss: 1.6228 - classification_loss: 0.5316\n",
            "36/86 [===========>..................] - ETA: 49s - loss: 2.1511 - regression_loss: 1.6189 - classification_loss: 0.5322\n",
            "37/86 [===========>..................] - ETA: 48s - loss: 2.1451 - regression_loss: 1.6134 - classification_loss: 0.5317\n",
            "38/86 [============>.................] - ETA: 47s - loss: 2.1421 - regression_loss: 1.6112 - classification_loss: 0.5310\n",
            "39/86 [============>.................] - ETA: 46s - loss: 2.1389 - regression_loss: 1.6079 - classification_loss: 0.5310\n",
            "40/86 [============>.................] - ETA: 45s - loss: 2.1406 - regression_loss: 1.6089 - classification_loss: 0.5317\n",
            "41/86 [=============>................] - ETA: 44s - loss: 2.1418 - regression_loss: 1.6095 - classification_loss: 0.5324\n",
            "42/86 [=============>................] - ETA: 43s - loss: 2.1350 - regression_loss: 1.6046 - classification_loss: 0.5304\n",
            "43/86 [==============>...............] - ETA: 42s - loss: 2.1293 - regression_loss: 1.6005 - classification_loss: 0.5288\n",
            "44/86 [==============>...............] - ETA: 41s - loss: 2.1256 - regression_loss: 1.5984 - classification_loss: 0.5272\n",
            "45/86 [==============>...............] - ETA: 40s - loss: 2.1392 - regression_loss: 1.6098 - classification_loss: 0.5294\n",
            "46/86 [===============>..............] - ETA: 39s - loss: 2.1453 - regression_loss: 1.6149 - classification_loss: 0.5305\n",
            "47/86 [===============>..............] - ETA: 38s - loss: 2.1438 - regression_loss: 1.6144 - classification_loss: 0.5294\n",
            "48/86 [===============>..............] - ETA: 37s - loss: 2.1393 - regression_loss: 1.6114 - classification_loss: 0.5280\n",
            "49/86 [================>.............] - ETA: 36s - loss: 2.1351 - regression_loss: 1.6077 - classification_loss: 0.5274\n",
            "50/86 [================>.............] - ETA: 35s - loss: 2.1332 - regression_loss: 1.6056 - classification_loss: 0.5276\n",
            "51/86 [================>.............] - ETA: 34s - loss: 2.1335 - regression_loss: 1.6057 - classification_loss: 0.5278\n",
            "52/86 [=================>............] - ETA: 33s - loss: 2.1316 - regression_loss: 1.6043 - classification_loss: 0.5273\n",
            "53/86 [=================>............] - ETA: 32s - loss: 2.1332 - regression_loss: 1.6060 - classification_loss: 0.5272\n",
            "54/86 [=================>............] - ETA: 31s - loss: 2.1361 - regression_loss: 1.6088 - classification_loss: 0.5273\n",
            "55/86 [==================>...........] - ETA: 30s - loss: 2.1408 - regression_loss: 1.6118 - classification_loss: 0.5290\n",
            "56/86 [==================>...........] - ETA: 29s - loss: 2.1428 - regression_loss: 1.6136 - classification_loss: 0.5292\n",
            "57/86 [==================>...........] - ETA: 28s - loss: 2.1391 - regression_loss: 1.6101 - classification_loss: 0.5290\n",
            "58/86 [===================>..........] - ETA: 27s - loss: 2.1382 - regression_loss: 1.6096 - classification_loss: 0.5286\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.1312 - regression_loss: 1.6044 - classification_loss: 0.5268\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.1311 - regression_loss: 1.6037 - classification_loss: 0.5274\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.1288 - regression_loss: 1.6022 - classification_loss: 0.5266\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.1229 - regression_loss: 1.5981 - classification_loss: 0.5248\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.1191 - regression_loss: 1.5955 - classification_loss: 0.5236\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.1137 - regression_loss: 1.5915 - classification_loss: 0.5222\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1165 - regression_loss: 1.5945 - classification_loss: 0.5221\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1210 - regression_loss: 1.5981 - classification_loss: 0.5230\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1197 - regression_loss: 1.5966 - classification_loss: 0.5232\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1127 - regression_loss: 1.5914 - classification_loss: 0.5213\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1095 - regression_loss: 1.5897 - classification_loss: 0.5198\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1065 - regression_loss: 1.5878 - classification_loss: 0.5187\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1070 - regression_loss: 1.5884 - classification_loss: 0.5186\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1089 - regression_loss: 1.5883 - classification_loss: 0.5206\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1088 - regression_loss: 1.5874 - classification_loss: 0.5214\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1073 - regression_loss: 1.5845 - classification_loss: 0.5228\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1120 - regression_loss: 1.5881 - classification_loss: 0.5239\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1095 - regression_loss: 1.5863 - classification_loss: 0.5232 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1114 - regression_loss: 1.5871 - classification_loss: 0.5243\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1106 - regression_loss: 1.5866 - classification_loss: 0.5240\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1128 - regression_loss: 1.5886 - classification_loss: 0.5241\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1137 - regression_loss: 1.5902 - classification_loss: 0.5235\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1147 - regression_loss: 1.5906 - classification_loss: 0.5241\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1077 - regression_loss: 1.5856 - classification_loss: 0.5221\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1040 - regression_loss: 1.5829 - classification_loss: 0.5211\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1049 - regression_loss: 1.5836 - classification_loss: 0.5213\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1092 - regression_loss: 1.5869 - classification_loss: 0.5223\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1115 - regression_loss: 1.5888 - classification_loss: 0.5226\n",
            "Epoch 13: saving model to ./snapshots\\resnet50_csv_13.h5\n",
            "\n",
            "86/86 [==============================] - 84s 977ms/step - loss: 2.1115 - regression_loss: 1.5888 - classification_loss: 0.5226 - lr: 1.0000e-05\n",
            "Epoch 14/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:13 - loss: 2.4491 - regression_loss: 1.9041 - classification_loss: 0.5450\n",
            " 2/86 [..............................] - ETA: 58s - loss: 2.2993 - regression_loss: 1.7669 - classification_loss: 0.5324 \n",
            " 3/86 [>.............................] - ETA: 1:08 - loss: 2.3286 - regression_loss: 1.7683 - classification_loss: 0.5602\n",
            " 4/86 [>.............................] - ETA: 1:08 - loss: 2.2776 - regression_loss: 1.7270 - classification_loss: 0.5506\n",
            " 5/86 [>.............................] - ETA: 1:12 - loss: 2.1825 - regression_loss: 1.6553 - classification_loss: 0.5272\n",
            " 6/86 [=>............................] - ETA: 1:10 - loss: 2.1282 - regression_loss: 1.6011 - classification_loss: 0.5271\n",
            " 7/86 [=>............................] - ETA: 1:12 - loss: 2.1313 - regression_loss: 1.6054 - classification_loss: 0.5260\n",
            " 8/86 [=>............................] - ETA: 1:11 - loss: 2.1717 - regression_loss: 1.6353 - classification_loss: 0.5364\n",
            " 9/86 [==>...........................] - ETA: 1:09 - loss: 2.1665 - regression_loss: 1.6287 - classification_loss: 0.5378\n",
            "10/86 [==>...........................] - ETA: 1:07 - loss: 2.1055 - regression_loss: 1.5835 - classification_loss: 0.5220\n",
            "11/86 [==>...........................] - ETA: 1:06 - loss: 2.1061 - regression_loss: 1.5844 - classification_loss: 0.5218\n",
            "12/86 [===>..........................] - ETA: 1:05 - loss: 2.0921 - regression_loss: 1.5748 - classification_loss: 0.5173\n",
            "13/86 [===>..........................] - ETA: 1:05 - loss: 2.0958 - regression_loss: 1.5751 - classification_loss: 0.5207\n",
            "14/86 [===>..........................] - ETA: 1:04 - loss: 2.0794 - regression_loss: 1.5619 - classification_loss: 0.5175\n",
            "15/86 [====>.........................] - ETA: 1:03 - loss: 2.0751 - regression_loss: 1.5583 - classification_loss: 0.5168\n",
            "16/86 [====>.........................] - ETA: 1:02 - loss: 2.0854 - regression_loss: 1.5650 - classification_loss: 0.5205\n",
            "17/86 [====>.........................] - ETA: 1:01 - loss: 2.0722 - regression_loss: 1.5552 - classification_loss: 0.5170\n",
            "18/86 [=====>........................] - ETA: 1:00 - loss: 2.0712 - regression_loss: 1.5560 - classification_loss: 0.5152\n",
            "19/86 [=====>........................] - ETA: 1:00 - loss: 2.0871 - regression_loss: 1.5627 - classification_loss: 0.5243\n",
            "20/86 [=====>........................] - ETA: 58s - loss: 2.0809 - regression_loss: 1.5556 - classification_loss: 0.5254 \n",
            "21/86 [======>.......................] - ETA: 57s - loss: 2.0883 - regression_loss: 1.5604 - classification_loss: 0.5279\n",
            "22/86 [======>.......................] - ETA: 56s - loss: 2.0980 - regression_loss: 1.5707 - classification_loss: 0.5273\n",
            "23/86 [=======>......................] - ETA: 56s - loss: 2.1125 - regression_loss: 1.5827 - classification_loss: 0.5298\n",
            "24/86 [=======>......................] - ETA: 56s - loss: 2.1132 - regression_loss: 1.5827 - classification_loss: 0.5305\n",
            "25/86 [=======>......................] - ETA: 55s - loss: 2.1089 - regression_loss: 1.5784 - classification_loss: 0.5306\n",
            "26/86 [========>.....................] - ETA: 54s - loss: 2.0960 - regression_loss: 1.5700 - classification_loss: 0.5260\n",
            "27/86 [========>.....................] - ETA: 52s - loss: 2.0932 - regression_loss: 1.5673 - classification_loss: 0.5259\n",
            "28/86 [========>.....................] - ETA: 52s - loss: 2.0971 - regression_loss: 1.5721 - classification_loss: 0.5251\n",
            "29/86 [=========>....................] - ETA: 51s - loss: 2.1067 - regression_loss: 1.5808 - classification_loss: 0.5259\n",
            "30/86 [=========>....................] - ETA: 51s - loss: 2.0996 - regression_loss: 1.5747 - classification_loss: 0.5249\n",
            "31/86 [=========>....................] - ETA: 50s - loss: 2.1139 - regression_loss: 1.5842 - classification_loss: 0.5298\n",
            "32/86 [==========>...................] - ETA: 49s - loss: 2.1242 - regression_loss: 1.5928 - classification_loss: 0.5315\n",
            "33/86 [==========>...................] - ETA: 48s - loss: 2.1261 - regression_loss: 1.5953 - classification_loss: 0.5308\n",
            "34/86 [==========>...................] - ETA: 47s - loss: 2.1289 - regression_loss: 1.5970 - classification_loss: 0.5318\n",
            "35/86 [===========>..................] - ETA: 46s - loss: 2.1312 - regression_loss: 1.6002 - classification_loss: 0.5309\n",
            "36/86 [===========>..................] - ETA: 45s - loss: 2.1345 - regression_loss: 1.6022 - classification_loss: 0.5322\n",
            "37/86 [===========>..................] - ETA: 44s - loss: 2.1368 - regression_loss: 1.6026 - classification_loss: 0.5342\n",
            "38/86 [============>.................] - ETA: 43s - loss: 2.1299 - regression_loss: 1.5982 - classification_loss: 0.5317\n",
            "39/86 [============>.................] - ETA: 42s - loss: 2.1197 - regression_loss: 1.5902 - classification_loss: 0.5294\n",
            "40/86 [============>.................] - ETA: 42s - loss: 2.1235 - regression_loss: 1.5946 - classification_loss: 0.5290\n",
            "41/86 [=============>................] - ETA: 41s - loss: 2.1263 - regression_loss: 1.5974 - classification_loss: 0.5289\n",
            "42/86 [=============>................] - ETA: 40s - loss: 2.1236 - regression_loss: 1.5951 - classification_loss: 0.5285\n",
            "43/86 [==============>...............] - ETA: 39s - loss: 2.1220 - regression_loss: 1.5939 - classification_loss: 0.5281\n",
            "44/86 [==============>...............] - ETA: 38s - loss: 2.1205 - regression_loss: 1.5917 - classification_loss: 0.5288\n",
            "45/86 [==============>...............] - ETA: 37s - loss: 2.1213 - regression_loss: 1.5925 - classification_loss: 0.5289\n",
            "46/86 [===============>..............] - ETA: 36s - loss: 2.1212 - regression_loss: 1.5932 - classification_loss: 0.5280\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.1173 - regression_loss: 1.5911 - classification_loss: 0.5262\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.1193 - regression_loss: 1.5929 - classification_loss: 0.5264\n",
            "49/86 [================>.............] - ETA: 34s - loss: 2.1180 - regression_loss: 1.5920 - classification_loss: 0.5260\n",
            "50/86 [================>.............] - ETA: 33s - loss: 2.1193 - regression_loss: 1.5922 - classification_loss: 0.5271\n",
            "51/86 [================>.............] - ETA: 32s - loss: 2.1172 - regression_loss: 1.5911 - classification_loss: 0.5260\n",
            "52/86 [=================>............] - ETA: 31s - loss: 2.1150 - regression_loss: 1.5899 - classification_loss: 0.5251\n",
            "53/86 [=================>............] - ETA: 30s - loss: 2.1118 - regression_loss: 1.5877 - classification_loss: 0.5241\n",
            "54/86 [=================>............] - ETA: 29s - loss: 2.1143 - regression_loss: 1.5890 - classification_loss: 0.5252\n",
            "55/86 [==================>...........] - ETA: 28s - loss: 2.1140 - regression_loss: 1.5903 - classification_loss: 0.5237\n",
            "56/86 [==================>...........] - ETA: 27s - loss: 2.1196 - regression_loss: 1.5935 - classification_loss: 0.5261\n",
            "57/86 [==================>...........] - ETA: 26s - loss: 2.1234 - regression_loss: 1.5959 - classification_loss: 0.5275\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1259 - regression_loss: 1.5983 - classification_loss: 0.5275\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1273 - regression_loss: 1.5995 - classification_loss: 0.5278\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1256 - regression_loss: 1.5996 - classification_loss: 0.5260\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1254 - regression_loss: 1.5994 - classification_loss: 0.5260\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1238 - regression_loss: 1.5975 - classification_loss: 0.5263\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1217 - regression_loss: 1.5965 - classification_loss: 0.5252\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1214 - regression_loss: 1.5962 - classification_loss: 0.5252\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1277 - regression_loss: 1.6000 - classification_loss: 0.5277\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.1345 - regression_loss: 1.6057 - classification_loss: 0.5288\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.1323 - regression_loss: 1.6048 - classification_loss: 0.5275\n",
            "68/86 [======================>.......] - ETA: 16s - loss: 2.1356 - regression_loss: 1.6058 - classification_loss: 0.5299\n",
            "69/86 [=======================>......] - ETA: 15s - loss: 2.1394 - regression_loss: 1.6086 - classification_loss: 0.5308\n",
            "70/86 [=======================>......] - ETA: 14s - loss: 2.1355 - regression_loss: 1.6057 - classification_loss: 0.5298\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1370 - regression_loss: 1.6074 - classification_loss: 0.5296\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1351 - regression_loss: 1.6062 - classification_loss: 0.5289\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1329 - regression_loss: 1.6052 - classification_loss: 0.5277\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1340 - regression_loss: 1.6066 - classification_loss: 0.5274\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1354 - regression_loss: 1.6074 - classification_loss: 0.5280\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1359 - regression_loss: 1.6089 - classification_loss: 0.5269 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1366 - regression_loss: 1.6094 - classification_loss: 0.5271\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1348 - regression_loss: 1.6082 - classification_loss: 0.5266\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1369 - regression_loss: 1.6100 - classification_loss: 0.5269\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1404 - regression_loss: 1.6126 - classification_loss: 0.5278\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1452 - regression_loss: 1.6162 - classification_loss: 0.5290\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1449 - regression_loss: 1.6155 - classification_loss: 0.5293\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1504 - regression_loss: 1.6200 - classification_loss: 0.5304\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1498 - regression_loss: 1.6192 - classification_loss: 0.5306\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1498 - regression_loss: 1.6193 - classification_loss: 0.5305\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1473 - regression_loss: 1.6178 - classification_loss: 0.5295\n",
            "Epoch 14: saving model to ./snapshots\\resnet50_csv_14.h5\n",
            "\n",
            "86/86 [==============================] - 82s 941ms/step - loss: 2.1473 - regression_loss: 1.6178 - classification_loss: 0.5295 - lr: 1.0000e-05\n",
            "Epoch 15/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:07 - loss: 1.9241 - regression_loss: 1.4856 - classification_loss: 0.4384\n",
            " 2/86 [..............................] - ETA: 52s - loss: 2.1938 - regression_loss: 1.6726 - classification_loss: 0.5211 \n",
            " 3/86 [>.............................] - ETA: 1:04 - loss: 2.1307 - regression_loss: 1.5961 - classification_loss: 0.5346\n",
            " 4/86 [>.............................] - ETA: 1:06 - loss: 2.1260 - regression_loss: 1.5836 - classification_loss: 0.5424\n",
            " 5/86 [>.............................] - ETA: 1:06 - loss: 2.0595 - regression_loss: 1.5425 - classification_loss: 0.5170\n",
            " 6/86 [=>............................] - ETA: 1:08 - loss: 2.0803 - regression_loss: 1.5585 - classification_loss: 0.5218\n",
            " 7/86 [=>............................] - ETA: 1:11 - loss: 2.1013 - regression_loss: 1.5709 - classification_loss: 0.5304\n",
            " 8/86 [=>............................] - ETA: 1:11 - loss: 2.0946 - regression_loss: 1.5746 - classification_loss: 0.5200\n",
            " 9/86 [==>...........................] - ETA: 1:11 - loss: 2.1325 - regression_loss: 1.6086 - classification_loss: 0.5239\n",
            "10/86 [==>...........................] - ETA: 1:09 - loss: 2.1055 - regression_loss: 1.5862 - classification_loss: 0.5194\n",
            "11/86 [==>...........................] - ETA: 1:10 - loss: 2.1254 - regression_loss: 1.5991 - classification_loss: 0.5263\n",
            "12/86 [===>..........................] - ETA: 1:09 - loss: 2.1611 - regression_loss: 1.6245 - classification_loss: 0.5366\n",
            "13/86 [===>..........................] - ETA: 1:08 - loss: 2.1760 - regression_loss: 1.6372 - classification_loss: 0.5388\n",
            "14/86 [===>..........................] - ETA: 1:08 - loss: 2.1637 - regression_loss: 1.6299 - classification_loss: 0.5338\n",
            "15/86 [====>.........................] - ETA: 1:07 - loss: 2.1852 - regression_loss: 1.6474 - classification_loss: 0.5378\n",
            "16/86 [====>.........................] - ETA: 1:07 - loss: 2.1787 - regression_loss: 1.6460 - classification_loss: 0.5326\n",
            "17/86 [====>.........................] - ETA: 1:06 - loss: 2.1831 - regression_loss: 1.6508 - classification_loss: 0.5324\n",
            "18/86 [=====>........................] - ETA: 1:04 - loss: 2.1706 - regression_loss: 1.6389 - classification_loss: 0.5317\n",
            "19/86 [=====>........................] - ETA: 1:04 - loss: 2.1583 - regression_loss: 1.6293 - classification_loss: 0.5290\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.1660 - regression_loss: 1.6379 - classification_loss: 0.5281\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.1876 - regression_loss: 1.6554 - classification_loss: 0.5322\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.1716 - regression_loss: 1.6459 - classification_loss: 0.5257\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.1600 - regression_loss: 1.6348 - classification_loss: 0.5252\n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.1563 - regression_loss: 1.6319 - classification_loss: 0.5244 \n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.1554 - regression_loss: 1.6293 - classification_loss: 0.5262\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.1372 - regression_loss: 1.6164 - classification_loss: 0.5209\n",
            "27/86 [========>.....................] - ETA: 55s - loss: 2.1484 - regression_loss: 1.6239 - classification_loss: 0.5245\n",
            "28/86 [========>.....................] - ETA: 54s - loss: 2.1488 - regression_loss: 1.6237 - classification_loss: 0.5251\n",
            "29/86 [=========>....................] - ETA: 53s - loss: 2.1461 - regression_loss: 1.6217 - classification_loss: 0.5244\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.1532 - regression_loss: 1.6278 - classification_loss: 0.5254\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.1559 - regression_loss: 1.6293 - classification_loss: 0.5265\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.1570 - regression_loss: 1.6291 - classification_loss: 0.5279\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.1583 - regression_loss: 1.6295 - classification_loss: 0.5289\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.1590 - regression_loss: 1.6301 - classification_loss: 0.5289\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.1497 - regression_loss: 1.6212 - classification_loss: 0.5285\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.1454 - regression_loss: 1.6174 - classification_loss: 0.5280\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.1399 - regression_loss: 1.6120 - classification_loss: 0.5279\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.1419 - regression_loss: 1.6120 - classification_loss: 0.5300\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.1368 - regression_loss: 1.6097 - classification_loss: 0.5270\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.1279 - regression_loss: 1.6029 - classification_loss: 0.5250\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.1312 - regression_loss: 1.6070 - classification_loss: 0.5242\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1344 - regression_loss: 1.6099 - classification_loss: 0.5245\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1311 - regression_loss: 1.6077 - classification_loss: 0.5234\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1291 - regression_loss: 1.6073 - classification_loss: 0.5217\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.1286 - regression_loss: 1.6070 - classification_loss: 0.5215\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1290 - regression_loss: 1.6084 - classification_loss: 0.5206\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1352 - regression_loss: 1.6143 - classification_loss: 0.5209\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1349 - regression_loss: 1.6138 - classification_loss: 0.5211\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1348 - regression_loss: 1.6143 - classification_loss: 0.5205\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1211 - regression_loss: 1.6042 - classification_loss: 0.5170\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1272 - regression_loss: 1.6086 - classification_loss: 0.5186\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1265 - regression_loss: 1.6076 - classification_loss: 0.5189\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1270 - regression_loss: 1.6071 - classification_loss: 0.5198\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1214 - regression_loss: 1.6029 - classification_loss: 0.5184\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1220 - regression_loss: 1.6034 - classification_loss: 0.5185\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1196 - regression_loss: 1.6017 - classification_loss: 0.5179\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1228 - regression_loss: 1.6055 - classification_loss: 0.5173\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1200 - regression_loss: 1.6030 - classification_loss: 0.5170\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1218 - regression_loss: 1.6031 - classification_loss: 0.5187\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1245 - regression_loss: 1.6058 - classification_loss: 0.5187\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1201 - regression_loss: 1.6031 - classification_loss: 0.5170\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1207 - regression_loss: 1.6034 - classification_loss: 0.5172\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.1160 - regression_loss: 1.6001 - classification_loss: 0.5159\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.1146 - regression_loss: 1.5988 - classification_loss: 0.5158\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1139 - regression_loss: 1.5991 - classification_loss: 0.5148\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1156 - regression_loss: 1.6006 - classification_loss: 0.5150\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1164 - regression_loss: 1.6017 - classification_loss: 0.5147\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1163 - regression_loss: 1.6014 - classification_loss: 0.5148\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1200 - regression_loss: 1.6045 - classification_loss: 0.5155\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1242 - regression_loss: 1.6083 - classification_loss: 0.5158\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1238 - regression_loss: 1.6081 - classification_loss: 0.5157\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1203 - regression_loss: 1.6057 - classification_loss: 0.5146\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1220 - regression_loss: 1.6065 - classification_loss: 0.5155\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1258 - regression_loss: 1.6093 - classification_loss: 0.5165\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1288 - regression_loss: 1.6113 - classification_loss: 0.5175\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1293 - regression_loss: 1.6123 - classification_loss: 0.5170 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1293 - regression_loss: 1.6122 - classification_loss: 0.5170\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1258 - regression_loss: 1.6097 - classification_loss: 0.5161\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1261 - regression_loss: 1.6099 - classification_loss: 0.5162\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1225 - regression_loss: 1.6070 - classification_loss: 0.5154\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1221 - regression_loss: 1.6070 - classification_loss: 0.5151\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1221 - regression_loss: 1.6065 - classification_loss: 0.5157\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1224 - regression_loss: 1.6071 - classification_loss: 0.5153\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1212 - regression_loss: 1.6067 - classification_loss: 0.5144\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1222 - regression_loss: 1.6068 - classification_loss: 0.5154\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1243 - regression_loss: 1.6088 - classification_loss: 0.5154\n",
            "Epoch 15: saving model to ./snapshots\\resnet50_csv_15.h5\n",
            "\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "\n",
            "86/86 [==============================] - 83s 957ms/step - loss: 2.1243 - regression_loss: 1.6088 - classification_loss: 0.5154 - lr: 1.0000e-05\n",
            "Epoch 16/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:50 - loss: 2.1253 - regression_loss: 1.6140 - classification_loss: 0.5113\n",
            " 2/86 [..............................] - ETA: 1:17 - loss: 2.1588 - regression_loss: 1.6182 - classification_loss: 0.5406\n",
            " 3/86 [>.............................] - ETA: 1:17 - loss: 2.2507 - regression_loss: 1.6765 - classification_loss: 0.5742\n",
            " 4/86 [>.............................] - ETA: 1:16 - loss: 2.2225 - regression_loss: 1.6538 - classification_loss: 0.5687\n",
            " 5/86 [>.............................] - ETA: 1:16 - loss: 2.0743 - regression_loss: 1.5530 - classification_loss: 0.5214\n",
            " 6/86 [=>............................] - ETA: 1:13 - loss: 2.1382 - regression_loss: 1.6086 - classification_loss: 0.5296\n",
            " 7/86 [=>............................] - ETA: 1:13 - loss: 2.1248 - regression_loss: 1.6060 - classification_loss: 0.5189\n",
            " 8/86 [=>............................] - ETA: 1:16 - loss: 2.1745 - regression_loss: 1.6499 - classification_loss: 0.5246\n",
            " 9/86 [==>...........................] - ETA: 1:16 - loss: 2.1648 - regression_loss: 1.6353 - classification_loss: 0.5295\n",
            "10/86 [==>...........................] - ETA: 1:15 - loss: 2.1418 - regression_loss: 1.6113 - classification_loss: 0.5305\n",
            "11/86 [==>...........................] - ETA: 1:14 - loss: 2.1543 - regression_loss: 1.6242 - classification_loss: 0.5301\n",
            "12/86 [===>..........................] - ETA: 1:12 - loss: 2.1478 - regression_loss: 1.6198 - classification_loss: 0.5281\n",
            "13/86 [===>..........................] - ETA: 1:11 - loss: 2.1326 - regression_loss: 1.6104 - classification_loss: 0.5222\n",
            "14/86 [===>..........................] - ETA: 1:10 - loss: 2.1247 - regression_loss: 1.6067 - classification_loss: 0.5180\n",
            "15/86 [====>.........................] - ETA: 1:08 - loss: 2.1164 - regression_loss: 1.5991 - classification_loss: 0.5173\n",
            "16/86 [====>.........................] - ETA: 1:07 - loss: 2.1148 - regression_loss: 1.5962 - classification_loss: 0.5186\n",
            "17/86 [====>.........................] - ETA: 1:06 - loss: 2.1043 - regression_loss: 1.5869 - classification_loss: 0.5174\n",
            "18/86 [=====>........................] - ETA: 1:04 - loss: 2.1029 - regression_loss: 1.5836 - classification_loss: 0.5193\n",
            "19/86 [=====>........................] - ETA: 1:03 - loss: 2.0933 - regression_loss: 1.5780 - classification_loss: 0.5154\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.1089 - regression_loss: 1.5910 - classification_loss: 0.5179\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.0966 - regression_loss: 1.5792 - classification_loss: 0.5174\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.0941 - regression_loss: 1.5797 - classification_loss: 0.5143\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.1054 - regression_loss: 1.5877 - classification_loss: 0.5177\n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.0940 - regression_loss: 1.5790 - classification_loss: 0.5150 \n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.0955 - regression_loss: 1.5797 - classification_loss: 0.5158\n",
            "26/86 [========>.....................] - ETA: 56s - loss: 2.0940 - regression_loss: 1.5779 - classification_loss: 0.5161\n",
            "27/86 [========>.....................] - ETA: 55s - loss: 2.0812 - regression_loss: 1.5693 - classification_loss: 0.5119\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.0781 - regression_loss: 1.5672 - classification_loss: 0.5110\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.0769 - regression_loss: 1.5650 - classification_loss: 0.5118\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.0686 - regression_loss: 1.5592 - classification_loss: 0.5095\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.0617 - regression_loss: 1.5515 - classification_loss: 0.5102\n",
            "32/86 [==========>...................] - ETA: 50s - loss: 2.0623 - regression_loss: 1.5527 - classification_loss: 0.5096\n",
            "33/86 [==========>...................] - ETA: 49s - loss: 2.0674 - regression_loss: 1.5560 - classification_loss: 0.5115\n",
            "34/86 [==========>...................] - ETA: 48s - loss: 2.0663 - regression_loss: 1.5553 - classification_loss: 0.5109\n",
            "35/86 [===========>..................] - ETA: 47s - loss: 2.0678 - regression_loss: 1.5562 - classification_loss: 0.5116\n",
            "36/86 [===========>..................] - ETA: 46s - loss: 2.0741 - regression_loss: 1.5618 - classification_loss: 0.5123\n",
            "37/86 [===========>..................] - ETA: 45s - loss: 2.0690 - regression_loss: 1.5574 - classification_loss: 0.5116\n",
            "38/86 [============>.................] - ETA: 44s - loss: 2.0694 - regression_loss: 1.5576 - classification_loss: 0.5117\n",
            "39/86 [============>.................] - ETA: 43s - loss: 2.0726 - regression_loss: 1.5605 - classification_loss: 0.5121\n",
            "40/86 [============>.................] - ETA: 42s - loss: 2.0715 - regression_loss: 1.5599 - classification_loss: 0.5117\n",
            "41/86 [=============>................] - ETA: 41s - loss: 2.0783 - regression_loss: 1.5646 - classification_loss: 0.5136\n",
            "42/86 [=============>................] - ETA: 40s - loss: 2.0742 - regression_loss: 1.5618 - classification_loss: 0.5125\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.0723 - regression_loss: 1.5625 - classification_loss: 0.5099\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.0738 - regression_loss: 1.5638 - classification_loss: 0.5100\n",
            "45/86 [==============>...............] - ETA: 37s - loss: 2.0733 - regression_loss: 1.5635 - classification_loss: 0.5098\n",
            "46/86 [===============>..............] - ETA: 36s - loss: 2.0722 - regression_loss: 1.5631 - classification_loss: 0.5091\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.0761 - regression_loss: 1.5653 - classification_loss: 0.5108\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.0692 - regression_loss: 1.5607 - classification_loss: 0.5084\n",
            "49/86 [================>.............] - ETA: 34s - loss: 2.0643 - regression_loss: 1.5572 - classification_loss: 0.5071\n",
            "50/86 [================>.............] - ETA: 33s - loss: 2.0636 - regression_loss: 1.5565 - classification_loss: 0.5072\n",
            "51/86 [================>.............] - ETA: 32s - loss: 2.0693 - regression_loss: 1.5596 - classification_loss: 0.5097\n",
            "52/86 [=================>............] - ETA: 31s - loss: 2.0730 - regression_loss: 1.5634 - classification_loss: 0.5096\n",
            "53/86 [=================>............] - ETA: 30s - loss: 2.0713 - regression_loss: 1.5624 - classification_loss: 0.5088\n",
            "54/86 [=================>............] - ETA: 29s - loss: 2.0697 - regression_loss: 1.5602 - classification_loss: 0.5095\n",
            "55/86 [==================>...........] - ETA: 28s - loss: 2.0736 - regression_loss: 1.5635 - classification_loss: 0.5101\n",
            "56/86 [==================>...........] - ETA: 27s - loss: 2.0749 - regression_loss: 1.5643 - classification_loss: 0.5106\n",
            "57/86 [==================>...........] - ETA: 26s - loss: 2.0707 - regression_loss: 1.5611 - classification_loss: 0.5096\n",
            "58/86 [===================>..........] - ETA: 25s - loss: 2.0712 - regression_loss: 1.5602 - classification_loss: 0.5110\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0764 - regression_loss: 1.5648 - classification_loss: 0.5116\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0784 - regression_loss: 1.5667 - classification_loss: 0.5117\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0815 - regression_loss: 1.5685 - classification_loss: 0.5130\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.0848 - regression_loss: 1.5715 - classification_loss: 0.5132\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.0863 - regression_loss: 1.5723 - classification_loss: 0.5140\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.0888 - regression_loss: 1.5732 - classification_loss: 0.5156\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.0892 - regression_loss: 1.5739 - classification_loss: 0.5154\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.0906 - regression_loss: 1.5749 - classification_loss: 0.5157\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.0937 - regression_loss: 1.5759 - classification_loss: 0.5178\n",
            "68/86 [======================>.......] - ETA: 16s - loss: 2.0885 - regression_loss: 1.5718 - classification_loss: 0.5167\n",
            "69/86 [=======================>......] - ETA: 15s - loss: 2.0888 - regression_loss: 1.5716 - classification_loss: 0.5172\n",
            "70/86 [=======================>......] - ETA: 14s - loss: 2.0906 - regression_loss: 1.5741 - classification_loss: 0.5165\n",
            "71/86 [=======================>......] - ETA: 13s - loss: 2.0876 - regression_loss: 1.5719 - classification_loss: 0.5157\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0939 - regression_loss: 1.5772 - classification_loss: 0.5168\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0912 - regression_loss: 1.5754 - classification_loss: 0.5158\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0891 - regression_loss: 1.5741 - classification_loss: 0.5150\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0823 - regression_loss: 1.5693 - classification_loss: 0.5131\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0835 - regression_loss: 1.5700 - classification_loss: 0.5135 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0831 - regression_loss: 1.5695 - classification_loss: 0.5136\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0835 - regression_loss: 1.5699 - classification_loss: 0.5136\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0826 - regression_loss: 1.5696 - classification_loss: 0.5129\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0835 - regression_loss: 1.5706 - classification_loss: 0.5129\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0778 - regression_loss: 1.5662 - classification_loss: 0.5116\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0817 - regression_loss: 1.5689 - classification_loss: 0.5128\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0789 - regression_loss: 1.5662 - classification_loss: 0.5127\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0809 - regression_loss: 1.5678 - classification_loss: 0.5130\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0849 - regression_loss: 1.5707 - classification_loss: 0.5142\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0858 - regression_loss: 1.5715 - classification_loss: 0.5143\n",
            "Epoch 16: saving model to ./snapshots\\resnet50_csv_16.h5\n",
            "\n",
            "86/86 [==============================] - 82s 951ms/step - loss: 2.0858 - regression_loss: 1.5715 - classification_loss: 0.5143 - lr: 1.0000e-06\n",
            "Epoch 17/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:08 - loss: 2.1972 - regression_loss: 1.5668 - classification_loss: 0.6305\n",
            " 2/86 [..............................] - ETA: 1:43 - loss: 2.0653 - regression_loss: 1.5037 - classification_loss: 0.5616\n",
            " 3/86 [>.............................] - ETA: 1:22 - loss: 1.9963 - regression_loss: 1.4887 - classification_loss: 0.5076\n",
            " 4/86 [>.............................] - ETA: 1:18 - loss: 2.0026 - regression_loss: 1.5123 - classification_loss: 0.4904\n",
            " 5/86 [>.............................] - ETA: 1:20 - loss: 2.0255 - regression_loss: 1.5276 - classification_loss: 0.4979\n",
            " 6/86 [=>............................] - ETA: 1:17 - loss: 2.0047 - regression_loss: 1.5128 - classification_loss: 0.4919\n",
            " 7/86 [=>............................] - ETA: 1:14 - loss: 2.0114 - regression_loss: 1.5217 - classification_loss: 0.4897\n",
            " 8/86 [=>............................] - ETA: 1:12 - loss: 2.0365 - regression_loss: 1.5411 - classification_loss: 0.4954\n",
            " 9/86 [==>...........................] - ETA: 1:10 - loss: 2.0244 - regression_loss: 1.5304 - classification_loss: 0.4940\n",
            "10/86 [==>...........................] - ETA: 1:09 - loss: 2.0039 - regression_loss: 1.5063 - classification_loss: 0.4976\n",
            "11/86 [==>...........................] - ETA: 1:08 - loss: 2.0091 - regression_loss: 1.5092 - classification_loss: 0.4999\n",
            "12/86 [===>..........................] - ETA: 1:10 - loss: 2.0058 - regression_loss: 1.5082 - classification_loss: 0.4976\n",
            "13/86 [===>..........................] - ETA: 1:09 - loss: 2.0080 - regression_loss: 1.5075 - classification_loss: 0.5005\n",
            "14/86 [===>..........................] - ETA: 1:08 - loss: 2.0010 - regression_loss: 1.4999 - classification_loss: 0.5012\n",
            "15/86 [====>.........................] - ETA: 1:08 - loss: 2.0074 - regression_loss: 1.5040 - classification_loss: 0.5034\n",
            "16/86 [====>.........................] - ETA: 1:07 - loss: 2.0089 - regression_loss: 1.5063 - classification_loss: 0.5025\n",
            "17/86 [====>.........................] - ETA: 1:06 - loss: 2.0036 - regression_loss: 1.5049 - classification_loss: 0.4987\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.0100 - regression_loss: 1.5108 - classification_loss: 0.4992\n",
            "19/86 [=====>........................] - ETA: 1:04 - loss: 2.0160 - regression_loss: 1.5197 - classification_loss: 0.4963\n",
            "20/86 [=====>........................] - ETA: 1:04 - loss: 2.0240 - regression_loss: 1.5268 - classification_loss: 0.4972\n",
            "21/86 [======>.......................] - ETA: 1:03 - loss: 2.0344 - regression_loss: 1.5355 - classification_loss: 0.4989\n",
            "22/86 [======>.......................] - ETA: 1:02 - loss: 2.0431 - regression_loss: 1.5429 - classification_loss: 0.5002\n",
            "23/86 [=======>......................] - ETA: 1:01 - loss: 2.0499 - regression_loss: 1.5473 - classification_loss: 0.5026\n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.0429 - regression_loss: 1.5402 - classification_loss: 0.5027 \n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.0355 - regression_loss: 1.5353 - classification_loss: 0.5002\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.0332 - regression_loss: 1.5336 - classification_loss: 0.4997\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.0464 - regression_loss: 1.5415 - classification_loss: 0.5049\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.0461 - regression_loss: 1.5412 - classification_loss: 0.5049\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.0527 - regression_loss: 1.5478 - classification_loss: 0.5049\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.0358 - regression_loss: 1.5356 - classification_loss: 0.5002\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.0343 - regression_loss: 1.5357 - classification_loss: 0.4986\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.0374 - regression_loss: 1.5391 - classification_loss: 0.4984\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.0330 - regression_loss: 1.5356 - classification_loss: 0.4974\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.0397 - regression_loss: 1.5394 - classification_loss: 0.5003\n",
            "35/86 [===========>..................] - ETA: 51s - loss: 2.0440 - regression_loss: 1.5416 - classification_loss: 0.5024\n",
            "36/86 [===========>..................] - ETA: 51s - loss: 2.0432 - regression_loss: 1.5402 - classification_loss: 0.5029\n",
            "37/86 [===========>..................] - ETA: 52s - loss: 2.0439 - regression_loss: 1.5400 - classification_loss: 0.5039\n",
            "38/86 [============>.................] - ETA: 52s - loss: 2.0495 - regression_loss: 1.5450 - classification_loss: 0.5045\n",
            "39/86 [============>.................] - ETA: 52s - loss: 2.0429 - regression_loss: 1.5406 - classification_loss: 0.5023\n",
            "40/86 [============>.................] - ETA: 52s - loss: 2.0462 - regression_loss: 1.5432 - classification_loss: 0.5029\n",
            "41/86 [=============>................] - ETA: 52s - loss: 2.0522 - regression_loss: 1.5480 - classification_loss: 0.5042\n",
            "42/86 [=============>................] - ETA: 52s - loss: 2.0579 - regression_loss: 1.5531 - classification_loss: 0.5048\n",
            "43/86 [==============>...............] - ETA: 51s - loss: 2.0525 - regression_loss: 1.5481 - classification_loss: 0.5043\n",
            "44/86 [==============>...............] - ETA: 49s - loss: 2.0600 - regression_loss: 1.5539 - classification_loss: 0.5060\n",
            "45/86 [==============>...............] - ETA: 48s - loss: 2.0571 - regression_loss: 1.5511 - classification_loss: 0.5060\n",
            "46/86 [===============>..............] - ETA: 46s - loss: 2.0539 - regression_loss: 1.5474 - classification_loss: 0.5065\n",
            "47/86 [===============>..............] - ETA: 45s - loss: 2.0650 - regression_loss: 1.5556 - classification_loss: 0.5094\n",
            "48/86 [===============>..............] - ETA: 44s - loss: 2.0637 - regression_loss: 1.5544 - classification_loss: 0.5093\n",
            "49/86 [================>.............] - ETA: 42s - loss: 2.0663 - regression_loss: 1.5565 - classification_loss: 0.5097\n",
            "50/86 [================>.............] - ETA: 41s - loss: 2.0591 - regression_loss: 1.5516 - classification_loss: 0.5075\n",
            "51/86 [================>.............] - ETA: 40s - loss: 2.0577 - regression_loss: 1.5501 - classification_loss: 0.5076\n",
            "52/86 [=================>............] - ETA: 38s - loss: 2.0585 - regression_loss: 1.5511 - classification_loss: 0.5074\n",
            "53/86 [=================>............] - ETA: 37s - loss: 2.0539 - regression_loss: 1.5480 - classification_loss: 0.5059\n",
            "54/86 [=================>............] - ETA: 36s - loss: 2.0555 - regression_loss: 1.5483 - classification_loss: 0.5072\n",
            "55/86 [==================>...........] - ETA: 35s - loss: 2.0565 - regression_loss: 1.5478 - classification_loss: 0.5086\n",
            "56/86 [==================>...........] - ETA: 33s - loss: 2.0454 - regression_loss: 1.5398 - classification_loss: 0.5056\n",
            "57/86 [==================>...........] - ETA: 32s - loss: 2.0510 - regression_loss: 1.5441 - classification_loss: 0.5069\n",
            "58/86 [===================>..........] - ETA: 31s - loss: 2.0534 - regression_loss: 1.5467 - classification_loss: 0.5067\n",
            "59/86 [===================>..........] - ETA: 30s - loss: 2.0587 - regression_loss: 1.5496 - classification_loss: 0.5090\n",
            "60/86 [===================>..........] - ETA: 29s - loss: 2.0588 - regression_loss: 1.5502 - classification_loss: 0.5086\n",
            "61/86 [====================>.........] - ETA: 27s - loss: 2.0593 - regression_loss: 1.5505 - classification_loss: 0.5087\n",
            "62/86 [====================>.........] - ETA: 26s - loss: 2.0590 - regression_loss: 1.5517 - classification_loss: 0.5073\n",
            "63/86 [====================>.........] - ETA: 25s - loss: 2.0635 - regression_loss: 1.5567 - classification_loss: 0.5068\n",
            "64/86 [=====================>........] - ETA: 24s - loss: 2.0650 - regression_loss: 1.5591 - classification_loss: 0.5059\n",
            "65/86 [=====================>........] - ETA: 23s - loss: 2.0642 - regression_loss: 1.5583 - classification_loss: 0.5058\n",
            "66/86 [======================>.......] - ETA: 22s - loss: 2.0703 - regression_loss: 1.5633 - classification_loss: 0.5070\n",
            "67/86 [======================>.......] - ETA: 20s - loss: 2.0760 - regression_loss: 1.5675 - classification_loss: 0.5085\n",
            "68/86 [======================>.......] - ETA: 19s - loss: 2.0732 - regression_loss: 1.5656 - classification_loss: 0.5076\n",
            "69/86 [=======================>......] - ETA: 18s - loss: 2.0717 - regression_loss: 1.5647 - classification_loss: 0.5070\n",
            "70/86 [=======================>......] - ETA: 17s - loss: 2.0723 - regression_loss: 1.5650 - classification_loss: 0.5073\n",
            "71/86 [=======================>......] - ETA: 16s - loss: 2.0702 - regression_loss: 1.5637 - classification_loss: 0.5065\n",
            "72/86 [========================>.....] - ETA: 15s - loss: 2.0688 - regression_loss: 1.5625 - classification_loss: 0.5064\n",
            "73/86 [========================>.....] - ETA: 14s - loss: 2.0736 - regression_loss: 1.5662 - classification_loss: 0.5074\n",
            "74/86 [========================>.....] - ETA: 13s - loss: 2.0760 - regression_loss: 1.5690 - classification_loss: 0.5070\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.0759 - regression_loss: 1.5694 - classification_loss: 0.5065\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.0799 - regression_loss: 1.5726 - classification_loss: 0.5074\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.0812 - regression_loss: 1.5738 - classification_loss: 0.5074 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.0794 - regression_loss: 1.5723 - classification_loss: 0.5071\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.0828 - regression_loss: 1.5748 - classification_loss: 0.5080\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.0856 - regression_loss: 1.5777 - classification_loss: 0.5078\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.0841 - regression_loss: 1.5770 - classification_loss: 0.5071\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.0847 - regression_loss: 1.5781 - classification_loss: 0.5066\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.0797 - regression_loss: 1.5743 - classification_loss: 0.5054\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.0800 - regression_loss: 1.5746 - classification_loss: 0.5054\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.0780 - regression_loss: 1.5736 - classification_loss: 0.5043\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0767 - regression_loss: 1.5722 - classification_loss: 0.5045\n",
            "Epoch 17: saving model to ./snapshots\\resnet50_csv_17.h5\n",
            "\n",
            "86/86 [==============================] - 93s 1s/step - loss: 2.0767 - regression_loss: 1.5722 - classification_loss: 0.5045 - lr: 1.0000e-06\n",
            "Epoch 18/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:24 - loss: 1.9217 - regression_loss: 1.4727 - classification_loss: 0.4489\n",
            " 2/86 [..............................] - ETA: 59s - loss: 2.0560 - regression_loss: 1.5775 - classification_loss: 0.4785 \n",
            " 3/86 [>.............................] - ETA: 1:05 - loss: 1.9860 - regression_loss: 1.5209 - classification_loss: 0.4651\n",
            " 4/86 [>.............................] - ETA: 1:08 - loss: 2.0479 - regression_loss: 1.5653 - classification_loss: 0.4826\n",
            " 5/86 [>.............................] - ETA: 1:09 - loss: 2.0134 - regression_loss: 1.5243 - classification_loss: 0.4890\n",
            " 6/86 [=>............................] - ETA: 1:10 - loss: 2.0423 - regression_loss: 1.5478 - classification_loss: 0.4945\n",
            " 7/86 [=>............................] - ETA: 1:10 - loss: 2.0330 - regression_loss: 1.5439 - classification_loss: 0.4891\n",
            " 8/86 [=>............................] - ETA: 1:13 - loss: 2.0238 - regression_loss: 1.5336 - classification_loss: 0.4902\n",
            " 9/86 [==>...........................] - ETA: 1:10 - loss: 2.0504 - regression_loss: 1.5525 - classification_loss: 0.4979\n",
            "10/86 [==>...........................] - ETA: 1:09 - loss: 2.0673 - regression_loss: 1.5631 - classification_loss: 0.5042\n",
            "11/86 [==>...........................] - ETA: 1:08 - loss: 2.0503 - regression_loss: 1.5463 - classification_loss: 0.5040\n",
            "12/86 [===>..........................] - ETA: 1:07 - loss: 2.0358 - regression_loss: 1.5332 - classification_loss: 0.5026\n",
            "13/86 [===>..........................] - ETA: 1:05 - loss: 2.0244 - regression_loss: 1.5234 - classification_loss: 0.5010\n",
            "14/86 [===>..........................] - ETA: 1:04 - loss: 2.0313 - regression_loss: 1.5290 - classification_loss: 0.5023\n",
            "15/86 [====>.........................] - ETA: 1:03 - loss: 2.0283 - regression_loss: 1.5262 - classification_loss: 0.5021\n",
            "16/86 [====>.........................] - ETA: 1:02 - loss: 2.0248 - regression_loss: 1.5260 - classification_loss: 0.4988\n",
            "17/86 [====>.........................] - ETA: 1:02 - loss: 2.0245 - regression_loss: 1.5274 - classification_loss: 0.4970\n",
            "18/86 [=====>........................] - ETA: 1:00 - loss: 2.0324 - regression_loss: 1.5335 - classification_loss: 0.4989\n",
            "19/86 [=====>........................] - ETA: 59s - loss: 2.0322 - regression_loss: 1.5338 - classification_loss: 0.4984 \n",
            "20/86 [=====>........................] - ETA: 59s - loss: 2.0297 - regression_loss: 1.5294 - classification_loss: 0.5003\n",
            "21/86 [======>.......................] - ETA: 59s - loss: 2.0297 - regression_loss: 1.5295 - classification_loss: 0.5002\n",
            "22/86 [======>.......................] - ETA: 58s - loss: 2.0261 - regression_loss: 1.5273 - classification_loss: 0.4988\n",
            "23/86 [=======>......................] - ETA: 57s - loss: 2.0319 - regression_loss: 1.5313 - classification_loss: 0.5006\n",
            "24/86 [=======>......................] - ETA: 57s - loss: 2.0316 - regression_loss: 1.5318 - classification_loss: 0.4998\n",
            "25/86 [=======>......................] - ETA: 56s - loss: 2.0401 - regression_loss: 1.5404 - classification_loss: 0.4998\n",
            "26/86 [========>.....................] - ETA: 56s - loss: 2.0426 - regression_loss: 1.5436 - classification_loss: 0.4990\n",
            "27/86 [========>.....................] - ETA: 55s - loss: 2.0400 - regression_loss: 1.5400 - classification_loss: 0.5000\n",
            "28/86 [========>.....................] - ETA: 54s - loss: 2.0474 - regression_loss: 1.5475 - classification_loss: 0.4999\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.0442 - regression_loss: 1.5452 - classification_loss: 0.4990\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.0549 - regression_loss: 1.5551 - classification_loss: 0.4998\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.0663 - regression_loss: 1.5642 - classification_loss: 0.5021\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.0695 - regression_loss: 1.5671 - classification_loss: 0.5024\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.0614 - regression_loss: 1.5604 - classification_loss: 0.5009\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.0463 - regression_loss: 1.5489 - classification_loss: 0.4974\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.0496 - regression_loss: 1.5481 - classification_loss: 0.5015\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.0479 - regression_loss: 1.5486 - classification_loss: 0.4992\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.0562 - regression_loss: 1.5543 - classification_loss: 0.5019\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.0573 - regression_loss: 1.5558 - classification_loss: 0.5015\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.0645 - regression_loss: 1.5620 - classification_loss: 0.5025\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.0686 - regression_loss: 1.5653 - classification_loss: 0.5033\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.0653 - regression_loss: 1.5618 - classification_loss: 0.5036\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.0666 - regression_loss: 1.5639 - classification_loss: 0.5026\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.0692 - regression_loss: 1.5661 - classification_loss: 0.5032\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.0689 - regression_loss: 1.5674 - classification_loss: 0.5016\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.0661 - regression_loss: 1.5655 - classification_loss: 0.5007\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.0719 - regression_loss: 1.5703 - classification_loss: 0.5015\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.0711 - regression_loss: 1.5698 - classification_loss: 0.5014\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.0789 - regression_loss: 1.5760 - classification_loss: 0.5030\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.0764 - regression_loss: 1.5741 - classification_loss: 0.5023\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.0755 - regression_loss: 1.5740 - classification_loss: 0.5014\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.0768 - regression_loss: 1.5753 - classification_loss: 0.5016\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.0700 - regression_loss: 1.5703 - classification_loss: 0.4997\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0703 - regression_loss: 1.5703 - classification_loss: 0.5000\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0693 - regression_loss: 1.5703 - classification_loss: 0.4990\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0687 - regression_loss: 1.5707 - classification_loss: 0.4981\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0736 - regression_loss: 1.5744 - classification_loss: 0.4993\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0726 - regression_loss: 1.5750 - classification_loss: 0.4976\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0679 - regression_loss: 1.5710 - classification_loss: 0.4969\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0713 - regression_loss: 1.5738 - classification_loss: 0.4975\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0627 - regression_loss: 1.5673 - classification_loss: 0.4954\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0693 - regression_loss: 1.5722 - classification_loss: 0.4971\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.0719 - regression_loss: 1.5744 - classification_loss: 0.4974\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.0709 - regression_loss: 1.5735 - classification_loss: 0.4974\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.0672 - regression_loss: 1.5709 - classification_loss: 0.4963\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.0662 - regression_loss: 1.5716 - classification_loss: 0.4947\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.0721 - regression_loss: 1.5762 - classification_loss: 0.4959\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.0714 - regression_loss: 1.5749 - classification_loss: 0.4965\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0739 - regression_loss: 1.5755 - classification_loss: 0.4984\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0764 - regression_loss: 1.5766 - classification_loss: 0.4998\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0762 - regression_loss: 1.5769 - classification_loss: 0.4994\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0760 - regression_loss: 1.5764 - classification_loss: 0.4996\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0772 - regression_loss: 1.5771 - classification_loss: 0.5000\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0804 - regression_loss: 1.5797 - classification_loss: 0.5006\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0768 - regression_loss: 1.5773 - classification_loss: 0.4995\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0790 - regression_loss: 1.5793 - classification_loss: 0.4997\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0758 - regression_loss: 1.5766 - classification_loss: 0.4992 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0770 - regression_loss: 1.5774 - classification_loss: 0.4996\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0794 - regression_loss: 1.5791 - classification_loss: 0.5002\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0826 - regression_loss: 1.5817 - classification_loss: 0.5009\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0835 - regression_loss: 1.5817 - classification_loss: 0.5018\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0852 - regression_loss: 1.5826 - classification_loss: 0.5026\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0890 - regression_loss: 1.5856 - classification_loss: 0.5035\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0902 - regression_loss: 1.5863 - classification_loss: 0.5039\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0886 - regression_loss: 1.5849 - classification_loss: 0.5037\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0869 - regression_loss: 1.5840 - classification_loss: 0.5029\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0875 - regression_loss: 1.5847 - classification_loss: 0.5028\n",
            "Epoch 18: saving model to ./snapshots\\resnet50_csv_18.h5\n",
            "\n",
            "86/86 [==============================] - 84s 964ms/step - loss: 2.0875 - regression_loss: 1.5847 - classification_loss: 0.5028 - lr: 1.0000e-06\n",
            "Epoch 19/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:34 - loss: 1.9903 - regression_loss: 1.5870 - classification_loss: 0.4033\n",
            " 2/86 [..............................] - ETA: 57s - loss: 2.0762 - regression_loss: 1.6001 - classification_loss: 0.4761 \n",
            " 3/86 [>.............................] - ETA: 1:24 - loss: 1.9643 - regression_loss: 1.5142 - classification_loss: 0.4501\n",
            " 4/86 [>.............................] - ETA: 1:20 - loss: 2.0351 - regression_loss: 1.5626 - classification_loss: 0.4725\n",
            " 5/86 [>.............................] - ETA: 1:18 - loss: 2.0001 - regression_loss: 1.5199 - classification_loss: 0.4802\n",
            " 6/86 [=>............................] - ETA: 1:15 - loss: 2.0149 - regression_loss: 1.5326 - classification_loss: 0.4824\n",
            " 7/86 [=>............................] - ETA: 1:14 - loss: 2.0187 - regression_loss: 1.5316 - classification_loss: 0.4871\n",
            " 8/86 [=>............................] - ETA: 1:16 - loss: 2.0378 - regression_loss: 1.5443 - classification_loss: 0.4934\n",
            " 9/86 [==>...........................] - ETA: 1:17 - loss: 2.0563 - regression_loss: 1.5542 - classification_loss: 0.5022\n",
            "10/86 [==>...........................] - ETA: 1:16 - loss: 2.0109 - regression_loss: 1.5216 - classification_loss: 0.4893\n",
            "11/86 [==>...........................] - ETA: 1:14 - loss: 1.9964 - regression_loss: 1.5131 - classification_loss: 0.4833\n",
            "12/86 [===>..........................] - ETA: 1:13 - loss: 2.0168 - regression_loss: 1.5329 - classification_loss: 0.4839\n",
            "13/86 [===>..........................] - ETA: 1:11 - loss: 2.0236 - regression_loss: 1.5414 - classification_loss: 0.4822\n",
            "14/86 [===>..........................] - ETA: 1:11 - loss: 2.0244 - regression_loss: 1.5377 - classification_loss: 0.4867\n",
            "15/86 [====>.........................] - ETA: 1:10 - loss: 2.0346 - regression_loss: 1.5456 - classification_loss: 0.4889\n",
            "16/86 [====>.........................] - ETA: 1:09 - loss: 2.0313 - regression_loss: 1.5438 - classification_loss: 0.4875\n",
            "17/86 [====>.........................] - ETA: 1:08 - loss: 2.0263 - regression_loss: 1.5407 - classification_loss: 0.4856\n",
            "18/86 [=====>........................] - ETA: 1:07 - loss: 2.0340 - regression_loss: 1.5469 - classification_loss: 0.4871\n",
            "19/86 [=====>........................] - ETA: 1:06 - loss: 2.0486 - regression_loss: 1.5586 - classification_loss: 0.4900\n",
            "20/86 [=====>........................] - ETA: 1:04 - loss: 2.0454 - regression_loss: 1.5527 - classification_loss: 0.4928\n",
            "21/86 [======>.......................] - ETA: 1:04 - loss: 2.0450 - regression_loss: 1.5525 - classification_loss: 0.4925\n",
            "22/86 [======>.......................] - ETA: 1:03 - loss: 2.0540 - regression_loss: 1.5559 - classification_loss: 0.4981\n",
            "23/86 [=======>......................] - ETA: 1:01 - loss: 2.0686 - regression_loss: 1.5636 - classification_loss: 0.5050\n",
            "24/86 [=======>......................] - ETA: 1:00 - loss: 2.0882 - regression_loss: 1.5783 - classification_loss: 0.5100\n",
            "25/86 [=======>......................] - ETA: 59s - loss: 2.0888 - regression_loss: 1.5797 - classification_loss: 0.5091 \n",
            "26/86 [========>.....................] - ETA: 58s - loss: 2.0907 - regression_loss: 1.5816 - classification_loss: 0.5091\n",
            "27/86 [========>.....................] - ETA: 57s - loss: 2.1017 - regression_loss: 1.5894 - classification_loss: 0.5123\n",
            "28/86 [========>.....................] - ETA: 56s - loss: 2.1052 - regression_loss: 1.5907 - classification_loss: 0.5145\n",
            "29/86 [=========>....................] - ETA: 55s - loss: 2.1095 - regression_loss: 1.5938 - classification_loss: 0.5157\n",
            "30/86 [=========>....................] - ETA: 54s - loss: 2.1214 - regression_loss: 1.6029 - classification_loss: 0.5186\n",
            "31/86 [=========>....................] - ETA: 53s - loss: 2.1132 - regression_loss: 1.5960 - classification_loss: 0.5172\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.1123 - regression_loss: 1.5952 - classification_loss: 0.5171\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.1131 - regression_loss: 1.5955 - classification_loss: 0.5176\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.1223 - regression_loss: 1.6021 - classification_loss: 0.5201\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.1325 - regression_loss: 1.6102 - classification_loss: 0.5223\n",
            "36/86 [===========>..................] - ETA: 48s - loss: 2.1229 - regression_loss: 1.6028 - classification_loss: 0.5201\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.1239 - regression_loss: 1.6030 - classification_loss: 0.5209\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.1233 - regression_loss: 1.6025 - classification_loss: 0.5209\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.1283 - regression_loss: 1.6065 - classification_loss: 0.5217\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.1422 - regression_loss: 1.6181 - classification_loss: 0.5241\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.1444 - regression_loss: 1.6212 - classification_loss: 0.5232\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1551 - regression_loss: 1.6298 - classification_loss: 0.5252\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1548 - regression_loss: 1.6301 - classification_loss: 0.5247\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1485 - regression_loss: 1.6259 - classification_loss: 0.5226\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.1443 - regression_loss: 1.6225 - classification_loss: 0.5218\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1382 - regression_loss: 1.6176 - classification_loss: 0.5206\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1343 - regression_loss: 1.6155 - classification_loss: 0.5188\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1358 - regression_loss: 1.6172 - classification_loss: 0.5186\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1390 - regression_loss: 1.6195 - classification_loss: 0.5195\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1399 - regression_loss: 1.6194 - classification_loss: 0.5204\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1350 - regression_loss: 1.6161 - classification_loss: 0.5189\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1400 - regression_loss: 1.6194 - classification_loss: 0.5206\n",
            "53/86 [=================>............] - ETA: 32s - loss: 2.1417 - regression_loss: 1.6212 - classification_loss: 0.5205\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1423 - regression_loss: 1.6218 - classification_loss: 0.5205\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1420 - regression_loss: 1.6213 - classification_loss: 0.5207\n",
            "56/86 [==================>...........] - ETA: 29s - loss: 2.1386 - regression_loss: 1.6192 - classification_loss: 0.5194\n",
            "57/86 [==================>...........] - ETA: 28s - loss: 2.1358 - regression_loss: 1.6172 - classification_loss: 0.5186\n",
            "58/86 [===================>..........] - ETA: 27s - loss: 2.1363 - regression_loss: 1.6170 - classification_loss: 0.5193\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.1329 - regression_loss: 1.6142 - classification_loss: 0.5187\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.1363 - regression_loss: 1.6150 - classification_loss: 0.5212\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1362 - regression_loss: 1.6136 - classification_loss: 0.5226\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1387 - regression_loss: 1.6158 - classification_loss: 0.5229\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.1388 - regression_loss: 1.6156 - classification_loss: 0.5233\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.1387 - regression_loss: 1.6158 - classification_loss: 0.5229\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1341 - regression_loss: 1.6117 - classification_loss: 0.5224\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1327 - regression_loss: 1.6117 - classification_loss: 0.5210\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1238 - regression_loss: 1.6048 - classification_loss: 0.5190\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1153 - regression_loss: 1.5987 - classification_loss: 0.5166\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1131 - regression_loss: 1.5968 - classification_loss: 0.5163\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1151 - regression_loss: 1.5974 - classification_loss: 0.5176\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1164 - regression_loss: 1.5987 - classification_loss: 0.5177\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1196 - regression_loss: 1.6016 - classification_loss: 0.5179\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1180 - regression_loss: 1.6005 - classification_loss: 0.5176\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1171 - regression_loss: 1.6003 - classification_loss: 0.5169\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1128 - regression_loss: 1.5973 - classification_loss: 0.5155\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1170 - regression_loss: 1.5997 - classification_loss: 0.5173 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1132 - regression_loss: 1.5967 - classification_loss: 0.5165\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1168 - regression_loss: 1.5992 - classification_loss: 0.5176\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1191 - regression_loss: 1.6007 - classification_loss: 0.5184\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1189 - regression_loss: 1.6010 - classification_loss: 0.5179\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1208 - regression_loss: 1.6025 - classification_loss: 0.5183\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1196 - regression_loss: 1.6009 - classification_loss: 0.5187\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1224 - regression_loss: 1.6031 - classification_loss: 0.5192\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1213 - regression_loss: 1.6022 - classification_loss: 0.5191\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1197 - regression_loss: 1.6016 - classification_loss: 0.5181\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1221 - regression_loss: 1.6035 - classification_loss: 0.5186\n",
            "Epoch 19: saving model to ./snapshots\\resnet50_csv_19.h5\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
            "\n",
            "86/86 [==============================] - 84s 964ms/step - loss: 2.1221 - regression_loss: 1.6035 - classification_loss: 0.5186 - lr: 1.0000e-06\n",
            "Epoch 20/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:47 - loss: 1.5404 - regression_loss: 1.1761 - classification_loss: 0.3643\n",
            " 2/86 [..............................] - ETA: 1:17 - loss: 1.7985 - regression_loss: 1.3548 - classification_loss: 0.4437\n",
            " 3/86 [>.............................] - ETA: 1:30 - loss: 1.8484 - regression_loss: 1.4191 - classification_loss: 0.4292\n",
            " 4/86 [>.............................] - ETA: 1:19 - loss: 1.8220 - regression_loss: 1.3936 - classification_loss: 0.4284\n",
            " 5/86 [>.............................] - ETA: 1:16 - loss: 1.8606 - regression_loss: 1.4221 - classification_loss: 0.4385\n",
            " 6/86 [=>............................] - ETA: 1:14 - loss: 1.9400 - regression_loss: 1.4858 - classification_loss: 0.4542\n",
            " 7/86 [=>............................] - ETA: 1:14 - loss: 1.9438 - regression_loss: 1.4955 - classification_loss: 0.4483\n",
            " 8/86 [=>............................] - ETA: 1:14 - loss: 1.9377 - regression_loss: 1.4852 - classification_loss: 0.4525\n",
            " 9/86 [==>...........................] - ETA: 1:12 - loss: 1.9319 - regression_loss: 1.4791 - classification_loss: 0.4528\n",
            "10/86 [==>...........................] - ETA: 1:11 - loss: 1.9154 - regression_loss: 1.4681 - classification_loss: 0.4473\n",
            "11/86 [==>...........................] - ETA: 1:10 - loss: 1.9332 - regression_loss: 1.4829 - classification_loss: 0.4503\n",
            "12/86 [===>..........................] - ETA: 1:11 - loss: 1.9667 - regression_loss: 1.5071 - classification_loss: 0.4595\n",
            "13/86 [===>..........................] - ETA: 1:11 - loss: 1.9860 - regression_loss: 1.5226 - classification_loss: 0.4634\n",
            "14/86 [===>..........................] - ETA: 1:11 - loss: 2.0201 - regression_loss: 1.5490 - classification_loss: 0.4710\n",
            "15/86 [====>.........................] - ETA: 1:10 - loss: 2.0166 - regression_loss: 1.5463 - classification_loss: 0.4703\n",
            "16/86 [====>.........................] - ETA: 1:08 - loss: 2.0145 - regression_loss: 1.5441 - classification_loss: 0.4703\n",
            "17/86 [====>.........................] - ETA: 1:07 - loss: 2.0322 - regression_loss: 1.5556 - classification_loss: 0.4766\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.0319 - regression_loss: 1.5565 - classification_loss: 0.4754\n",
            "19/86 [=====>........................] - ETA: 1:04 - loss: 2.0033 - regression_loss: 1.5349 - classification_loss: 0.4684\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.0163 - regression_loss: 1.5450 - classification_loss: 0.4713\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.0180 - regression_loss: 1.5434 - classification_loss: 0.4746\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.0206 - regression_loss: 1.5451 - classification_loss: 0.4755\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.0267 - regression_loss: 1.5485 - classification_loss: 0.4782\n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.0320 - regression_loss: 1.5540 - classification_loss: 0.4780 \n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.0317 - regression_loss: 1.5507 - classification_loss: 0.4810\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.0534 - regression_loss: 1.5670 - classification_loss: 0.4864\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.0574 - regression_loss: 1.5707 - classification_loss: 0.4867\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.0608 - regression_loss: 1.5721 - classification_loss: 0.4886\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.0587 - regression_loss: 1.5702 - classification_loss: 0.4885\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.0615 - regression_loss: 1.5705 - classification_loss: 0.4910\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.0616 - regression_loss: 1.5687 - classification_loss: 0.4929\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.0641 - regression_loss: 1.5705 - classification_loss: 0.4936\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.0584 - regression_loss: 1.5653 - classification_loss: 0.4931\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.0661 - regression_loss: 1.5718 - classification_loss: 0.4943\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.0647 - regression_loss: 1.5706 - classification_loss: 0.4941\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.0663 - regression_loss: 1.5724 - classification_loss: 0.4939\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.0694 - regression_loss: 1.5743 - classification_loss: 0.4951\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.0757 - regression_loss: 1.5786 - classification_loss: 0.4970\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.0779 - regression_loss: 1.5804 - classification_loss: 0.4976\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.0732 - regression_loss: 1.5771 - classification_loss: 0.4961\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.0767 - regression_loss: 1.5803 - classification_loss: 0.4964\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.0808 - regression_loss: 1.5828 - classification_loss: 0.4980\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.0886 - regression_loss: 1.5899 - classification_loss: 0.4987\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.0812 - regression_loss: 1.5842 - classification_loss: 0.4970\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.0729 - regression_loss: 1.5782 - classification_loss: 0.4947\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.0675 - regression_loss: 1.5743 - classification_loss: 0.4931\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.0782 - regression_loss: 1.5813 - classification_loss: 0.4968\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.0850 - regression_loss: 1.5860 - classification_loss: 0.4991\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.0871 - regression_loss: 1.5876 - classification_loss: 0.4995\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.0884 - regression_loss: 1.5884 - classification_loss: 0.5000\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.0882 - regression_loss: 1.5881 - classification_loss: 0.5001\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.0876 - regression_loss: 1.5875 - classification_loss: 0.5001\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0890 - regression_loss: 1.5882 - classification_loss: 0.5008\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0878 - regression_loss: 1.5870 - classification_loss: 0.5008\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0919 - regression_loss: 1.5906 - classification_loss: 0.5013\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0974 - regression_loss: 1.5944 - classification_loss: 0.5030\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0943 - regression_loss: 1.5920 - classification_loss: 0.5023\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0919 - regression_loss: 1.5903 - classification_loss: 0.5016\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0933 - regression_loss: 1.5910 - classification_loss: 0.5023\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0953 - regression_loss: 1.5931 - classification_loss: 0.5022\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0939 - regression_loss: 1.5919 - classification_loss: 0.5020\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.0926 - regression_loss: 1.5908 - classification_loss: 0.5018\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.0904 - regression_loss: 1.5890 - classification_loss: 0.5014\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.0897 - regression_loss: 1.5881 - classification_loss: 0.5017\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.0875 - regression_loss: 1.5865 - classification_loss: 0.5009\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0855 - regression_loss: 1.5855 - classification_loss: 0.5000\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0866 - regression_loss: 1.5863 - classification_loss: 0.5003\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0825 - regression_loss: 1.5831 - classification_loss: 0.4994\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0787 - regression_loss: 1.5803 - classification_loss: 0.4984\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0776 - regression_loss: 1.5792 - classification_loss: 0.4984\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0795 - regression_loss: 1.5805 - classification_loss: 0.4990\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0801 - regression_loss: 1.5803 - classification_loss: 0.4998\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0808 - regression_loss: 1.5812 - classification_loss: 0.4996\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0818 - regression_loss: 1.5825 - classification_loss: 0.4993\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0795 - regression_loss: 1.5806 - classification_loss: 0.4989\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0829 - regression_loss: 1.5831 - classification_loss: 0.4997 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0859 - regression_loss: 1.5857 - classification_loss: 0.5003\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0885 - regression_loss: 1.5882 - classification_loss: 0.5003\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0893 - regression_loss: 1.5886 - classification_loss: 0.5007\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0907 - regression_loss: 1.5896 - classification_loss: 0.5011\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0935 - regression_loss: 1.5918 - classification_loss: 0.5016\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0937 - regression_loss: 1.5920 - classification_loss: 0.5017\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0914 - regression_loss: 1.5901 - classification_loss: 0.5013\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0899 - regression_loss: 1.5890 - classification_loss: 0.5009\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0902 - regression_loss: 1.5890 - classification_loss: 0.5012\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0919 - regression_loss: 1.5902 - classification_loss: 0.5016\n",
            "Epoch 20: saving model to ./snapshots\\resnet50_csv_20.h5\n",
            "\n",
            "86/86 [==============================] - 82s 956ms/step - loss: 2.0919 - regression_loss: 1.5902 - classification_loss: 0.5016 - lr: 1.0000e-07\n",
            "Epoch 21/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:11 - loss: 1.7661 - regression_loss: 1.3590 - classification_loss: 0.4071\n",
            " 2/86 [..............................] - ETA: 1:56 - loss: 1.8617 - regression_loss: 1.4091 - classification_loss: 0.4525\n",
            " 3/86 [>.............................] - ETA: 1:30 - loss: 1.9340 - regression_loss: 1.4701 - classification_loss: 0.4640\n",
            " 4/86 [>.............................] - ETA: 1:29 - loss: 1.9431 - regression_loss: 1.4583 - classification_loss: 0.4848\n",
            " 5/86 [>.............................] - ETA: 1:27 - loss: 1.9866 - regression_loss: 1.4965 - classification_loss: 0.4901\n",
            " 6/86 [=>............................] - ETA: 1:22 - loss: 2.0118 - regression_loss: 1.5145 - classification_loss: 0.4973\n",
            " 7/86 [=>............................] - ETA: 1:19 - loss: 2.0607 - regression_loss: 1.5570 - classification_loss: 0.5037\n",
            " 8/86 [=>............................] - ETA: 1:19 - loss: 2.0854 - regression_loss: 1.5835 - classification_loss: 0.5019\n",
            " 9/86 [==>...........................] - ETA: 1:14 - loss: 2.0846 - regression_loss: 1.5792 - classification_loss: 0.5054\n",
            "10/86 [==>...........................] - ETA: 1:13 - loss: 2.0534 - regression_loss: 1.5562 - classification_loss: 0.4972\n",
            "11/86 [==>...........................] - ETA: 1:11 - loss: 2.0504 - regression_loss: 1.5534 - classification_loss: 0.4971\n",
            "12/86 [===>..........................] - ETA: 1:11 - loss: 2.0808 - regression_loss: 1.5662 - classification_loss: 0.5146\n",
            "13/86 [===>..........................] - ETA: 1:09 - loss: 2.0886 - regression_loss: 1.5762 - classification_loss: 0.5123\n",
            "14/86 [===>..........................] - ETA: 1:08 - loss: 2.0927 - regression_loss: 1.5821 - classification_loss: 0.5105\n",
            "15/86 [====>.........................] - ETA: 1:07 - loss: 2.0976 - regression_loss: 1.5902 - classification_loss: 0.5074\n",
            "16/86 [====>.........................] - ETA: 1:06 - loss: 2.0920 - regression_loss: 1.5881 - classification_loss: 0.5039\n",
            "17/86 [====>.........................] - ETA: 1:05 - loss: 2.0908 - regression_loss: 1.5873 - classification_loss: 0.5035\n",
            "18/86 [=====>........................] - ETA: 1:03 - loss: 2.0898 - regression_loss: 1.5892 - classification_loss: 0.5007\n",
            "19/86 [=====>........................] - ETA: 1:04 - loss: 2.0756 - regression_loss: 1.5787 - classification_loss: 0.4969\n",
            "20/86 [=====>........................] - ETA: 1:02 - loss: 2.0733 - regression_loss: 1.5765 - classification_loss: 0.4968\n",
            "21/86 [======>.......................] - ETA: 1:01 - loss: 2.0759 - regression_loss: 1.5791 - classification_loss: 0.4968\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.0819 - regression_loss: 1.5842 - classification_loss: 0.4976\n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.0868 - regression_loss: 1.5890 - classification_loss: 0.4978 \n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.0918 - regression_loss: 1.5895 - classification_loss: 0.5023\n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.0875 - regression_loss: 1.5846 - classification_loss: 0.5028\n",
            "26/86 [========>.....................] - ETA: 56s - loss: 2.0839 - regression_loss: 1.5846 - classification_loss: 0.4993\n",
            "27/86 [========>.....................] - ETA: 55s - loss: 2.0815 - regression_loss: 1.5833 - classification_loss: 0.4982\n",
            "28/86 [========>.....................] - ETA: 54s - loss: 2.0853 - regression_loss: 1.5882 - classification_loss: 0.4971\n",
            "29/86 [=========>....................] - ETA: 53s - loss: 2.0833 - regression_loss: 1.5872 - classification_loss: 0.4961\n",
            "30/86 [=========>....................] - ETA: 52s - loss: 2.0726 - regression_loss: 1.5789 - classification_loss: 0.4937\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.0791 - regression_loss: 1.5824 - classification_loss: 0.4967\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.0804 - regression_loss: 1.5829 - classification_loss: 0.4975\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.0884 - regression_loss: 1.5887 - classification_loss: 0.4997\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.0908 - regression_loss: 1.5895 - classification_loss: 0.5013\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.0885 - regression_loss: 1.5873 - classification_loss: 0.5013\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.0950 - regression_loss: 1.5891 - classification_loss: 0.5059\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.0972 - regression_loss: 1.5900 - classification_loss: 0.5072\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.0949 - regression_loss: 1.5893 - classification_loss: 0.5056\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.0892 - regression_loss: 1.5830 - classification_loss: 0.5062\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.0981 - regression_loss: 1.5897 - classification_loss: 0.5084\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.0979 - regression_loss: 1.5896 - classification_loss: 0.5083\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.1043 - regression_loss: 1.5948 - classification_loss: 0.5095\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.0994 - regression_loss: 1.5912 - classification_loss: 0.5082\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.0956 - regression_loss: 1.5888 - classification_loss: 0.5067\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.0971 - regression_loss: 1.5898 - classification_loss: 0.5072\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.0918 - regression_loss: 1.5852 - classification_loss: 0.5066\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1008 - regression_loss: 1.5918 - classification_loss: 0.5090\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.0975 - regression_loss: 1.5902 - classification_loss: 0.5073\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1015 - regression_loss: 1.5933 - classification_loss: 0.5082\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.0982 - regression_loss: 1.5910 - classification_loss: 0.5072\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.0871 - regression_loss: 1.5828 - classification_loss: 0.5043\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.0835 - regression_loss: 1.5805 - classification_loss: 0.5029\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0820 - regression_loss: 1.5798 - classification_loss: 0.5022\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0803 - regression_loss: 1.5785 - classification_loss: 0.5018\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0804 - regression_loss: 1.5781 - classification_loss: 0.5023\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0800 - regression_loss: 1.5779 - classification_loss: 0.5021\n",
            "57/86 [==================>...........] - ETA: 28s - loss: 2.0797 - regression_loss: 1.5770 - classification_loss: 0.5027\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0786 - regression_loss: 1.5760 - classification_loss: 0.5026\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0746 - regression_loss: 1.5728 - classification_loss: 0.5019\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.0794 - regression_loss: 1.5774 - classification_loss: 0.5021\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.0876 - regression_loss: 1.5823 - classification_loss: 0.5053\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.0894 - regression_loss: 1.5829 - classification_loss: 0.5065\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.0894 - regression_loss: 1.5823 - classification_loss: 0.5071\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.0918 - regression_loss: 1.5842 - classification_loss: 0.5075\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.0897 - regression_loss: 1.5827 - classification_loss: 0.5070\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0911 - regression_loss: 1.5844 - classification_loss: 0.5067\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0879 - regression_loss: 1.5823 - classification_loss: 0.5056\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0877 - regression_loss: 1.5817 - classification_loss: 0.5060\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0862 - regression_loss: 1.5812 - classification_loss: 0.5050\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0810 - regression_loss: 1.5765 - classification_loss: 0.5045\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0772 - regression_loss: 1.5735 - classification_loss: 0.5037\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0775 - regression_loss: 1.5729 - classification_loss: 0.5046\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0768 - regression_loss: 1.5718 - classification_loss: 0.5051\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0767 - regression_loss: 1.5717 - classification_loss: 0.5050\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0752 - regression_loss: 1.5698 - classification_loss: 0.5054\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0760 - regression_loss: 1.5702 - classification_loss: 0.5058 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0785 - regression_loss: 1.5720 - classification_loss: 0.5066\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0812 - regression_loss: 1.5739 - classification_loss: 0.5073\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0822 - regression_loss: 1.5748 - classification_loss: 0.5073\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0840 - regression_loss: 1.5766 - classification_loss: 0.5074\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0804 - regression_loss: 1.5744 - classification_loss: 0.5060\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0804 - regression_loss: 1.5747 - classification_loss: 0.5057\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0860 - regression_loss: 1.5793 - classification_loss: 0.5067\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0855 - regression_loss: 1.5774 - classification_loss: 0.5081\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0871 - regression_loss: 1.5792 - classification_loss: 0.5078\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0853 - regression_loss: 1.5782 - classification_loss: 0.5071\n",
            "Epoch 21: saving model to ./snapshots\\resnet50_csv_21.h5\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
            "\n",
            "86/86 [==============================] - 85s 980ms/step - loss: 2.0853 - regression_loss: 1.5782 - classification_loss: 0.5071 - lr: 1.0000e-07\n",
            "Epoch 22/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:33 - loss: 2.2523 - regression_loss: 1.7144 - classification_loss: 0.5379\n",
            " 2/86 [..............................] - ETA: 1:44 - loss: 2.1942 - regression_loss: 1.6760 - classification_loss: 0.5183\n",
            " 3/86 [>.............................] - ETA: 1:17 - loss: 2.0259 - regression_loss: 1.5501 - classification_loss: 0.4758\n",
            " 4/86 [>.............................] - ETA: 1:22 - loss: 2.0539 - regression_loss: 1.5812 - classification_loss: 0.4727\n",
            " 5/86 [>.............................] - ETA: 1:14 - loss: 2.0264 - regression_loss: 1.5576 - classification_loss: 0.4688\n",
            " 6/86 [=>............................] - ETA: 1:15 - loss: 2.0684 - regression_loss: 1.5798 - classification_loss: 0.4886\n",
            " 7/86 [=>............................] - ETA: 1:17 - loss: 2.1099 - regression_loss: 1.6138 - classification_loss: 0.4960\n",
            " 8/86 [=>............................] - ETA: 1:16 - loss: 2.1053 - regression_loss: 1.6178 - classification_loss: 0.4875\n",
            " 9/86 [==>...........................] - ETA: 1:14 - loss: 2.1169 - regression_loss: 1.6252 - classification_loss: 0.4917\n",
            "10/86 [==>...........................] - ETA: 1:13 - loss: 2.1100 - regression_loss: 1.6163 - classification_loss: 0.4937\n",
            "11/86 [==>...........................] - ETA: 1:11 - loss: 2.1021 - regression_loss: 1.6076 - classification_loss: 0.4945\n",
            "12/86 [===>..........................] - ETA: 1:10 - loss: 2.0863 - regression_loss: 1.5907 - classification_loss: 0.4956\n",
            "13/86 [===>..........................] - ETA: 1:09 - loss: 2.0877 - regression_loss: 1.5904 - classification_loss: 0.4973\n",
            "14/86 [===>..........................] - ETA: 1:08 - loss: 2.0823 - regression_loss: 1.5928 - classification_loss: 0.4895\n",
            "15/86 [====>.........................] - ETA: 1:07 - loss: 2.1169 - regression_loss: 1.6192 - classification_loss: 0.4977\n",
            "16/86 [====>.........................] - ETA: 1:07 - loss: 2.1243 - regression_loss: 1.6246 - classification_loss: 0.4997\n",
            "17/86 [====>.........................] - ETA: 1:05 - loss: 2.1090 - regression_loss: 1.6091 - classification_loss: 0.5000\n",
            "18/86 [=====>........................] - ETA: 1:04 - loss: 2.1193 - regression_loss: 1.6169 - classification_loss: 0.5023\n",
            "19/86 [=====>........................] - ETA: 1:03 - loss: 2.1312 - regression_loss: 1.6246 - classification_loss: 0.5065\n",
            "20/86 [=====>........................] - ETA: 1:02 - loss: 2.1312 - regression_loss: 1.6237 - classification_loss: 0.5075\n",
            "21/86 [======>.......................] - ETA: 1:01 - loss: 2.1243 - regression_loss: 1.6188 - classification_loss: 0.5055\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.1351 - regression_loss: 1.6264 - classification_loss: 0.5088\n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.1415 - regression_loss: 1.6287 - classification_loss: 0.5128 \n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.1413 - regression_loss: 1.6281 - classification_loss: 0.5132\n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.1496 - regression_loss: 1.6382 - classification_loss: 0.5114\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.1458 - regression_loss: 1.6355 - classification_loss: 0.5103\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.1351 - regression_loss: 1.6270 - classification_loss: 0.5081\n",
            "28/86 [========>.....................] - ETA: 56s - loss: 2.1297 - regression_loss: 1.6228 - classification_loss: 0.5070\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.1287 - regression_loss: 1.6210 - classification_loss: 0.5077\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.1321 - regression_loss: 1.6234 - classification_loss: 0.5087\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.1289 - regression_loss: 1.6209 - classification_loss: 0.5080\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.1250 - regression_loss: 1.6159 - classification_loss: 0.5090\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.1340 - regression_loss: 1.6221 - classification_loss: 0.5119\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.1410 - regression_loss: 1.6243 - classification_loss: 0.5166\n",
            "35/86 [===========>..................] - ETA: 47s - loss: 2.1334 - regression_loss: 1.6161 - classification_loss: 0.5172\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.1326 - regression_loss: 1.6152 - classification_loss: 0.5174\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.1429 - regression_loss: 1.6236 - classification_loss: 0.5192\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.1430 - regression_loss: 1.6247 - classification_loss: 0.5183\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.1432 - regression_loss: 1.6247 - classification_loss: 0.5184\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.1429 - regression_loss: 1.6254 - classification_loss: 0.5175\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.1427 - regression_loss: 1.6268 - classification_loss: 0.5159\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1401 - regression_loss: 1.6247 - classification_loss: 0.5154\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1398 - regression_loss: 1.6251 - classification_loss: 0.5147\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1349 - regression_loss: 1.6213 - classification_loss: 0.5136\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.1348 - regression_loss: 1.6214 - classification_loss: 0.5134\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1381 - regression_loss: 1.6243 - classification_loss: 0.5138\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1347 - regression_loss: 1.6209 - classification_loss: 0.5139\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1366 - regression_loss: 1.6209 - classification_loss: 0.5157\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1341 - regression_loss: 1.6199 - classification_loss: 0.5143\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1353 - regression_loss: 1.6200 - classification_loss: 0.5153\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1316 - regression_loss: 1.6169 - classification_loss: 0.5147\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1323 - regression_loss: 1.6179 - classification_loss: 0.5144\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1347 - regression_loss: 1.6194 - classification_loss: 0.5153\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1321 - regression_loss: 1.6162 - classification_loss: 0.5158\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1304 - regression_loss: 1.6153 - classification_loss: 0.5151\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1320 - regression_loss: 1.6164 - classification_loss: 0.5156\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1333 - regression_loss: 1.6170 - classification_loss: 0.5163\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1261 - regression_loss: 1.6120 - classification_loss: 0.5141\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1267 - regression_loss: 1.6129 - classification_loss: 0.5139\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1252 - regression_loss: 1.6127 - classification_loss: 0.5124\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1245 - regression_loss: 1.6118 - classification_loss: 0.5127\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1284 - regression_loss: 1.6152 - classification_loss: 0.5132\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1254 - regression_loss: 1.6136 - classification_loss: 0.5118\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1206 - regression_loss: 1.6098 - classification_loss: 0.5108\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1207 - regression_loss: 1.6093 - classification_loss: 0.5114\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.1210 - regression_loss: 1.6090 - classification_loss: 0.5121\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.1206 - regression_loss: 1.6078 - classification_loss: 0.5129\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1178 - regression_loss: 1.6057 - classification_loss: 0.5121\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1177 - regression_loss: 1.6057 - classification_loss: 0.5120\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1187 - regression_loss: 1.6065 - classification_loss: 0.5122\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1208 - regression_loss: 1.6083 - classification_loss: 0.5125\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1212 - regression_loss: 1.6084 - classification_loss: 0.5127\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1278 - regression_loss: 1.6138 - classification_loss: 0.5141\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1281 - regression_loss: 1.6138 - classification_loss: 0.5142\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1247 - regression_loss: 1.6112 - classification_loss: 0.5135\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1228 - regression_loss: 1.6099 - classification_loss: 0.5129 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1237 - regression_loss: 1.6105 - classification_loss: 0.5132\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1247 - regression_loss: 1.6099 - classification_loss: 0.5148\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1251 - regression_loss: 1.6114 - classification_loss: 0.5137\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1252 - regression_loss: 1.6116 - classification_loss: 0.5136\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1251 - regression_loss: 1.6117 - classification_loss: 0.5135\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1232 - regression_loss: 1.6103 - classification_loss: 0.5129\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1201 - regression_loss: 1.6079 - classification_loss: 0.5122\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1193 - regression_loss: 1.6068 - classification_loss: 0.5125\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1200 - regression_loss: 1.6073 - classification_loss: 0.5127\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1233 - regression_loss: 1.6099 - classification_loss: 0.5135\n",
            "Epoch 22: saving model to ./snapshots\\resnet50_csv_22.h5\n",
            "\n",
            "86/86 [==============================] - 84s 964ms/step - loss: 2.1233 - regression_loss: 1.6099 - classification_loss: 0.5135 - lr: 1.0000e-08\n",
            "Epoch 23/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:25 - loss: 2.1992 - regression_loss: 1.7461 - classification_loss: 0.4531\n",
            " 2/86 [..............................] - ETA: 1:45 - loss: 2.1416 - regression_loss: 1.6824 - classification_loss: 0.4592\n",
            " 3/86 [>.............................] - ETA: 1:22 - loss: 2.0357 - regression_loss: 1.5525 - classification_loss: 0.4832\n",
            " 4/86 [>.............................] - ETA: 1:15 - loss: 2.1089 - regression_loss: 1.6208 - classification_loss: 0.4882\n",
            " 5/86 [>.............................] - ETA: 1:24 - loss: 2.0868 - regression_loss: 1.6160 - classification_loss: 0.4708\n",
            " 6/86 [=>............................] - ETA: 1:17 - loss: 2.1335 - regression_loss: 1.6441 - classification_loss: 0.4894\n",
            " 7/86 [=>............................] - ETA: 1:16 - loss: 2.1105 - regression_loss: 1.6290 - classification_loss: 0.4815\n",
            " 8/86 [=>............................] - ETA: 1:19 - loss: 2.1071 - regression_loss: 1.6189 - classification_loss: 0.4881\n",
            " 9/86 [==>...........................] - ETA: 1:16 - loss: 2.0740 - regression_loss: 1.5925 - classification_loss: 0.4815\n",
            "10/86 [==>...........................] - ETA: 1:13 - loss: 2.0752 - regression_loss: 1.5929 - classification_loss: 0.4823\n",
            "11/86 [==>...........................] - ETA: 1:12 - loss: 2.1018 - regression_loss: 1.6103 - classification_loss: 0.4916\n",
            "12/86 [===>..........................] - ETA: 1:14 - loss: 2.1099 - regression_loss: 1.6151 - classification_loss: 0.4948\n",
            "13/86 [===>..........................] - ETA: 1:12 - loss: 2.1180 - regression_loss: 1.6194 - classification_loss: 0.4986\n",
            "14/86 [===>..........................] - ETA: 1:10 - loss: 2.1458 - regression_loss: 1.6406 - classification_loss: 0.5052\n",
            "15/86 [====>.........................] - ETA: 1:09 - loss: 2.1364 - regression_loss: 1.6304 - classification_loss: 0.5060\n",
            "16/86 [====>.........................] - ETA: 1:10 - loss: 2.1388 - regression_loss: 1.6324 - classification_loss: 0.5064\n",
            "17/86 [====>.........................] - ETA: 1:08 - loss: 2.1479 - regression_loss: 1.6392 - classification_loss: 0.5086\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.1417 - regression_loss: 1.6302 - classification_loss: 0.5115\n",
            "19/86 [=====>........................] - ETA: 1:04 - loss: 2.1315 - regression_loss: 1.6211 - classification_loss: 0.5104\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.1229 - regression_loss: 1.6144 - classification_loss: 0.5086\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.0958 - regression_loss: 1.5937 - classification_loss: 0.5022\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.0829 - regression_loss: 1.5855 - classification_loss: 0.4974\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.0546 - regression_loss: 1.5639 - classification_loss: 0.4908\n",
            "24/86 [=======>......................] - ETA: 57s - loss: 2.0572 - regression_loss: 1.5670 - classification_loss: 0.4902 \n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.0637 - regression_loss: 1.5707 - classification_loss: 0.4930\n",
            "26/86 [========>.....................] - ETA: 56s - loss: 2.0544 - regression_loss: 1.5600 - classification_loss: 0.4944\n",
            "27/86 [========>.....................] - ETA: 55s - loss: 2.0526 - regression_loss: 1.5574 - classification_loss: 0.4952\n",
            "28/86 [========>.....................] - ETA: 54s - loss: 2.0650 - regression_loss: 1.5690 - classification_loss: 0.4960\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.0695 - regression_loss: 1.5712 - classification_loss: 0.4982\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.0722 - regression_loss: 1.5733 - classification_loss: 0.4989\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.0817 - regression_loss: 1.5836 - classification_loss: 0.4981\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.0764 - regression_loss: 1.5790 - classification_loss: 0.4974\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.0761 - regression_loss: 1.5772 - classification_loss: 0.4988\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.0754 - regression_loss: 1.5756 - classification_loss: 0.4999\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.0800 - regression_loss: 1.5799 - classification_loss: 0.5000\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.0777 - regression_loss: 1.5778 - classification_loss: 0.4999\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.0780 - regression_loss: 1.5761 - classification_loss: 0.5018\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.0803 - regression_loss: 1.5776 - classification_loss: 0.5027\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.0781 - regression_loss: 1.5755 - classification_loss: 0.5026\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.0755 - regression_loss: 1.5732 - classification_loss: 0.5023\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.0742 - regression_loss: 1.5710 - classification_loss: 0.5033\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.0695 - regression_loss: 1.5663 - classification_loss: 0.5033\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.0718 - regression_loss: 1.5680 - classification_loss: 0.5038\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.0696 - regression_loss: 1.5668 - classification_loss: 0.5029\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.0692 - regression_loss: 1.5670 - classification_loss: 0.5023\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.0700 - regression_loss: 1.5670 - classification_loss: 0.5029\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.0664 - regression_loss: 1.5645 - classification_loss: 0.5019\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.0664 - regression_loss: 1.5642 - classification_loss: 0.5023\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.0621 - regression_loss: 1.5614 - classification_loss: 0.5007\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.0596 - regression_loss: 1.5599 - classification_loss: 0.4996\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.0574 - regression_loss: 1.5580 - classification_loss: 0.4995\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.0552 - regression_loss: 1.5568 - classification_loss: 0.4985\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0537 - regression_loss: 1.5561 - classification_loss: 0.4975\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0522 - regression_loss: 1.5552 - classification_loss: 0.4970\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0573 - regression_loss: 1.5585 - classification_loss: 0.4988\n",
            "56/86 [==================>...........] - ETA: 29s - loss: 2.0565 - regression_loss: 1.5581 - classification_loss: 0.4984\n",
            "57/86 [==================>...........] - ETA: 28s - loss: 2.0595 - regression_loss: 1.5597 - classification_loss: 0.4998\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0599 - regression_loss: 1.5593 - classification_loss: 0.5006\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.0583 - regression_loss: 1.5588 - classification_loss: 0.4995\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.0646 - regression_loss: 1.5638 - classification_loss: 0.5008\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.0681 - regression_loss: 1.5671 - classification_loss: 0.5010\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.0662 - regression_loss: 1.5650 - classification_loss: 0.5012\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.0674 - regression_loss: 1.5656 - classification_loss: 0.5018\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.0656 - regression_loss: 1.5640 - classification_loss: 0.5016\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.0658 - regression_loss: 1.5639 - classification_loss: 0.5019\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0697 - regression_loss: 1.5677 - classification_loss: 0.5020\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0742 - regression_loss: 1.5699 - classification_loss: 0.5043\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0717 - regression_loss: 1.5657 - classification_loss: 0.5059\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0694 - regression_loss: 1.5638 - classification_loss: 0.5056\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0659 - regression_loss: 1.5613 - classification_loss: 0.5046\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0663 - regression_loss: 1.5621 - classification_loss: 0.5042\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0689 - regression_loss: 1.5637 - classification_loss: 0.5052\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0657 - regression_loss: 1.5616 - classification_loss: 0.5042\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0718 - regression_loss: 1.5662 - classification_loss: 0.5056\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0739 - regression_loss: 1.5684 - classification_loss: 0.5055\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0777 - regression_loss: 1.5712 - classification_loss: 0.5065 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0836 - regression_loss: 1.5761 - classification_loss: 0.5075\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0847 - regression_loss: 1.5768 - classification_loss: 0.5079\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0866 - regression_loss: 1.5780 - classification_loss: 0.5086\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0869 - regression_loss: 1.5791 - classification_loss: 0.5078\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0869 - regression_loss: 1.5789 - classification_loss: 0.5080\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0845 - regression_loss: 1.5772 - classification_loss: 0.5074\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0835 - regression_loss: 1.5766 - classification_loss: 0.5069\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0811 - regression_loss: 1.5745 - classification_loss: 0.5066\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0810 - regression_loss: 1.5742 - classification_loss: 0.5068\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0818 - regression_loss: 1.5742 - classification_loss: 0.5077\n",
            "Epoch 23: saving model to ./snapshots\\resnet50_csv_23.h5\n",
            "\n",
            "Epoch 23: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
            "\n",
            "86/86 [==============================] - 84s 972ms/step - loss: 2.0818 - regression_loss: 1.5742 - classification_loss: 0.5077 - lr: 1.0000e-08\n",
            "Epoch 24/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:10 - loss: 1.9717 - regression_loss: 1.4927 - classification_loss: 0.4790\n",
            " 2/86 [..............................] - ETA: 1:21 - loss: 2.0698 - regression_loss: 1.5666 - classification_loss: 0.5032\n",
            " 3/86 [>.............................] - ETA: 1:31 - loss: 2.1802 - regression_loss: 1.6626 - classification_loss: 0.5176\n",
            " 4/86 [>.............................] - ETA: 1:16 - loss: 2.2556 - regression_loss: 1.7125 - classification_loss: 0.5431\n",
            " 5/86 [>.............................] - ETA: 1:19 - loss: 2.2169 - regression_loss: 1.6863 - classification_loss: 0.5305\n",
            " 6/86 [=>............................] - ETA: 1:22 - loss: 2.2500 - regression_loss: 1.7112 - classification_loss: 0.5388\n",
            " 7/86 [=>............................] - ETA: 1:18 - loss: 2.2506 - regression_loss: 1.7108 - classification_loss: 0.5398\n",
            " 8/86 [=>............................] - ETA: 1:16 - loss: 2.2196 - regression_loss: 1.6977 - classification_loss: 0.5219\n",
            " 9/86 [==>...........................] - ETA: 1:14 - loss: 2.2024 - regression_loss: 1.6887 - classification_loss: 0.5138\n",
            "10/86 [==>...........................] - ETA: 1:12 - loss: 2.1824 - regression_loss: 1.6656 - classification_loss: 0.5168\n",
            "11/86 [==>...........................] - ETA: 1:11 - loss: 2.1687 - regression_loss: 1.6518 - classification_loss: 0.5168\n",
            "12/86 [===>..........................] - ETA: 1:10 - loss: 2.1567 - regression_loss: 1.6444 - classification_loss: 0.5123\n",
            "13/86 [===>..........................] - ETA: 1:12 - loss: 2.1292 - regression_loss: 1.6204 - classification_loss: 0.5088\n",
            "14/86 [===>..........................] - ETA: 1:10 - loss: 2.1174 - regression_loss: 1.6116 - classification_loss: 0.5058\n",
            "15/86 [====>.........................] - ETA: 1:08 - loss: 2.0814 - regression_loss: 1.5842 - classification_loss: 0.4973\n",
            "16/86 [====>.........................] - ETA: 1:07 - loss: 2.0714 - regression_loss: 1.5751 - classification_loss: 0.4963\n",
            "17/86 [====>.........................] - ETA: 1:06 - loss: 2.0728 - regression_loss: 1.5764 - classification_loss: 0.4964\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.0622 - regression_loss: 1.5657 - classification_loss: 0.4964\n",
            "19/86 [=====>........................] - ETA: 1:04 - loss: 2.0625 - regression_loss: 1.5631 - classification_loss: 0.4994\n",
            "20/86 [=====>........................] - ETA: 1:02 - loss: 2.0443 - regression_loss: 1.5476 - classification_loss: 0.4967\n",
            "21/86 [======>.......................] - ETA: 1:01 - loss: 2.0341 - regression_loss: 1.5397 - classification_loss: 0.4944\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.0420 - regression_loss: 1.5450 - classification_loss: 0.4970\n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.0397 - regression_loss: 1.5332 - classification_loss: 0.5066 \n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.0441 - regression_loss: 1.5386 - classification_loss: 0.5055\n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.0486 - regression_loss: 1.5429 - classification_loss: 0.5057\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.0545 - regression_loss: 1.5450 - classification_loss: 0.5094\n",
            "27/86 [========>.....................] - ETA: 55s - loss: 2.0441 - regression_loss: 1.5369 - classification_loss: 0.5072\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.0542 - regression_loss: 1.5462 - classification_loss: 0.5080\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.0494 - regression_loss: 1.5412 - classification_loss: 0.5082\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.0471 - regression_loss: 1.5392 - classification_loss: 0.5079\n",
            "31/86 [=========>....................] - ETA: 51s - loss: 2.0378 - regression_loss: 1.5340 - classification_loss: 0.5037\n",
            "32/86 [==========>...................] - ETA: 50s - loss: 2.0430 - regression_loss: 1.5392 - classification_loss: 0.5038\n",
            "33/86 [==========>...................] - ETA: 49s - loss: 2.0364 - regression_loss: 1.5337 - classification_loss: 0.5028\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.0475 - regression_loss: 1.5427 - classification_loss: 0.5048\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.0536 - regression_loss: 1.5474 - classification_loss: 0.5062\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.0544 - regression_loss: 1.5489 - classification_loss: 0.5055\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.0572 - regression_loss: 1.5507 - classification_loss: 0.5065\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.0581 - regression_loss: 1.5524 - classification_loss: 0.5057\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.0598 - regression_loss: 1.5546 - classification_loss: 0.5052\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.0523 - regression_loss: 1.5495 - classification_loss: 0.5028\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.0566 - regression_loss: 1.5514 - classification_loss: 0.5052\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.0549 - regression_loss: 1.5501 - classification_loss: 0.5048\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.0563 - regression_loss: 1.5506 - classification_loss: 0.5057\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.0599 - regression_loss: 1.5546 - classification_loss: 0.5053\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.0636 - regression_loss: 1.5562 - classification_loss: 0.5074\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.0623 - regression_loss: 1.5549 - classification_loss: 0.5074\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.0677 - regression_loss: 1.5610 - classification_loss: 0.5067\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.0682 - regression_loss: 1.5620 - classification_loss: 0.5062\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.0647 - regression_loss: 1.5596 - classification_loss: 0.5052\n",
            "50/86 [================>.............] - ETA: 33s - loss: 2.0673 - regression_loss: 1.5615 - classification_loss: 0.5059\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.0660 - regression_loss: 1.5600 - classification_loss: 0.5060\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.0642 - regression_loss: 1.5595 - classification_loss: 0.5047\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0678 - regression_loss: 1.5617 - classification_loss: 0.5061\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0733 - regression_loss: 1.5679 - classification_loss: 0.5055\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0715 - regression_loss: 1.5665 - classification_loss: 0.5049\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0747 - regression_loss: 1.5696 - classification_loss: 0.5051\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0732 - regression_loss: 1.5685 - classification_loss: 0.5047\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0740 - regression_loss: 1.5696 - classification_loss: 0.5044\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0766 - regression_loss: 1.5720 - classification_loss: 0.5047\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0766 - regression_loss: 1.5722 - classification_loss: 0.5044\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0776 - regression_loss: 1.5734 - classification_loss: 0.5042\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.0757 - regression_loss: 1.5727 - classification_loss: 0.5030\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.0728 - regression_loss: 1.5710 - classification_loss: 0.5018\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.0751 - regression_loss: 1.5728 - classification_loss: 0.5023\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.0797 - regression_loss: 1.5772 - classification_loss: 0.5026\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0836 - regression_loss: 1.5809 - classification_loss: 0.5027\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0815 - regression_loss: 1.5799 - classification_loss: 0.5016\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0811 - regression_loss: 1.5804 - classification_loss: 0.5008\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0801 - regression_loss: 1.5788 - classification_loss: 0.5013\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0782 - regression_loss: 1.5780 - classification_loss: 0.5001\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0827 - regression_loss: 1.5800 - classification_loss: 0.5026\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0803 - regression_loss: 1.5779 - classification_loss: 0.5024\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0820 - regression_loss: 1.5799 - classification_loss: 0.5022\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0815 - regression_loss: 1.5794 - classification_loss: 0.5021\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0793 - regression_loss: 1.5776 - classification_loss: 0.5016\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0806 - regression_loss: 1.5783 - classification_loss: 0.5023 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0808 - regression_loss: 1.5784 - classification_loss: 0.5025\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0831 - regression_loss: 1.5812 - classification_loss: 0.5019\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0785 - regression_loss: 1.5781 - classification_loss: 0.5004\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0793 - regression_loss: 1.5786 - classification_loss: 0.5006\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0793 - regression_loss: 1.5784 - classification_loss: 0.5008\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0837 - regression_loss: 1.5822 - classification_loss: 0.5015\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0845 - regression_loss: 1.5826 - classification_loss: 0.5019\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0858 - regression_loss: 1.5836 - classification_loss: 0.5022\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0825 - regression_loss: 1.5815 - classification_loss: 0.5009\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0839 - regression_loss: 1.5827 - classification_loss: 0.5012\n",
            "Epoch 24: saving model to ./snapshots\\resnet50_csv_24.h5\n",
            "\n",
            "86/86 [==============================] - 83s 956ms/step - loss: 2.0839 - regression_loss: 1.5827 - classification_loss: 0.5012 - lr: 1.0000e-09\n",
            "Epoch 25/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:48 - loss: 2.2686 - regression_loss: 1.6873 - classification_loss: 0.5813\n",
            " 2/86 [..............................] - ETA: 1:35 - loss: 2.3610 - regression_loss: 1.7790 - classification_loss: 0.5821\n",
            " 3/86 [>.............................] - ETA: 1:32 - loss: 2.2624 - regression_loss: 1.7018 - classification_loss: 0.5607\n",
            " 4/86 [>.............................] - ETA: 1:14 - loss: 2.2497 - regression_loss: 1.6817 - classification_loss: 0.5680\n",
            " 5/86 [>.............................] - ETA: 1:19 - loss: 2.2525 - regression_loss: 1.6936 - classification_loss: 0.5589\n",
            " 6/86 [=>............................] - ETA: 1:25 - loss: 2.2796 - regression_loss: 1.7270 - classification_loss: 0.5526\n",
            " 7/86 [=>............................] - ETA: 1:19 - loss: 2.2394 - regression_loss: 1.7004 - classification_loss: 0.5391\n",
            " 8/86 [=>............................] - ETA: 1:20 - loss: 2.2495 - regression_loss: 1.7095 - classification_loss: 0.5400\n",
            " 9/86 [==>...........................] - ETA: 1:15 - loss: 2.2371 - regression_loss: 1.6984 - classification_loss: 0.5387\n",
            "10/86 [==>...........................] - ETA: 1:14 - loss: 2.2164 - regression_loss: 1.6809 - classification_loss: 0.5355\n",
            "11/86 [==>...........................] - ETA: 1:13 - loss: 2.1844 - regression_loss: 1.6612 - classification_loss: 0.5232\n",
            "12/86 [===>..........................] - ETA: 1:11 - loss: 2.1992 - regression_loss: 1.6762 - classification_loss: 0.5230\n",
            "13/86 [===>..........................] - ETA: 1:09 - loss: 2.1794 - regression_loss: 1.6617 - classification_loss: 0.5177\n",
            "14/86 [===>..........................] - ETA: 1:10 - loss: 2.1690 - regression_loss: 1.6508 - classification_loss: 0.5182\n",
            "15/86 [====>.........................] - ETA: 1:09 - loss: 2.1651 - regression_loss: 1.6510 - classification_loss: 0.5140\n",
            "16/86 [====>.........................] - ETA: 1:08 - loss: 2.1562 - regression_loss: 1.6462 - classification_loss: 0.5100\n",
            "17/86 [====>.........................] - ETA: 1:06 - loss: 2.1366 - regression_loss: 1.6315 - classification_loss: 0.5051\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.1252 - regression_loss: 1.6212 - classification_loss: 0.5041\n",
            "19/86 [=====>........................] - ETA: 1:04 - loss: 2.1187 - regression_loss: 1.6164 - classification_loss: 0.5023\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.1036 - regression_loss: 1.6013 - classification_loss: 0.5024\n",
            "21/86 [======>.......................] - ETA: 1:03 - loss: 2.1108 - regression_loss: 1.6034 - classification_loss: 0.5074\n",
            "22/86 [======>.......................] - ETA: 1:02 - loss: 2.0984 - regression_loss: 1.5932 - classification_loss: 0.5053\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.0889 - regression_loss: 1.5861 - classification_loss: 0.5028\n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.0923 - regression_loss: 1.5857 - classification_loss: 0.5066 \n",
            "25/86 [=======>......................] - ETA: 59s - loss: 2.0926 - regression_loss: 1.5846 - classification_loss: 0.5080\n",
            "26/86 [========>.....................] - ETA: 58s - loss: 2.1006 - regression_loss: 1.5937 - classification_loss: 0.5069\n",
            "27/86 [========>.....................] - ETA: 57s - loss: 2.0937 - regression_loss: 1.5891 - classification_loss: 0.5046\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.0925 - regression_loss: 1.5880 - classification_loss: 0.5044\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.0889 - regression_loss: 1.5852 - classification_loss: 0.5038\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.0766 - regression_loss: 1.5763 - classification_loss: 0.5002\n",
            "31/86 [=========>....................] - ETA: 53s - loss: 2.0774 - regression_loss: 1.5774 - classification_loss: 0.5000\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.0885 - regression_loss: 1.5821 - classification_loss: 0.5064\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.0894 - regression_loss: 1.5821 - classification_loss: 0.5073\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.0928 - regression_loss: 1.5846 - classification_loss: 0.5082\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.0962 - regression_loss: 1.5865 - classification_loss: 0.5096\n",
            "36/86 [===========>..................] - ETA: 48s - loss: 2.0998 - regression_loss: 1.5897 - classification_loss: 0.5100\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.0963 - regression_loss: 1.5872 - classification_loss: 0.5091\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.1014 - regression_loss: 1.5916 - classification_loss: 0.5098\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.1095 - regression_loss: 1.5981 - classification_loss: 0.5114\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.1055 - regression_loss: 1.5956 - classification_loss: 0.5099\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.1067 - regression_loss: 1.5956 - classification_loss: 0.5111\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1102 - regression_loss: 1.5979 - classification_loss: 0.5123\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1055 - regression_loss: 1.5934 - classification_loss: 0.5121\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1088 - regression_loss: 1.5963 - classification_loss: 0.5125\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.1037 - regression_loss: 1.5920 - classification_loss: 0.5117\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1061 - regression_loss: 1.5932 - classification_loss: 0.5129\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1050 - regression_loss: 1.5922 - classification_loss: 0.5129\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1034 - regression_loss: 1.5919 - classification_loss: 0.5115\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1062 - regression_loss: 1.5943 - classification_loss: 0.5119\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1064 - regression_loss: 1.5929 - classification_loss: 0.5135\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1045 - regression_loss: 1.5924 - classification_loss: 0.5121\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1092 - regression_loss: 1.5961 - classification_loss: 0.5131\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1064 - regression_loss: 1.5941 - classification_loss: 0.5123\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1062 - regression_loss: 1.5940 - classification_loss: 0.5121\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1104 - regression_loss: 1.5975 - classification_loss: 0.5129\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1194 - regression_loss: 1.6048 - classification_loss: 0.5146\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1180 - regression_loss: 1.6027 - classification_loss: 0.5152\n",
            "58/86 [===================>..........] - ETA: 27s - loss: 2.1123 - regression_loss: 1.5978 - classification_loss: 0.5145\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.1125 - regression_loss: 1.5976 - classification_loss: 0.5150\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.1129 - regression_loss: 1.5982 - classification_loss: 0.5147\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.1139 - regression_loss: 1.5972 - classification_loss: 0.5167\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.1146 - regression_loss: 1.5978 - classification_loss: 0.5167\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.1138 - regression_loss: 1.5980 - classification_loss: 0.5158\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.1153 - regression_loss: 1.6000 - classification_loss: 0.5154\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1179 - regression_loss: 1.6029 - classification_loss: 0.5150\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1164 - regression_loss: 1.6013 - classification_loss: 0.5151\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1180 - regression_loss: 1.6028 - classification_loss: 0.5152\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1165 - regression_loss: 1.6014 - classification_loss: 0.5151\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1191 - regression_loss: 1.6039 - classification_loss: 0.5152\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1186 - regression_loss: 1.6028 - classification_loss: 0.5157\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1259 - regression_loss: 1.6088 - classification_loss: 0.5171\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1242 - regression_loss: 1.6072 - classification_loss: 0.5170\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1200 - regression_loss: 1.6041 - classification_loss: 0.5159\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1204 - regression_loss: 1.6044 - classification_loss: 0.5160\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1185 - regression_loss: 1.6023 - classification_loss: 0.5162\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1199 - regression_loss: 1.6043 - classification_loss: 0.5157 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1202 - regression_loss: 1.6044 - classification_loss: 0.5158\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1190 - regression_loss: 1.6036 - classification_loss: 0.5154\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1181 - regression_loss: 1.6040 - classification_loss: 0.5142\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1180 - regression_loss: 1.6038 - classification_loss: 0.5142\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.1142 - regression_loss: 1.6011 - classification_loss: 0.5130\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.1162 - regression_loss: 1.6032 - classification_loss: 0.5131\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.1161 - regression_loss: 1.6032 - classification_loss: 0.5129\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.1174 - regression_loss: 1.6044 - classification_loss: 0.5131\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.1156 - regression_loss: 1.6029 - classification_loss: 0.5127\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1129 - regression_loss: 1.6005 - classification_loss: 0.5124\n",
            "Epoch 25: saving model to ./snapshots\\resnet50_csv_25.h5\n",
            "\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
            "\n",
            "86/86 [==============================] - 94s 1s/step - loss: 2.1129 - regression_loss: 1.6005 - classification_loss: 0.5124 - lr: 1.0000e-09\n",
            "Epoch 26/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:06 - loss: 2.1277 - regression_loss: 1.6357 - classification_loss: 0.4920\n",
            " 2/86 [..............................] - ETA: 1:19 - loss: 2.2683 - regression_loss: 1.6891 - classification_loss: 0.5792\n",
            " 3/86 [>.............................] - ETA: 1:12 - loss: 2.3519 - regression_loss: 1.7583 - classification_loss: 0.5936\n",
            " 4/86 [>.............................] - ETA: 1:10 - loss: 2.3294 - regression_loss: 1.7471 - classification_loss: 0.5822\n",
            " 5/86 [>.............................] - ETA: 1:31 - loss: 2.3135 - regression_loss: 1.7466 - classification_loss: 0.5668\n",
            " 6/86 [=>............................] - ETA: 1:22 - loss: 2.2812 - regression_loss: 1.7243 - classification_loss: 0.5569\n",
            " 7/86 [=>............................] - ETA: 1:19 - loss: 2.2359 - regression_loss: 1.6934 - classification_loss: 0.5425\n",
            " 8/86 [=>............................] - ETA: 1:16 - loss: 2.2106 - regression_loss: 1.6743 - classification_loss: 0.5363\n",
            " 9/86 [==>...........................] - ETA: 1:16 - loss: 2.1979 - regression_loss: 1.6704 - classification_loss: 0.5274\n",
            "10/86 [==>...........................] - ETA: 1:15 - loss: 2.2012 - regression_loss: 1.6708 - classification_loss: 0.5303\n",
            "11/86 [==>...........................] - ETA: 1:14 - loss: 2.1428 - regression_loss: 1.6251 - classification_loss: 0.5177\n",
            "12/86 [===>..........................] - ETA: 1:11 - loss: 2.1571 - regression_loss: 1.6351 - classification_loss: 0.5219\n",
            "13/86 [===>..........................] - ETA: 1:09 - loss: 2.1523 - regression_loss: 1.6312 - classification_loss: 0.5211\n",
            "14/86 [===>..........................] - ETA: 1:08 - loss: 2.1539 - regression_loss: 1.6314 - classification_loss: 0.5226\n",
            "15/86 [====>.........................] - ETA: 1:07 - loss: 2.1592 - regression_loss: 1.6339 - classification_loss: 0.5252\n",
            "16/86 [====>.........................] - ETA: 1:06 - loss: 2.1439 - regression_loss: 1.6256 - classification_loss: 0.5183\n",
            "17/86 [====>.........................] - ETA: 1:04 - loss: 2.1329 - regression_loss: 1.6133 - classification_loss: 0.5197\n",
            "18/86 [=====>........................] - ETA: 1:03 - loss: 2.1456 - regression_loss: 1.6232 - classification_loss: 0.5224\n",
            "19/86 [=====>........................] - ETA: 1:04 - loss: 2.1428 - regression_loss: 1.6217 - classification_loss: 0.5211\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.1506 - regression_loss: 1.6289 - classification_loss: 0.5217\n",
            "21/86 [======>.......................] - ETA: 1:01 - loss: 2.1365 - regression_loss: 1.6159 - classification_loss: 0.5207\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.1253 - regression_loss: 1.6087 - classification_loss: 0.5166\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.1218 - regression_loss: 1.6040 - classification_loss: 0.5178\n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.1118 - regression_loss: 1.5957 - classification_loss: 0.5161 \n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.1208 - regression_loss: 1.6043 - classification_loss: 0.5165\n",
            "26/86 [========>.....................] - ETA: 56s - loss: 2.1284 - regression_loss: 1.6116 - classification_loss: 0.5168\n",
            "27/86 [========>.....................] - ETA: 55s - loss: 2.1331 - regression_loss: 1.6149 - classification_loss: 0.5182\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.1304 - regression_loss: 1.6132 - classification_loss: 0.5171\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.1356 - regression_loss: 1.6164 - classification_loss: 0.5192\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.1353 - regression_loss: 1.6163 - classification_loss: 0.5191\n",
            "31/86 [=========>....................] - ETA: 53s - loss: 2.1342 - regression_loss: 1.6150 - classification_loss: 0.5192\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.1396 - regression_loss: 1.6194 - classification_loss: 0.5202\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.1421 - regression_loss: 1.6201 - classification_loss: 0.5221\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.1465 - regression_loss: 1.6232 - classification_loss: 0.5233\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.1473 - regression_loss: 1.6247 - classification_loss: 0.5226\n",
            "36/86 [===========>..................] - ETA: 48s - loss: 2.1425 - regression_loss: 1.6239 - classification_loss: 0.5187\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.1401 - regression_loss: 1.6234 - classification_loss: 0.5166\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.1391 - regression_loss: 1.6222 - classification_loss: 0.5169\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.1431 - regression_loss: 1.6246 - classification_loss: 0.5185\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.1409 - regression_loss: 1.6234 - classification_loss: 0.5175\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.1382 - regression_loss: 1.6217 - classification_loss: 0.5165\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1356 - regression_loss: 1.6196 - classification_loss: 0.5160\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1356 - regression_loss: 1.6195 - classification_loss: 0.5161\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1378 - regression_loss: 1.6205 - classification_loss: 0.5173\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.1325 - regression_loss: 1.6163 - classification_loss: 0.5162\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1285 - regression_loss: 1.6134 - classification_loss: 0.5151\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1393 - regression_loss: 1.6223 - classification_loss: 0.5170\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1366 - regression_loss: 1.6199 - classification_loss: 0.5167\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1298 - regression_loss: 1.6148 - classification_loss: 0.5150\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1267 - regression_loss: 1.6133 - classification_loss: 0.5133\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1292 - regression_loss: 1.6142 - classification_loss: 0.5150\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1265 - regression_loss: 1.6124 - classification_loss: 0.5141\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1262 - regression_loss: 1.6121 - classification_loss: 0.5141\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1234 - regression_loss: 1.6097 - classification_loss: 0.5137\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1213 - regression_loss: 1.6070 - classification_loss: 0.5143\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1228 - regression_loss: 1.6086 - classification_loss: 0.5142\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1218 - regression_loss: 1.6079 - classification_loss: 0.5139\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1204 - regression_loss: 1.6064 - classification_loss: 0.5140\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1196 - regression_loss: 1.6058 - classification_loss: 0.5138\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1165 - regression_loss: 1.6045 - classification_loss: 0.5119\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1114 - regression_loss: 1.6010 - classification_loss: 0.5104\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1144 - regression_loss: 1.6029 - classification_loss: 0.5115\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.1172 - regression_loss: 1.6048 - classification_loss: 0.5124\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.1196 - regression_loss: 1.6069 - classification_loss: 0.5126\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1195 - regression_loss: 1.6065 - classification_loss: 0.5130\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1172 - regression_loss: 1.6044 - classification_loss: 0.5128\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1167 - regression_loss: 1.6040 - classification_loss: 0.5126\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1179 - regression_loss: 1.6054 - classification_loss: 0.5125\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1212 - regression_loss: 1.6086 - classification_loss: 0.5126\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1210 - regression_loss: 1.6089 - classification_loss: 0.5121\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1198 - regression_loss: 1.6080 - classification_loss: 0.5118\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1192 - regression_loss: 1.6072 - classification_loss: 0.5120\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1160 - regression_loss: 1.6043 - classification_loss: 0.5117\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1145 - regression_loss: 1.6036 - classification_loss: 0.5109\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1119 - regression_loss: 1.6016 - classification_loss: 0.5103\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1155 - regression_loss: 1.6042 - classification_loss: 0.5113 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1154 - regression_loss: 1.6038 - classification_loss: 0.5116\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1168 - regression_loss: 1.6048 - classification_loss: 0.5120\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1207 - regression_loss: 1.6078 - classification_loss: 0.5129\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1203 - regression_loss: 1.6085 - classification_loss: 0.5118\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1188 - regression_loss: 1.6068 - classification_loss: 0.5120\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1223 - regression_loss: 1.6096 - classification_loss: 0.5127\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1239 - regression_loss: 1.6111 - classification_loss: 0.5128\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1228 - regression_loss: 1.6100 - classification_loss: 0.5127\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1255 - regression_loss: 1.6123 - classification_loss: 0.5132\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1213 - regression_loss: 1.6088 - classification_loss: 0.5125\n",
            "Epoch 26: saving model to ./snapshots\\resnet50_csv_26.h5\n",
            "\n",
            "86/86 [==============================] - 83s 957ms/step - loss: 2.1213 - regression_loss: 1.6088 - classification_loss: 0.5125 - lr: 1.0000e-10\n",
            "Epoch 27/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:00 - loss: 2.0023 - regression_loss: 1.5245 - classification_loss: 0.4778\n",
            " 2/86 [..............................] - ETA: 1:08 - loss: 2.1228 - regression_loss: 1.6388 - classification_loss: 0.4840\n",
            " 3/86 [>.............................] - ETA: 1:09 - loss: 2.0878 - regression_loss: 1.5978 - classification_loss: 0.4900\n",
            " 4/86 [>.............................] - ETA: 1:16 - loss: 2.1490 - regression_loss: 1.6263 - classification_loss: 0.5227\n",
            " 5/86 [>.............................] - ETA: 1:14 - loss: 2.1732 - regression_loss: 1.6489 - classification_loss: 0.5244\n",
            " 6/86 [=>............................] - ETA: 1:13 - loss: 2.2195 - regression_loss: 1.6874 - classification_loss: 0.5321\n",
            " 7/86 [=>............................] - ETA: 1:12 - loss: 2.2418 - regression_loss: 1.7081 - classification_loss: 0.5337\n",
            " 8/86 [=>............................] - ETA: 1:12 - loss: 2.2604 - regression_loss: 1.7185 - classification_loss: 0.5419\n",
            " 9/86 [==>...........................] - ETA: 1:11 - loss: 2.2146 - regression_loss: 1.6852 - classification_loss: 0.5294\n",
            "10/86 [==>...........................] - ETA: 1:10 - loss: 2.2271 - regression_loss: 1.6953 - classification_loss: 0.5318\n",
            "11/86 [==>...........................] - ETA: 1:09 - loss: 2.1601 - regression_loss: 1.6449 - classification_loss: 0.5152\n",
            "12/86 [===>..........................] - ETA: 1:08 - loss: 2.1654 - regression_loss: 1.6504 - classification_loss: 0.5150\n",
            "13/86 [===>..........................] - ETA: 1:07 - loss: 2.1894 - regression_loss: 1.6690 - classification_loss: 0.5204\n",
            "14/86 [===>..........................] - ETA: 1:06 - loss: 2.1641 - regression_loss: 1.6446 - classification_loss: 0.5195\n",
            "15/86 [====>.........................] - ETA: 1:05 - loss: 2.1583 - regression_loss: 1.6408 - classification_loss: 0.5175\n",
            "16/86 [====>.........................] - ETA: 1:05 - loss: 2.1535 - regression_loss: 1.6372 - classification_loss: 0.5164\n",
            "17/86 [====>.........................] - ETA: 1:04 - loss: 2.1640 - regression_loss: 1.6434 - classification_loss: 0.5206\n",
            "18/86 [=====>........................] - ETA: 1:03 - loss: 2.1547 - regression_loss: 1.6376 - classification_loss: 0.5172\n",
            "19/86 [=====>........................] - ETA: 1:01 - loss: 2.1532 - regression_loss: 1.6356 - classification_loss: 0.5175\n",
            "20/86 [=====>........................] - ETA: 1:01 - loss: 2.1439 - regression_loss: 1.6281 - classification_loss: 0.5158\n",
            "21/86 [======>.......................] - ETA: 1:01 - loss: 2.1324 - regression_loss: 1.6191 - classification_loss: 0.5134\n",
            "22/86 [======>.......................] - ETA: 1:03 - loss: 2.1509 - regression_loss: 1.6318 - classification_loss: 0.5191\n",
            "23/86 [=======>......................] - ETA: 1:03 - loss: 2.1613 - regression_loss: 1.6387 - classification_loss: 0.5226\n",
            "24/86 [=======>......................] - ETA: 1:04 - loss: 2.1617 - regression_loss: 1.6389 - classification_loss: 0.5228\n",
            "25/86 [=======>......................] - ETA: 1:06 - loss: 2.1638 - regression_loss: 1.6377 - classification_loss: 0.5261\n",
            "26/86 [========>.....................] - ETA: 1:07 - loss: 2.1602 - regression_loss: 1.6352 - classification_loss: 0.5250\n",
            "27/86 [========>.....................] - ETA: 1:08 - loss: 2.1764 - regression_loss: 1.6454 - classification_loss: 0.5309\n",
            "28/86 [========>.....................] - ETA: 1:08 - loss: 2.1815 - regression_loss: 1.6497 - classification_loss: 0.5318\n",
            "29/86 [=========>....................] - ETA: 1:08 - loss: 2.1752 - regression_loss: 1.6451 - classification_loss: 0.5301\n",
            "30/86 [=========>....................] - ETA: 1:06 - loss: 2.1668 - regression_loss: 1.6365 - classification_loss: 0.5303\n",
            "31/86 [=========>....................] - ETA: 1:05 - loss: 2.1657 - regression_loss: 1.6350 - classification_loss: 0.5307\n",
            "32/86 [==========>...................] - ETA: 1:04 - loss: 2.1631 - regression_loss: 1.6326 - classification_loss: 0.5305\n",
            "33/86 [==========>...................] - ETA: 1:02 - loss: 2.1626 - regression_loss: 1.6320 - classification_loss: 0.5306\n",
            "34/86 [==========>...................] - ETA: 1:01 - loss: 2.1628 - regression_loss: 1.6323 - classification_loss: 0.5305\n",
            "35/86 [===========>..................] - ETA: 59s - loss: 2.1653 - regression_loss: 1.6349 - classification_loss: 0.5303 \n",
            "36/86 [===========>..................] - ETA: 58s - loss: 2.1663 - regression_loss: 1.6368 - classification_loss: 0.5295\n",
            "37/86 [===========>..................] - ETA: 57s - loss: 2.1696 - regression_loss: 1.6420 - classification_loss: 0.5276\n",
            "38/86 [============>.................] - ETA: 55s - loss: 2.1598 - regression_loss: 1.6338 - classification_loss: 0.5260\n",
            "39/86 [============>.................] - ETA: 54s - loss: 2.1564 - regression_loss: 1.6304 - classification_loss: 0.5259\n",
            "40/86 [============>.................] - ETA: 53s - loss: 2.1611 - regression_loss: 1.6351 - classification_loss: 0.5260\n",
            "41/86 [=============>................] - ETA: 51s - loss: 2.1597 - regression_loss: 1.6290 - classification_loss: 0.5307\n",
            "42/86 [=============>................] - ETA: 50s - loss: 2.1585 - regression_loss: 1.6281 - classification_loss: 0.5304\n",
            "43/86 [==============>...............] - ETA: 49s - loss: 2.1579 - regression_loss: 1.6279 - classification_loss: 0.5300\n",
            "44/86 [==============>...............] - ETA: 47s - loss: 2.1560 - regression_loss: 1.6257 - classification_loss: 0.5303\n",
            "45/86 [==============>...............] - ETA: 46s - loss: 2.1482 - regression_loss: 1.6198 - classification_loss: 0.5284\n",
            "46/86 [===============>..............] - ETA: 44s - loss: 2.1432 - regression_loss: 1.6160 - classification_loss: 0.5272\n",
            "47/86 [===============>..............] - ETA: 43s - loss: 2.1385 - regression_loss: 1.6129 - classification_loss: 0.5256\n",
            "48/86 [===============>..............] - ETA: 42s - loss: 2.1358 - regression_loss: 1.6113 - classification_loss: 0.5245\n",
            "49/86 [================>.............] - ETA: 41s - loss: 2.1387 - regression_loss: 1.6142 - classification_loss: 0.5245\n",
            "50/86 [================>.............] - ETA: 39s - loss: 2.1414 - regression_loss: 1.6164 - classification_loss: 0.5249\n",
            "51/86 [================>.............] - ETA: 38s - loss: 2.1362 - regression_loss: 1.6132 - classification_loss: 0.5230\n",
            "52/86 [=================>............] - ETA: 37s - loss: 2.1349 - regression_loss: 1.6128 - classification_loss: 0.5222\n",
            "53/86 [=================>............] - ETA: 36s - loss: 2.1321 - regression_loss: 1.6101 - classification_loss: 0.5220\n",
            "54/86 [=================>............] - ETA: 34s - loss: 2.1395 - regression_loss: 1.6156 - classification_loss: 0.5240\n",
            "55/86 [==================>...........] - ETA: 33s - loss: 2.1404 - regression_loss: 1.6163 - classification_loss: 0.5241\n",
            "56/86 [==================>...........] - ETA: 32s - loss: 2.1441 - regression_loss: 1.6195 - classification_loss: 0.5246\n",
            "57/86 [==================>...........] - ETA: 31s - loss: 2.1420 - regression_loss: 1.6177 - classification_loss: 0.5243\n",
            "58/86 [===================>..........] - ETA: 30s - loss: 2.1408 - regression_loss: 1.6166 - classification_loss: 0.5242\n",
            "59/86 [===================>..........] - ETA: 29s - loss: 2.1406 - regression_loss: 1.6165 - classification_loss: 0.5241\n",
            "60/86 [===================>..........] - ETA: 28s - loss: 2.1413 - regression_loss: 1.6174 - classification_loss: 0.5239\n",
            "61/86 [====================>.........] - ETA: 26s - loss: 2.1429 - regression_loss: 1.6197 - classification_loss: 0.5233\n",
            "62/86 [====================>.........] - ETA: 25s - loss: 2.1383 - regression_loss: 1.6167 - classification_loss: 0.5216\n",
            "63/86 [====================>.........] - ETA: 24s - loss: 2.1431 - regression_loss: 1.6200 - classification_loss: 0.5230\n",
            "64/86 [=====================>........] - ETA: 23s - loss: 2.1398 - regression_loss: 1.6170 - classification_loss: 0.5228\n",
            "65/86 [=====================>........] - ETA: 22s - loss: 2.1350 - regression_loss: 1.6143 - classification_loss: 0.5207\n",
            "66/86 [======================>.......] - ETA: 21s - loss: 2.1368 - regression_loss: 1.6160 - classification_loss: 0.5208\n",
            "67/86 [======================>.......] - ETA: 20s - loss: 2.1356 - regression_loss: 1.6155 - classification_loss: 0.5202\n",
            "68/86 [======================>.......] - ETA: 19s - loss: 2.1357 - regression_loss: 1.6153 - classification_loss: 0.5203\n",
            "69/86 [=======================>......] - ETA: 17s - loss: 2.1354 - regression_loss: 1.6158 - classification_loss: 0.5196\n",
            "70/86 [=======================>......] - ETA: 16s - loss: 2.1349 - regression_loss: 1.6156 - classification_loss: 0.5194\n",
            "71/86 [=======================>......] - ETA: 15s - loss: 2.1313 - regression_loss: 1.6131 - classification_loss: 0.5183\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.1290 - regression_loss: 1.6107 - classification_loss: 0.5183\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.1298 - regression_loss: 1.6115 - classification_loss: 0.5182\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.1303 - regression_loss: 1.6130 - classification_loss: 0.5173\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.1249 - regression_loss: 1.6091 - classification_loss: 0.5158\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.1248 - regression_loss: 1.6088 - classification_loss: 0.5160\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.1252 - regression_loss: 1.6091 - classification_loss: 0.5161 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.1237 - regression_loss: 1.6085 - classification_loss: 0.5151\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.1206 - regression_loss: 1.6065 - classification_loss: 0.5141\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.1184 - regression_loss: 1.6053 - classification_loss: 0.5131\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.1204 - regression_loss: 1.6056 - classification_loss: 0.5148\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.1161 - regression_loss: 1.6021 - classification_loss: 0.5140\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.1160 - regression_loss: 1.6018 - classification_loss: 0.5142\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.1158 - regression_loss: 1.6019 - classification_loss: 0.5139\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.1125 - regression_loss: 1.5988 - classification_loss: 0.5137\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1105 - regression_loss: 1.5975 - classification_loss: 0.5130\n",
            "Epoch 27: saving model to ./snapshots\\resnet50_csv_27.h5\n",
            "\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 9.99999943962493e-12.\n",
            "\n",
            "86/86 [==============================] - 91s 1s/step - loss: 2.1105 - regression_loss: 1.5975 - classification_loss: 0.5130 - lr: 1.0000e-10\n",
            "Epoch 28/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:16 - loss: 2.6378 - regression_loss: 2.0291 - classification_loss: 0.6087\n",
            " 2/86 [..............................] - ETA: 46s - loss: 2.3025 - regression_loss: 1.7552 - classification_loss: 0.5473 \n",
            " 3/86 [>.............................] - ETA: 1:17 - loss: 2.2863 - regression_loss: 1.7298 - classification_loss: 0.5565\n",
            " 4/86 [>.............................] - ETA: 1:22 - loss: 2.2900 - regression_loss: 1.7332 - classification_loss: 0.5568\n",
            " 5/86 [>.............................] - ETA: 1:27 - loss: 2.2367 - regression_loss: 1.6965 - classification_loss: 0.5402\n",
            " 6/86 [=>............................] - ETA: 1:22 - loss: 2.2317 - regression_loss: 1.6897 - classification_loss: 0.5420\n",
            " 7/86 [=>............................] - ETA: 1:20 - loss: 2.2219 - regression_loss: 1.6819 - classification_loss: 0.5400\n",
            " 8/86 [=>............................] - ETA: 1:21 - loss: 2.2203 - regression_loss: 1.6842 - classification_loss: 0.5361\n",
            " 9/86 [==>...........................] - ETA: 1:20 - loss: 2.2012 - regression_loss: 1.6644 - classification_loss: 0.5368\n",
            "10/86 [==>...........................] - ETA: 1:20 - loss: 2.1815 - regression_loss: 1.6467 - classification_loss: 0.5348\n",
            "11/86 [==>...........................] - ETA: 1:17 - loss: 2.1498 - regression_loss: 1.6221 - classification_loss: 0.5277\n",
            "12/86 [===>..........................] - ETA: 1:15 - loss: 2.1364 - regression_loss: 1.6200 - classification_loss: 0.5163\n",
            "13/86 [===>..........................] - ETA: 1:13 - loss: 2.1407 - regression_loss: 1.6225 - classification_loss: 0.5182\n",
            "14/86 [===>..........................] - ETA: 1:13 - loss: 2.1372 - regression_loss: 1.6165 - classification_loss: 0.5207\n",
            "15/86 [====>.........................] - ETA: 1:10 - loss: 2.1513 - regression_loss: 1.6211 - classification_loss: 0.5302\n",
            "16/86 [====>.........................] - ETA: 1:09 - loss: 2.1558 - regression_loss: 1.6269 - classification_loss: 0.5289\n",
            "17/86 [====>.........................] - ETA: 1:07 - loss: 2.1588 - regression_loss: 1.6297 - classification_loss: 0.5290\n",
            "18/86 [=====>........................] - ETA: 1:06 - loss: 2.1369 - regression_loss: 1.6161 - classification_loss: 0.5207\n",
            "19/86 [=====>........................] - ETA: 1:05 - loss: 2.1428 - regression_loss: 1.6215 - classification_loss: 0.5213\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.1202 - regression_loss: 1.6044 - classification_loss: 0.5158\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.1235 - regression_loss: 1.6058 - classification_loss: 0.5177\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.1224 - regression_loss: 1.6035 - classification_loss: 0.5189\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.1072 - regression_loss: 1.5927 - classification_loss: 0.5145\n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.1070 - regression_loss: 1.5924 - classification_loss: 0.5146 \n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.1055 - regression_loss: 1.5891 - classification_loss: 0.5163\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.1025 - regression_loss: 1.5878 - classification_loss: 0.5147\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.0941 - regression_loss: 1.5816 - classification_loss: 0.5125\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.0846 - regression_loss: 1.5753 - classification_loss: 0.5094\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.0858 - regression_loss: 1.5769 - classification_loss: 0.5089\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.0849 - regression_loss: 1.5691 - classification_loss: 0.5158\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.0872 - regression_loss: 1.5706 - classification_loss: 0.5166\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.0827 - regression_loss: 1.5660 - classification_loss: 0.5167\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.0856 - regression_loss: 1.5686 - classification_loss: 0.5170\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.0839 - regression_loss: 1.5667 - classification_loss: 0.5172\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.0845 - regression_loss: 1.5670 - classification_loss: 0.5175\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.0810 - regression_loss: 1.5643 - classification_loss: 0.5167\n",
            "37/86 [===========>..................] - ETA: 45s - loss: 2.0849 - regression_loss: 1.5688 - classification_loss: 0.5161\n",
            "38/86 [============>.................] - ETA: 44s - loss: 2.0865 - regression_loss: 1.5701 - classification_loss: 0.5165\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.0854 - regression_loss: 1.5688 - classification_loss: 0.5166\n",
            "40/86 [============>.................] - ETA: 42s - loss: 2.0869 - regression_loss: 1.5697 - classification_loss: 0.5172\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.0876 - regression_loss: 1.5715 - classification_loss: 0.5161\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.0876 - regression_loss: 1.5722 - classification_loss: 0.5155\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.0863 - regression_loss: 1.5708 - classification_loss: 0.5155\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.0742 - regression_loss: 1.5618 - classification_loss: 0.5124\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.0747 - regression_loss: 1.5636 - classification_loss: 0.5110\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.0774 - regression_loss: 1.5664 - classification_loss: 0.5110\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.0774 - regression_loss: 1.5664 - classification_loss: 0.5110\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.0854 - regression_loss: 1.5719 - classification_loss: 0.5135\n",
            "49/86 [================>.............] - ETA: 34s - loss: 2.0821 - regression_loss: 1.5687 - classification_loss: 0.5135\n",
            "50/86 [================>.............] - ETA: 33s - loss: 2.0844 - regression_loss: 1.5697 - classification_loss: 0.5146\n",
            "51/86 [================>.............] - ETA: 32s - loss: 2.0796 - regression_loss: 1.5647 - classification_loss: 0.5149\n",
            "52/86 [=================>............] - ETA: 31s - loss: 2.0787 - regression_loss: 1.5644 - classification_loss: 0.5144\n",
            "53/86 [=================>............] - ETA: 30s - loss: 2.0845 - regression_loss: 1.5687 - classification_loss: 0.5158\n",
            "54/86 [=================>............] - ETA: 29s - loss: 2.0903 - regression_loss: 1.5738 - classification_loss: 0.5165\n",
            "55/86 [==================>...........] - ETA: 28s - loss: 2.0853 - regression_loss: 1.5695 - classification_loss: 0.5159\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0882 - regression_loss: 1.5729 - classification_loss: 0.5153\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0896 - regression_loss: 1.5734 - classification_loss: 0.5161\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0848 - regression_loss: 1.5685 - classification_loss: 0.5163\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0859 - regression_loss: 1.5692 - classification_loss: 0.5166\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0849 - regression_loss: 1.5691 - classification_loss: 0.5158\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0893 - regression_loss: 1.5727 - classification_loss: 0.5166\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.0910 - regression_loss: 1.5731 - classification_loss: 0.5179\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.0918 - regression_loss: 1.5730 - classification_loss: 0.5188\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.0963 - regression_loss: 1.5770 - classification_loss: 0.5193\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.0973 - regression_loss: 1.5763 - classification_loss: 0.5209\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.0942 - regression_loss: 1.5737 - classification_loss: 0.5204\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.0911 - regression_loss: 1.5719 - classification_loss: 0.5192\n",
            "68/86 [======================>.......] - ETA: 16s - loss: 2.0950 - regression_loss: 1.5757 - classification_loss: 0.5193\n",
            "69/86 [=======================>......] - ETA: 15s - loss: 2.0963 - regression_loss: 1.5766 - classification_loss: 0.5197\n",
            "70/86 [=======================>......] - ETA: 14s - loss: 2.0960 - regression_loss: 1.5766 - classification_loss: 0.5193\n",
            "71/86 [=======================>......] - ETA: 13s - loss: 2.0990 - regression_loss: 1.5795 - classification_loss: 0.5195\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1015 - regression_loss: 1.5812 - classification_loss: 0.5203\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0993 - regression_loss: 1.5803 - classification_loss: 0.5190\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1012 - regression_loss: 1.5805 - classification_loss: 0.5208\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0953 - regression_loss: 1.5764 - classification_loss: 0.5189\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0978 - regression_loss: 1.5793 - classification_loss: 0.5184 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1003 - regression_loss: 1.5813 - classification_loss: 0.5190\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1022 - regression_loss: 1.5831 - classification_loss: 0.5192\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1047 - regression_loss: 1.5855 - classification_loss: 0.5193\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1103 - regression_loss: 1.5891 - classification_loss: 0.5212\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1077 - regression_loss: 1.5870 - classification_loss: 0.5208\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1069 - regression_loss: 1.5859 - classification_loss: 0.5210\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1039 - regression_loss: 1.5836 - classification_loss: 0.5203\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1061 - regression_loss: 1.5857 - classification_loss: 0.5204\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1033 - regression_loss: 1.5832 - classification_loss: 0.5201\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1033 - regression_loss: 1.5835 - classification_loss: 0.5198\n",
            "Epoch 28: saving model to ./snapshots\\resnet50_csv_28.h5\n",
            "\n",
            "86/86 [==============================] - 82s 942ms/step - loss: 2.1033 - regression_loss: 1.5835 - classification_loss: 0.5198 - lr: 1.0000e-11\n",
            "Epoch 29/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:02 - loss: 2.1838 - regression_loss: 1.6733 - classification_loss: 0.5105\n",
            " 2/86 [..............................] - ETA: 1:29 - loss: 2.1238 - regression_loss: 1.6410 - classification_loss: 0.4828\n",
            " 3/86 [>.............................] - ETA: 1:23 - loss: 2.0359 - regression_loss: 1.5701 - classification_loss: 0.4658\n",
            " 4/86 [>.............................] - ETA: 1:21 - loss: 1.9828 - regression_loss: 1.5277 - classification_loss: 0.4551\n",
            " 5/86 [>.............................] - ETA: 1:26 - loss: 2.0353 - regression_loss: 1.5691 - classification_loss: 0.4662\n",
            " 6/86 [=>............................] - ETA: 1:27 - loss: 2.0892 - regression_loss: 1.6100 - classification_loss: 0.4792\n",
            " 7/86 [=>............................] - ETA: 1:33 - loss: 2.0717 - regression_loss: 1.5945 - classification_loss: 0.4772\n",
            " 8/86 [=>............................] - ETA: 1:48 - loss: 2.0658 - regression_loss: 1.5830 - classification_loss: 0.4828\n",
            " 9/86 [==>...........................] - ETA: 1:50 - loss: 2.0445 - regression_loss: 1.5630 - classification_loss: 0.4815\n",
            "10/86 [==>...........................] - ETA: 1:55 - loss: 2.0366 - regression_loss: 1.5590 - classification_loss: 0.4776\n",
            "11/86 [==>...........................] - ETA: 1:57 - loss: 2.0501 - regression_loss: 1.5582 - classification_loss: 0.4919\n",
            "12/86 [===>..........................] - ETA: 1:56 - loss: 2.0335 - regression_loss: 1.5442 - classification_loss: 0.4894\n",
            "13/86 [===>..........................] - ETA: 1:53 - loss: 2.0431 - regression_loss: 1.5481 - classification_loss: 0.4951\n",
            "14/86 [===>..........................] - ETA: 1:51 - loss: 2.0427 - regression_loss: 1.5449 - classification_loss: 0.4977\n",
            "15/86 [====>.........................] - ETA: 1:45 - loss: 2.0410 - regression_loss: 1.5450 - classification_loss: 0.4960\n",
            "16/86 [====>.........................] - ETA: 1:42 - loss: 2.0389 - regression_loss: 1.5452 - classification_loss: 0.4937\n",
            "17/86 [====>.........................] - ETA: 1:38 - loss: 2.0315 - regression_loss: 1.5396 - classification_loss: 0.4919\n",
            "18/86 [=====>........................] - ETA: 1:35 - loss: 2.0388 - regression_loss: 1.5482 - classification_loss: 0.4906\n",
            "19/86 [=====>........................] - ETA: 1:32 - loss: 2.0555 - regression_loss: 1.5566 - classification_loss: 0.4989\n",
            "20/86 [=====>........................] - ETA: 1:30 - loss: 2.0601 - regression_loss: 1.5589 - classification_loss: 0.5012\n",
            "21/86 [======>.......................] - ETA: 1:27 - loss: 2.0768 - regression_loss: 1.5713 - classification_loss: 0.5055\n",
            "22/86 [======>.......................] - ETA: 1:25 - loss: 2.0840 - regression_loss: 1.5790 - classification_loss: 0.5050\n",
            "23/86 [=======>......................] - ETA: 1:22 - loss: 2.0797 - regression_loss: 1.5762 - classification_loss: 0.5036\n",
            "24/86 [=======>......................] - ETA: 1:20 - loss: 2.0889 - regression_loss: 1.5841 - classification_loss: 0.5048\n",
            "25/86 [=======>......................] - ETA: 1:18 - loss: 2.0902 - regression_loss: 1.5832 - classification_loss: 0.5070\n",
            "26/86 [========>.....................] - ETA: 1:16 - loss: 2.0823 - regression_loss: 1.5776 - classification_loss: 0.5047\n",
            "27/86 [========>.....................] - ETA: 1:14 - loss: 2.0630 - regression_loss: 1.5645 - classification_loss: 0.4985\n",
            "28/86 [========>.....................] - ETA: 1:12 - loss: 2.0615 - regression_loss: 1.5628 - classification_loss: 0.4986\n",
            "29/86 [=========>....................] - ETA: 1:10 - loss: 2.0540 - regression_loss: 1.5578 - classification_loss: 0.4962\n",
            "30/86 [=========>....................] - ETA: 1:08 - loss: 2.0617 - regression_loss: 1.5639 - classification_loss: 0.4978\n",
            "31/86 [=========>....................] - ETA: 1:06 - loss: 2.0549 - regression_loss: 1.5586 - classification_loss: 0.4963\n",
            "32/86 [==========>...................] - ETA: 1:05 - loss: 2.0577 - regression_loss: 1.5613 - classification_loss: 0.4964\n",
            "33/86 [==========>...................] - ETA: 1:03 - loss: 2.0584 - regression_loss: 1.5597 - classification_loss: 0.4987\n",
            "34/86 [==========>...................] - ETA: 1:01 - loss: 2.0569 - regression_loss: 1.5580 - classification_loss: 0.4989\n",
            "35/86 [===========>..................] - ETA: 1:00 - loss: 2.0555 - regression_loss: 1.5563 - classification_loss: 0.4991\n",
            "36/86 [===========>..................] - ETA: 58s - loss: 2.0641 - regression_loss: 1.5635 - classification_loss: 0.5006 \n",
            "37/86 [===========>..................] - ETA: 56s - loss: 2.0641 - regression_loss: 1.5615 - classification_loss: 0.5026\n",
            "38/86 [============>.................] - ETA: 55s - loss: 2.0715 - regression_loss: 1.5679 - classification_loss: 0.5036\n",
            "39/86 [============>.................] - ETA: 53s - loss: 2.0739 - regression_loss: 1.5703 - classification_loss: 0.5036\n",
            "40/86 [============>.................] - ETA: 52s - loss: 2.0705 - regression_loss: 1.5680 - classification_loss: 0.5026\n",
            "41/86 [=============>................] - ETA: 50s - loss: 2.0682 - regression_loss: 1.5650 - classification_loss: 0.5032\n",
            "42/86 [=============>................] - ETA: 49s - loss: 2.0807 - regression_loss: 1.5749 - classification_loss: 0.5059\n",
            "43/86 [==============>...............] - ETA: 48s - loss: 2.0877 - regression_loss: 1.5807 - classification_loss: 0.5070\n",
            "44/86 [==============>...............] - ETA: 46s - loss: 2.0930 - regression_loss: 1.5854 - classification_loss: 0.5076\n",
            "45/86 [==============>...............] - ETA: 45s - loss: 2.0880 - regression_loss: 1.5818 - classification_loss: 0.5062\n",
            "46/86 [===============>..............] - ETA: 44s - loss: 2.0826 - regression_loss: 1.5775 - classification_loss: 0.5051\n",
            "47/86 [===============>..............] - ETA: 42s - loss: 2.0759 - regression_loss: 1.5726 - classification_loss: 0.5033\n",
            "48/86 [===============>..............] - ETA: 41s - loss: 2.0746 - regression_loss: 1.5725 - classification_loss: 0.5021\n",
            "49/86 [================>.............] - ETA: 40s - loss: 2.0684 - regression_loss: 1.5683 - classification_loss: 0.5002\n",
            "50/86 [================>.............] - ETA: 39s - loss: 2.0730 - regression_loss: 1.5723 - classification_loss: 0.5006\n",
            "51/86 [================>.............] - ETA: 37s - loss: 2.0726 - regression_loss: 1.5718 - classification_loss: 0.5008\n",
            "52/86 [=================>............] - ETA: 36s - loss: 2.0762 - regression_loss: 1.5739 - classification_loss: 0.5022\n",
            "53/86 [=================>............] - ETA: 35s - loss: 2.0754 - regression_loss: 1.5693 - classification_loss: 0.5061\n",
            "54/86 [=================>............] - ETA: 34s - loss: 2.0779 - regression_loss: 1.5718 - classification_loss: 0.5062\n",
            "55/86 [==================>...........] - ETA: 33s - loss: 2.0761 - regression_loss: 1.5713 - classification_loss: 0.5048\n",
            "56/86 [==================>...........] - ETA: 32s - loss: 2.0780 - regression_loss: 1.5713 - classification_loss: 0.5067\n",
            "57/86 [==================>...........] - ETA: 31s - loss: 2.0797 - regression_loss: 1.5729 - classification_loss: 0.5069\n",
            "58/86 [===================>..........] - ETA: 30s - loss: 2.0859 - regression_loss: 1.5784 - classification_loss: 0.5075\n",
            "59/86 [===================>..........] - ETA: 28s - loss: 2.0888 - regression_loss: 1.5810 - classification_loss: 0.5078\n",
            "60/86 [===================>..........] - ETA: 27s - loss: 2.0886 - regression_loss: 1.5815 - classification_loss: 0.5071\n",
            "61/86 [====================>.........] - ETA: 26s - loss: 2.0900 - regression_loss: 1.5825 - classification_loss: 0.5076\n",
            "62/86 [====================>.........] - ETA: 25s - loss: 2.0887 - regression_loss: 1.5814 - classification_loss: 0.5073\n",
            "63/86 [====================>.........] - ETA: 24s - loss: 2.0898 - regression_loss: 1.5821 - classification_loss: 0.5076\n",
            "64/86 [=====================>........] - ETA: 23s - loss: 2.0872 - regression_loss: 1.5796 - classification_loss: 0.5076\n",
            "65/86 [=====================>........] - ETA: 22s - loss: 2.0904 - regression_loss: 1.5823 - classification_loss: 0.5081\n",
            "66/86 [======================>.......] - ETA: 21s - loss: 2.0901 - regression_loss: 1.5814 - classification_loss: 0.5088\n",
            "67/86 [======================>.......] - ETA: 20s - loss: 2.0887 - regression_loss: 1.5797 - classification_loss: 0.5090\n",
            "68/86 [======================>.......] - ETA: 18s - loss: 2.0905 - regression_loss: 1.5816 - classification_loss: 0.5089\n",
            "69/86 [=======================>......] - ETA: 17s - loss: 2.0908 - regression_loss: 1.5824 - classification_loss: 0.5084\n",
            "70/86 [=======================>......] - ETA: 16s - loss: 2.0880 - regression_loss: 1.5796 - classification_loss: 0.5083\n",
            "71/86 [=======================>......] - ETA: 15s - loss: 2.0885 - regression_loss: 1.5797 - classification_loss: 0.5088\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.0892 - regression_loss: 1.5804 - classification_loss: 0.5088\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.0945 - regression_loss: 1.5837 - classification_loss: 0.5107\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.0954 - regression_loss: 1.5834 - classification_loss: 0.5121\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.0969 - regression_loss: 1.5845 - classification_loss: 0.5124\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.0937 - regression_loss: 1.5822 - classification_loss: 0.5114\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.0934 - regression_loss: 1.5822 - classification_loss: 0.5112 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.0953 - regression_loss: 1.5833 - classification_loss: 0.5120\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.0965 - regression_loss: 1.5846 - classification_loss: 0.5119\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.0981 - regression_loss: 1.5858 - classification_loss: 0.5122\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.0962 - regression_loss: 1.5848 - classification_loss: 0.5115\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.0990 - regression_loss: 1.5869 - classification_loss: 0.5121\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.0999 - regression_loss: 1.5878 - classification_loss: 0.5121\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.1007 - regression_loss: 1.5886 - classification_loss: 0.5122\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.1033 - regression_loss: 1.5915 - classification_loss: 0.5117\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1034 - regression_loss: 1.5913 - classification_loss: 0.5121\n",
            "Epoch 29: saving model to ./snapshots\\resnet50_csv_29.h5\n",
            "\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
            "\n",
            "86/86 [==============================] - 90s 1s/step - loss: 2.1034 - regression_loss: 1.5913 - classification_loss: 0.5121 - lr: 1.0000e-11\n",
            "Epoch 30/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:06 - loss: 1.9021 - regression_loss: 1.4255 - classification_loss: 0.4766\n",
            " 2/86 [..............................] - ETA: 1:29 - loss: 2.0813 - regression_loss: 1.5800 - classification_loss: 0.5014\n",
            " 3/86 [>.............................] - ETA: 1:15 - loss: 2.1204 - regression_loss: 1.6237 - classification_loss: 0.4967\n",
            " 4/86 [>.............................] - ETA: 1:14 - loss: 2.1299 - regression_loss: 1.6258 - classification_loss: 0.5040\n",
            " 5/86 [>.............................] - ETA: 1:16 - loss: 2.0820 - regression_loss: 1.5915 - classification_loss: 0.4906\n",
            " 6/86 [=>............................] - ETA: 1:15 - loss: 2.0399 - regression_loss: 1.5620 - classification_loss: 0.4779\n",
            " 7/86 [=>............................] - ETA: 1:16 - loss: 2.0737 - regression_loss: 1.5885 - classification_loss: 0.4852\n",
            " 8/86 [=>............................] - ETA: 1:14 - loss: 2.0784 - regression_loss: 1.5923 - classification_loss: 0.4861\n",
            " 9/86 [==>...........................] - ETA: 1:14 - loss: 2.1164 - regression_loss: 1.6182 - classification_loss: 0.4982\n",
            "10/86 [==>...........................] - ETA: 1:13 - loss: 2.1244 - regression_loss: 1.6229 - classification_loss: 0.5015\n",
            "11/86 [==>...........................] - ETA: 1:11 - loss: 2.1363 - regression_loss: 1.6327 - classification_loss: 0.5036\n",
            "12/86 [===>..........................] - ETA: 1:09 - loss: 2.1195 - regression_loss: 1.6208 - classification_loss: 0.4987\n",
            "13/86 [===>..........................] - ETA: 1:10 - loss: 2.1272 - regression_loss: 1.6238 - classification_loss: 0.5034\n",
            "14/86 [===>..........................] - ETA: 1:10 - loss: 2.1327 - regression_loss: 1.6283 - classification_loss: 0.5044\n",
            "15/86 [====>.........................] - ETA: 1:08 - loss: 2.1348 - regression_loss: 1.6293 - classification_loss: 0.5055\n",
            "16/86 [====>.........................] - ETA: 1:07 - loss: 2.1004 - regression_loss: 1.6033 - classification_loss: 0.4971\n",
            "17/86 [====>.........................] - ETA: 1:06 - loss: 2.1040 - regression_loss: 1.6043 - classification_loss: 0.4997\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.1124 - regression_loss: 1.6091 - classification_loss: 0.5034\n",
            "19/86 [=====>........................] - ETA: 1:03 - loss: 2.1103 - regression_loss: 1.6067 - classification_loss: 0.5036\n",
            "20/86 [=====>........................] - ETA: 1:02 - loss: 2.0970 - regression_loss: 1.5969 - classification_loss: 0.5001\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.1055 - regression_loss: 1.6010 - classification_loss: 0.5045\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.1056 - regression_loss: 1.5999 - classification_loss: 0.5057\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.0962 - regression_loss: 1.5929 - classification_loss: 0.5032\n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.0935 - regression_loss: 1.5912 - classification_loss: 0.5024 \n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.0872 - regression_loss: 1.5855 - classification_loss: 0.5017\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.0912 - regression_loss: 1.5899 - classification_loss: 0.5013\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.0882 - regression_loss: 1.5850 - classification_loss: 0.5031\n",
            "28/86 [========>.....................] - ETA: 54s - loss: 2.0789 - regression_loss: 1.5776 - classification_loss: 0.5013\n",
            "29/86 [=========>....................] - ETA: 53s - loss: 2.0737 - regression_loss: 1.5745 - classification_loss: 0.4992\n",
            "30/86 [=========>....................] - ETA: 52s - loss: 2.0802 - regression_loss: 1.5783 - classification_loss: 0.5019\n",
            "31/86 [=========>....................] - ETA: 51s - loss: 2.0788 - regression_loss: 1.5787 - classification_loss: 0.5001\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.0767 - regression_loss: 1.5784 - classification_loss: 0.4983\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.0778 - regression_loss: 1.5776 - classification_loss: 0.5002\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.0843 - regression_loss: 1.5816 - classification_loss: 0.5027\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.0792 - regression_loss: 1.5787 - classification_loss: 0.5005\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.0692 - regression_loss: 1.5715 - classification_loss: 0.4977\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.0662 - regression_loss: 1.5684 - classification_loss: 0.4978\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.0588 - regression_loss: 1.5616 - classification_loss: 0.4972\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.0588 - regression_loss: 1.5626 - classification_loss: 0.4962\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.0607 - regression_loss: 1.5628 - classification_loss: 0.4978\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.0599 - regression_loss: 1.5616 - classification_loss: 0.4983\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.0540 - regression_loss: 1.5571 - classification_loss: 0.4969\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.0513 - regression_loss: 1.5535 - classification_loss: 0.4978\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.0502 - regression_loss: 1.5527 - classification_loss: 0.4975\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.0422 - regression_loss: 1.5470 - classification_loss: 0.4951\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.0356 - regression_loss: 1.5428 - classification_loss: 0.4929\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.0347 - regression_loss: 1.5412 - classification_loss: 0.4935\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.0327 - regression_loss: 1.5397 - classification_loss: 0.4930\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.0396 - regression_loss: 1.5454 - classification_loss: 0.4942\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.0406 - regression_loss: 1.5458 - classification_loss: 0.4948\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.0421 - regression_loss: 1.5477 - classification_loss: 0.4945\n",
            "52/86 [=================>............] - ETA: 33s - loss: 2.0427 - regression_loss: 1.5480 - classification_loss: 0.4947\n",
            "53/86 [=================>............] - ETA: 32s - loss: 2.0433 - regression_loss: 1.5477 - classification_loss: 0.4957\n",
            "54/86 [=================>............] - ETA: 32s - loss: 2.0532 - regression_loss: 1.5549 - classification_loss: 0.4983\n",
            "55/86 [==================>...........] - ETA: 32s - loss: 2.0553 - regression_loss: 1.5553 - classification_loss: 0.5000\n",
            "56/86 [==================>...........] - ETA: 31s - loss: 2.0557 - regression_loss: 1.5555 - classification_loss: 0.5002\n",
            "57/86 [==================>...........] - ETA: 30s - loss: 2.0584 - regression_loss: 1.5564 - classification_loss: 0.5020\n",
            "58/86 [===================>..........] - ETA: 29s - loss: 2.0610 - regression_loss: 1.5580 - classification_loss: 0.5030\n",
            "59/86 [===================>..........] - ETA: 28s - loss: 2.0627 - regression_loss: 1.5593 - classification_loss: 0.5034\n",
            "60/86 [===================>..........] - ETA: 27s - loss: 2.0616 - regression_loss: 1.5601 - classification_loss: 0.5015\n",
            "61/86 [====================>.........] - ETA: 26s - loss: 2.0616 - regression_loss: 1.5609 - classification_loss: 0.5007\n",
            "62/86 [====================>.........] - ETA: 25s - loss: 2.0568 - regression_loss: 1.5579 - classification_loss: 0.4989\n",
            "63/86 [====================>.........] - ETA: 24s - loss: 2.0589 - regression_loss: 1.5603 - classification_loss: 0.4986\n",
            "64/86 [=====================>........] - ETA: 23s - loss: 2.0607 - regression_loss: 1.5613 - classification_loss: 0.4995\n",
            "65/86 [=====================>........] - ETA: 22s - loss: 2.0628 - regression_loss: 1.5630 - classification_loss: 0.4998\n",
            "66/86 [======================>.......] - ETA: 20s - loss: 2.0626 - regression_loss: 1.5634 - classification_loss: 0.4993\n",
            "67/86 [======================>.......] - ETA: 19s - loss: 2.0670 - regression_loss: 1.5670 - classification_loss: 0.5000\n",
            "68/86 [======================>.......] - ETA: 18s - loss: 2.0671 - regression_loss: 1.5670 - classification_loss: 0.5001\n",
            "69/86 [=======================>......] - ETA: 17s - loss: 2.0662 - regression_loss: 1.5656 - classification_loss: 0.5006\n",
            "70/86 [=======================>......] - ETA: 16s - loss: 2.0638 - regression_loss: 1.5647 - classification_loss: 0.4992\n",
            "71/86 [=======================>......] - ETA: 15s - loss: 2.0648 - regression_loss: 1.5653 - classification_loss: 0.4995\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.0661 - regression_loss: 1.5665 - classification_loss: 0.4995\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.0721 - regression_loss: 1.5703 - classification_loss: 0.5018\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.0736 - regression_loss: 1.5711 - classification_loss: 0.5025\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.0730 - regression_loss: 1.5711 - classification_loss: 0.5019\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.0765 - regression_loss: 1.5739 - classification_loss: 0.5026\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.0725 - regression_loss: 1.5704 - classification_loss: 0.5021 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.0757 - regression_loss: 1.5732 - classification_loss: 0.5025\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.0806 - regression_loss: 1.5770 - classification_loss: 0.5035\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.0832 - regression_loss: 1.5789 - classification_loss: 0.5043\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.0813 - regression_loss: 1.5775 - classification_loss: 0.5038\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.0759 - regression_loss: 1.5737 - classification_loss: 0.5022\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.0724 - regression_loss: 1.5713 - classification_loss: 0.5011\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.0723 - regression_loss: 1.5711 - classification_loss: 0.5012\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.0698 - regression_loss: 1.5695 - classification_loss: 0.5003\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0682 - regression_loss: 1.5683 - classification_loss: 0.4999\n",
            "Epoch 30: saving model to ./snapshots\\resnet50_csv_30.h5\n",
            "\n",
            "86/86 [==============================] - 90s 1s/step - loss: 2.0682 - regression_loss: 1.5683 - classification_loss: 0.4999 - lr: 1.0000e-12\n",
            "Epoch 31/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:00 - loss: 1.9468 - regression_loss: 1.4759 - classification_loss: 0.4709\n",
            " 2/86 [..............................] - ETA: 1:00 - loss: 1.9408 - regression_loss: 1.4850 - classification_loss: 0.4558\n",
            " 3/86 [>.............................] - ETA: 1:15 - loss: 1.9740 - regression_loss: 1.4979 - classification_loss: 0.4761\n",
            " 4/86 [>.............................] - ETA: 1:41 - loss: 2.0586 - regression_loss: 1.5609 - classification_loss: 0.4977\n",
            " 5/86 [>.............................] - ETA: 1:53 - loss: 2.0931 - regression_loss: 1.5864 - classification_loss: 0.5067\n",
            " 6/86 [=>............................] - ETA: 1:59 - loss: 2.1331 - regression_loss: 1.6183 - classification_loss: 0.5148\n",
            " 7/86 [=>............................] - ETA: 2:05 - loss: 2.1217 - regression_loss: 1.6214 - classification_loss: 0.5004\n",
            " 8/86 [=>............................] - ETA: 2:10 - loss: 2.1062 - regression_loss: 1.6009 - classification_loss: 0.5053\n",
            " 9/86 [==>...........................] - ETA: 2:10 - loss: 2.1403 - regression_loss: 1.6246 - classification_loss: 0.5157\n",
            "10/86 [==>...........................] - ETA: 2:03 - loss: 2.1432 - regression_loss: 1.6264 - classification_loss: 0.5168\n",
            "11/86 [==>...........................] - ETA: 1:55 - loss: 2.1346 - regression_loss: 1.6223 - classification_loss: 0.5123\n",
            "12/86 [===>..........................] - ETA: 1:50 - loss: 2.1385 - regression_loss: 1.6287 - classification_loss: 0.5098\n",
            "13/86 [===>..........................] - ETA: 1:46 - loss: 2.1540 - regression_loss: 1.6464 - classification_loss: 0.5076\n",
            "14/86 [===>..........................] - ETA: 1:41 - loss: 2.1369 - regression_loss: 1.6274 - classification_loss: 0.5095\n",
            "15/86 [====>.........................] - ETA: 1:37 - loss: 2.1298 - regression_loss: 1.6221 - classification_loss: 0.5077\n",
            "16/86 [====>.........................] - ETA: 1:35 - loss: 2.1593 - regression_loss: 1.6459 - classification_loss: 0.5134\n",
            "17/86 [====>.........................] - ETA: 1:31 - loss: 2.1357 - regression_loss: 1.6242 - classification_loss: 0.5115\n",
            "18/86 [=====>........................] - ETA: 1:29 - loss: 2.1212 - regression_loss: 1.6101 - classification_loss: 0.5111\n",
            "19/86 [=====>........................] - ETA: 1:27 - loss: 2.1080 - regression_loss: 1.6009 - classification_loss: 0.5071\n",
            "20/86 [=====>........................] - ETA: 1:23 - loss: 2.1149 - regression_loss: 1.6068 - classification_loss: 0.5081\n",
            "21/86 [======>.......................] - ETA: 1:21 - loss: 2.1039 - regression_loss: 1.5985 - classification_loss: 0.5054\n",
            "22/86 [======>.......................] - ETA: 1:20 - loss: 2.1012 - regression_loss: 1.5970 - classification_loss: 0.5042\n",
            "23/86 [=======>......................] - ETA: 1:18 - loss: 2.0955 - regression_loss: 1.5924 - classification_loss: 0.5031\n",
            "24/86 [=======>......................] - ETA: 1:15 - loss: 2.1022 - regression_loss: 1.5966 - classification_loss: 0.5056\n",
            "25/86 [=======>......................] - ETA: 1:13 - loss: 2.1095 - regression_loss: 1.6015 - classification_loss: 0.5080\n",
            "26/86 [========>.....................] - ETA: 1:12 - loss: 2.0893 - regression_loss: 1.5857 - classification_loss: 0.5036\n",
            "27/86 [========>.....................] - ETA: 1:10 - loss: 2.0941 - regression_loss: 1.5869 - classification_loss: 0.5072\n",
            "28/86 [========>.....................] - ETA: 1:08 - loss: 2.0983 - regression_loss: 1.5889 - classification_loss: 0.5094\n",
            "29/86 [=========>....................] - ETA: 1:07 - loss: 2.1056 - regression_loss: 1.5936 - classification_loss: 0.5121\n",
            "30/86 [=========>....................] - ETA: 1:06 - loss: 2.1051 - regression_loss: 1.5925 - classification_loss: 0.5126\n",
            "31/86 [=========>....................] - ETA: 1:04 - loss: 2.1039 - regression_loss: 1.5922 - classification_loss: 0.5117\n",
            "32/86 [==========>...................] - ETA: 1:02 - loss: 2.1115 - regression_loss: 1.5994 - classification_loss: 0.5121\n",
            "33/86 [==========>...................] - ETA: 1:01 - loss: 2.1179 - regression_loss: 1.6035 - classification_loss: 0.5144\n",
            "34/86 [==========>...................] - ETA: 59s - loss: 2.1198 - regression_loss: 1.6048 - classification_loss: 0.5150 \n",
            "35/86 [===========>..................] - ETA: 58s - loss: 2.1240 - regression_loss: 1.6086 - classification_loss: 0.5154\n",
            "36/86 [===========>..................] - ETA: 57s - loss: 2.1281 - regression_loss: 1.6114 - classification_loss: 0.5166\n",
            "37/86 [===========>..................] - ETA: 55s - loss: 2.1283 - regression_loss: 1.6110 - classification_loss: 0.5172\n",
            "38/86 [============>.................] - ETA: 54s - loss: 2.1396 - regression_loss: 1.6205 - classification_loss: 0.5191\n",
            "39/86 [============>.................] - ETA: 53s - loss: 2.1406 - regression_loss: 1.6195 - classification_loss: 0.5211\n",
            "40/86 [============>.................] - ETA: 51s - loss: 2.1387 - regression_loss: 1.6176 - classification_loss: 0.5211\n",
            "41/86 [=============>................] - ETA: 50s - loss: 2.1298 - regression_loss: 1.6120 - classification_loss: 0.5178\n",
            "42/86 [=============>................] - ETA: 49s - loss: 2.1319 - regression_loss: 1.6147 - classification_loss: 0.5172\n",
            "43/86 [==============>...............] - ETA: 47s - loss: 2.1261 - regression_loss: 1.6110 - classification_loss: 0.5151\n",
            "44/86 [==============>...............] - ETA: 46s - loss: 2.1222 - regression_loss: 1.6084 - classification_loss: 0.5138\n",
            "45/86 [==============>...............] - ETA: 45s - loss: 2.1284 - regression_loss: 1.6112 - classification_loss: 0.5172\n",
            "46/86 [===============>..............] - ETA: 43s - loss: 2.1241 - regression_loss: 1.6074 - classification_loss: 0.5166\n",
            "47/86 [===============>..............] - ETA: 42s - loss: 2.1264 - regression_loss: 1.6083 - classification_loss: 0.5181\n",
            "48/86 [===============>..............] - ETA: 41s - loss: 2.1144 - regression_loss: 1.5993 - classification_loss: 0.5151\n",
            "49/86 [================>.............] - ETA: 40s - loss: 2.1127 - regression_loss: 1.5977 - classification_loss: 0.5150\n",
            "50/86 [================>.............] - ETA: 39s - loss: 2.1122 - regression_loss: 1.5968 - classification_loss: 0.5154\n",
            "51/86 [================>.............] - ETA: 37s - loss: 2.1079 - regression_loss: 1.5940 - classification_loss: 0.5139\n",
            "52/86 [=================>............] - ETA: 36s - loss: 2.1076 - regression_loss: 1.5932 - classification_loss: 0.5144\n",
            "53/86 [=================>............] - ETA: 35s - loss: 2.1111 - regression_loss: 1.5948 - classification_loss: 0.5163\n",
            "54/86 [=================>............] - ETA: 34s - loss: 2.1120 - regression_loss: 1.5955 - classification_loss: 0.5165\n",
            "55/86 [==================>...........] - ETA: 33s - loss: 2.1142 - regression_loss: 1.5981 - classification_loss: 0.5161\n",
            "56/86 [==================>...........] - ETA: 32s - loss: 2.1126 - regression_loss: 1.5968 - classification_loss: 0.5157\n",
            "57/86 [==================>...........] - ETA: 30s - loss: 2.1083 - regression_loss: 1.5928 - classification_loss: 0.5155\n",
            "58/86 [===================>..........] - ETA: 29s - loss: 2.1063 - regression_loss: 1.5907 - classification_loss: 0.5156\n",
            "59/86 [===================>..........] - ETA: 28s - loss: 2.1106 - regression_loss: 1.5926 - classification_loss: 0.5180\n",
            "60/86 [===================>..........] - ETA: 27s - loss: 2.1072 - regression_loss: 1.5905 - classification_loss: 0.5167\n",
            "61/86 [====================>.........] - ETA: 26s - loss: 2.1089 - regression_loss: 1.5921 - classification_loss: 0.5167\n",
            "62/86 [====================>.........] - ETA: 25s - loss: 2.1109 - regression_loss: 1.5933 - classification_loss: 0.5176\n",
            "63/86 [====================>.........] - ETA: 24s - loss: 2.1063 - regression_loss: 1.5897 - classification_loss: 0.5166\n",
            "64/86 [=====================>........] - ETA: 23s - loss: 2.1087 - regression_loss: 1.5920 - classification_loss: 0.5167\n",
            "65/86 [=====================>........] - ETA: 21s - loss: 2.1107 - regression_loss: 1.5942 - classification_loss: 0.5165\n",
            "66/86 [======================>.......] - ETA: 20s - loss: 2.1086 - regression_loss: 1.5927 - classification_loss: 0.5159\n",
            "67/86 [======================>.......] - ETA: 19s - loss: 2.1056 - regression_loss: 1.5915 - classification_loss: 0.5141\n",
            "68/86 [======================>.......] - ETA: 18s - loss: 2.1043 - regression_loss: 1.5903 - classification_loss: 0.5141\n",
            "69/86 [=======================>......] - ETA: 17s - loss: 2.1024 - regression_loss: 1.5889 - classification_loss: 0.5134\n",
            "70/86 [=======================>......] - ETA: 16s - loss: 2.1017 - regression_loss: 1.5886 - classification_loss: 0.5130\n",
            "71/86 [=======================>......] - ETA: 15s - loss: 2.0990 - regression_loss: 1.5870 - classification_loss: 0.5120\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.0964 - regression_loss: 1.5861 - classification_loss: 0.5103\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.0986 - regression_loss: 1.5881 - classification_loss: 0.5104\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.1013 - regression_loss: 1.5906 - classification_loss: 0.5106\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.1000 - regression_loss: 1.5881 - classification_loss: 0.5119\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.0979 - regression_loss: 1.5866 - classification_loss: 0.5113\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.0979 - regression_loss: 1.5863 - classification_loss: 0.5116 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.0982 - regression_loss: 1.5862 - classification_loss: 0.5121\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.0986 - regression_loss: 1.5871 - classification_loss: 0.5115\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.1004 - regression_loss: 1.5889 - classification_loss: 0.5115\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.0996 - regression_loss: 1.5878 - classification_loss: 0.5118\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.0985 - regression_loss: 1.5868 - classification_loss: 0.5117\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.0987 - regression_loss: 1.5879 - classification_loss: 0.5108\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.1020 - regression_loss: 1.5902 - classification_loss: 0.5118\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.1052 - regression_loss: 1.5914 - classification_loss: 0.5138\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1045 - regression_loss: 1.5914 - classification_loss: 0.5131\n",
            "Epoch 31: saving model to ./snapshots\\resnet50_csv_31.h5\n",
            "\n",
            "86/86 [==============================] - 88s 1s/step - loss: 2.1045 - regression_loss: 1.5914 - classification_loss: 0.5131 - lr: 1.0000e-12\n",
            "Epoch 32/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:25 - loss: 1.9086 - regression_loss: 1.5106 - classification_loss: 0.3980\n",
            " 2/86 [..............................] - ETA: 58s - loss: 1.9262 - regression_loss: 1.4958 - classification_loss: 0.4304 \n",
            " 3/86 [>.............................] - ETA: 1:05 - loss: 2.0521 - regression_loss: 1.5888 - classification_loss: 0.4634\n",
            " 4/86 [>.............................] - ETA: 1:15 - loss: 2.1089 - regression_loss: 1.6086 - classification_loss: 0.5002\n",
            " 5/86 [>.............................] - ETA: 1:13 - loss: 2.1148 - regression_loss: 1.6107 - classification_loss: 0.5041\n",
            " 6/86 [=>............................] - ETA: 1:20 - loss: 2.0934 - regression_loss: 1.5865 - classification_loss: 0.5069\n",
            " 7/86 [=>............................] - ETA: 1:33 - loss: 2.1336 - regression_loss: 1.6140 - classification_loss: 0.5196\n",
            " 8/86 [=>............................] - ETA: 1:49 - loss: 2.1263 - regression_loss: 1.6156 - classification_loss: 0.5107\n",
            " 9/86 [==>...........................] - ETA: 1:47 - loss: 2.0913 - regression_loss: 1.5875 - classification_loss: 0.5038\n",
            "10/86 [==>...........................] - ETA: 1:56 - loss: 2.1135 - regression_loss: 1.6028 - classification_loss: 0.5107\n",
            "11/86 [==>...........................] - ETA: 1:56 - loss: 2.0957 - regression_loss: 1.5873 - classification_loss: 0.5084\n",
            "12/86 [===>..........................] - ETA: 1:52 - loss: 2.0944 - regression_loss: 1.5903 - classification_loss: 0.5041\n",
            "13/86 [===>..........................] - ETA: 1:48 - loss: 2.1150 - regression_loss: 1.6071 - classification_loss: 0.5078\n",
            "14/86 [===>..........................] - ETA: 1:43 - loss: 2.1324 - regression_loss: 1.6187 - classification_loss: 0.5138\n",
            "15/86 [====>.........................] - ETA: 1:39 - loss: 2.1342 - regression_loss: 1.6193 - classification_loss: 0.5149\n",
            "16/86 [====>.........................] - ETA: 1:36 - loss: 2.1366 - regression_loss: 1.6222 - classification_loss: 0.5144\n",
            "17/86 [====>.........................] - ETA: 1:33 - loss: 2.1363 - regression_loss: 1.6216 - classification_loss: 0.5147\n",
            "18/86 [=====>........................] - ETA: 1:29 - loss: 2.1144 - regression_loss: 1.6068 - classification_loss: 0.5076\n",
            "19/86 [=====>........................] - ETA: 1:26 - loss: 2.1104 - regression_loss: 1.6041 - classification_loss: 0.5064\n",
            "20/86 [=====>........................] - ETA: 1:24 - loss: 2.1083 - regression_loss: 1.6001 - classification_loss: 0.5082\n",
            "21/86 [======>.......................] - ETA: 1:21 - loss: 2.1089 - regression_loss: 1.6026 - classification_loss: 0.5063\n",
            "22/86 [======>.......................] - ETA: 1:18 - loss: 2.1074 - regression_loss: 1.6001 - classification_loss: 0.5073\n",
            "23/86 [=======>......................] - ETA: 1:16 - loss: 2.1074 - regression_loss: 1.5905 - classification_loss: 0.5169\n",
            "24/86 [=======>......................] - ETA: 1:15 - loss: 2.1023 - regression_loss: 1.5865 - classification_loss: 0.5158\n",
            "25/86 [=======>......................] - ETA: 1:13 - loss: 2.1125 - regression_loss: 1.5931 - classification_loss: 0.5194\n",
            "26/86 [========>.....................] - ETA: 1:11 - loss: 2.1013 - regression_loss: 1.5833 - classification_loss: 0.5180\n",
            "27/86 [========>.....................] - ETA: 1:09 - loss: 2.0957 - regression_loss: 1.5795 - classification_loss: 0.5162\n",
            "28/86 [========>.....................] - ETA: 1:07 - loss: 2.0956 - regression_loss: 1.5783 - classification_loss: 0.5172\n",
            "29/86 [=========>....................] - ETA: 1:06 - loss: 2.0978 - regression_loss: 1.5810 - classification_loss: 0.5168\n",
            "30/86 [=========>....................] - ETA: 1:04 - loss: 2.0949 - regression_loss: 1.5783 - classification_loss: 0.5166\n",
            "31/86 [=========>....................] - ETA: 1:02 - loss: 2.0872 - regression_loss: 1.5720 - classification_loss: 0.5152\n",
            "32/86 [==========>...................] - ETA: 1:00 - loss: 2.0891 - regression_loss: 1.5734 - classification_loss: 0.5158\n",
            "33/86 [==========>...................] - ETA: 59s - loss: 2.0901 - regression_loss: 1.5743 - classification_loss: 0.5158 \n",
            "34/86 [==========>...................] - ETA: 58s - loss: 2.0831 - regression_loss: 1.5671 - classification_loss: 0.5161\n",
            "35/86 [===========>..................] - ETA: 56s - loss: 2.0767 - regression_loss: 1.5608 - classification_loss: 0.5158\n",
            "36/86 [===========>..................] - ETA: 55s - loss: 2.0844 - regression_loss: 1.5664 - classification_loss: 0.5180\n",
            "37/86 [===========>..................] - ETA: 54s - loss: 2.0841 - regression_loss: 1.5670 - classification_loss: 0.5171\n",
            "38/86 [============>.................] - ETA: 52s - loss: 2.0806 - regression_loss: 1.5644 - classification_loss: 0.5162\n",
            "39/86 [============>.................] - ETA: 52s - loss: 2.0841 - regression_loss: 1.5682 - classification_loss: 0.5160\n",
            "40/86 [============>.................] - ETA: 50s - loss: 2.0887 - regression_loss: 1.5708 - classification_loss: 0.5179\n",
            "41/86 [=============>................] - ETA: 49s - loss: 2.0817 - regression_loss: 1.5657 - classification_loss: 0.5160\n",
            "42/86 [=============>................] - ETA: 47s - loss: 2.0825 - regression_loss: 1.5668 - classification_loss: 0.5157\n",
            "43/86 [==============>...............] - ETA: 46s - loss: 2.0804 - regression_loss: 1.5654 - classification_loss: 0.5151\n",
            "44/86 [==============>...............] - ETA: 45s - loss: 2.0783 - regression_loss: 1.5642 - classification_loss: 0.5141\n",
            "45/86 [==============>...............] - ETA: 44s - loss: 2.0786 - regression_loss: 1.5644 - classification_loss: 0.5142\n",
            "46/86 [===============>..............] - ETA: 42s - loss: 2.0828 - regression_loss: 1.5681 - classification_loss: 0.5147\n",
            "47/86 [===============>..............] - ETA: 41s - loss: 2.0828 - regression_loss: 1.5679 - classification_loss: 0.5149\n",
            "48/86 [===============>..............] - ETA: 40s - loss: 2.0853 - regression_loss: 1.5681 - classification_loss: 0.5171\n",
            "49/86 [================>.............] - ETA: 39s - loss: 2.0837 - regression_loss: 1.5671 - classification_loss: 0.5166\n",
            "50/86 [================>.............] - ETA: 38s - loss: 2.0862 - regression_loss: 1.5695 - classification_loss: 0.5166\n",
            "51/86 [================>.............] - ETA: 36s - loss: 2.0907 - regression_loss: 1.5737 - classification_loss: 0.5170\n",
            "52/86 [=================>............] - ETA: 35s - loss: 2.0911 - regression_loss: 1.5742 - classification_loss: 0.5169\n",
            "53/86 [=================>............] - ETA: 34s - loss: 2.0930 - regression_loss: 1.5755 - classification_loss: 0.5175\n",
            "54/86 [=================>............] - ETA: 33s - loss: 2.0900 - regression_loss: 1.5740 - classification_loss: 0.5160\n",
            "55/86 [==================>...........] - ETA: 32s - loss: 2.0880 - regression_loss: 1.5728 - classification_loss: 0.5152\n",
            "56/86 [==================>...........] - ETA: 31s - loss: 2.0894 - regression_loss: 1.5738 - classification_loss: 0.5156\n",
            "57/86 [==================>...........] - ETA: 30s - loss: 2.0878 - regression_loss: 1.5730 - classification_loss: 0.5148\n",
            "58/86 [===================>..........] - ETA: 29s - loss: 2.0892 - regression_loss: 1.5738 - classification_loss: 0.5153\n",
            "59/86 [===================>..........] - ETA: 28s - loss: 2.0897 - regression_loss: 1.5746 - classification_loss: 0.5151\n",
            "60/86 [===================>..........] - ETA: 27s - loss: 2.0871 - regression_loss: 1.5736 - classification_loss: 0.5135\n",
            "61/86 [====================>.........] - ETA: 26s - loss: 2.0919 - regression_loss: 1.5784 - classification_loss: 0.5136\n",
            "62/86 [====================>.........] - ETA: 25s - loss: 2.0918 - regression_loss: 1.5780 - classification_loss: 0.5138\n",
            "63/86 [====================>.........] - ETA: 23s - loss: 2.0891 - regression_loss: 1.5762 - classification_loss: 0.5130\n",
            "64/86 [=====================>........] - ETA: 23s - loss: 2.0893 - regression_loss: 1.5760 - classification_loss: 0.5132\n",
            "65/86 [=====================>........] - ETA: 21s - loss: 2.0879 - regression_loss: 1.5745 - classification_loss: 0.5133\n",
            "66/86 [======================>.......] - ETA: 20s - loss: 2.0839 - regression_loss: 1.5712 - classification_loss: 0.5127\n",
            "67/86 [======================>.......] - ETA: 19s - loss: 2.0836 - regression_loss: 1.5703 - classification_loss: 0.5132\n",
            "68/86 [======================>.......] - ETA: 18s - loss: 2.0830 - regression_loss: 1.5702 - classification_loss: 0.5129\n",
            "69/86 [=======================>......] - ETA: 17s - loss: 2.0846 - regression_loss: 1.5715 - classification_loss: 0.5131\n",
            "70/86 [=======================>......] - ETA: 16s - loss: 2.0831 - regression_loss: 1.5701 - classification_loss: 0.5130\n",
            "71/86 [=======================>......] - ETA: 15s - loss: 2.0844 - regression_loss: 1.5707 - classification_loss: 0.5137\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.0849 - regression_loss: 1.5706 - classification_loss: 0.5143\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.0862 - regression_loss: 1.5713 - classification_loss: 0.5150\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.0880 - regression_loss: 1.5731 - classification_loss: 0.5149\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.0847 - regression_loss: 1.5708 - classification_loss: 0.5139\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.0829 - regression_loss: 1.5697 - classification_loss: 0.5132\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.0813 - regression_loss: 1.5692 - classification_loss: 0.5120 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.0835 - regression_loss: 1.5707 - classification_loss: 0.5129\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.0799 - regression_loss: 1.5683 - classification_loss: 0.5116\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.0827 - regression_loss: 1.5707 - classification_loss: 0.5120\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.0889 - regression_loss: 1.5756 - classification_loss: 0.5132\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.0890 - regression_loss: 1.5769 - classification_loss: 0.5121\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.0898 - regression_loss: 1.5778 - classification_loss: 0.5120\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.0914 - regression_loss: 1.5793 - classification_loss: 0.5121\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.0945 - regression_loss: 1.5811 - classification_loss: 0.5134\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0962 - regression_loss: 1.5833 - classification_loss: 0.5129\n",
            "Epoch 32: saving model to ./snapshots\\resnet50_csv_32.h5\n",
            "\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
            "\n",
            "86/86 [==============================] - 89s 1s/step - loss: 2.0962 - regression_loss: 1.5833 - classification_loss: 0.5129 - lr: 1.0000e-12\n",
            "Epoch 33/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:06 - loss: 1.9812 - regression_loss: 1.4737 - classification_loss: 0.5075\n",
            " 2/86 [..............................] - ETA: 1:39 - loss: 2.0590 - regression_loss: 1.5503 - classification_loss: 0.5087\n",
            " 3/86 [>.............................] - ETA: 1:21 - loss: 2.0978 - regression_loss: 1.5950 - classification_loss: 0.5028\n",
            " 4/86 [>.............................] - ETA: 1:28 - loss: 2.1262 - regression_loss: 1.6235 - classification_loss: 0.5027\n",
            " 5/86 [>.............................] - ETA: 1:21 - loss: 2.1643 - regression_loss: 1.6457 - classification_loss: 0.5187\n",
            " 6/86 [=>............................] - ETA: 1:19 - loss: 2.1571 - regression_loss: 1.6398 - classification_loss: 0.5173\n",
            " 7/86 [=>............................] - ETA: 1:17 - loss: 2.1475 - regression_loss: 1.6322 - classification_loss: 0.5153\n",
            " 8/86 [=>............................] - ETA: 1:18 - loss: 2.1279 - regression_loss: 1.6173 - classification_loss: 0.5106\n",
            " 9/86 [==>...........................] - ETA: 1:24 - loss: 2.1291 - regression_loss: 1.6164 - classification_loss: 0.5127\n",
            "10/86 [==>...........................] - ETA: 1:31 - loss: 2.1746 - regression_loss: 1.6419 - classification_loss: 0.5327\n",
            "11/86 [==>...........................] - ETA: 1:36 - loss: 2.1560 - regression_loss: 1.6233 - classification_loss: 0.5327\n",
            "12/86 [===>..........................] - ETA: 1:42 - loss: 2.1614 - regression_loss: 1.6226 - classification_loss: 0.5388\n",
            "13/86 [===>..........................] - ETA: 1:41 - loss: 2.1712 - regression_loss: 1.6302 - classification_loss: 0.5410\n",
            "14/86 [===>..........................] - ETA: 1:39 - loss: 2.1566 - regression_loss: 1.6259 - classification_loss: 0.5307\n",
            "15/86 [====>.........................] - ETA: 1:34 - loss: 2.1527 - regression_loss: 1.6247 - classification_loss: 0.5280\n",
            "16/86 [====>.........................] - ETA: 1:31 - loss: 2.1396 - regression_loss: 1.6161 - classification_loss: 0.5235\n",
            "17/86 [====>.........................] - ETA: 1:28 - loss: 2.1648 - regression_loss: 1.6375 - classification_loss: 0.5273\n",
            "18/86 [=====>........................] - ETA: 1:25 - loss: 2.1625 - regression_loss: 1.6361 - classification_loss: 0.5264\n",
            "19/86 [=====>........................] - ETA: 1:22 - loss: 2.1735 - regression_loss: 1.6401 - classification_loss: 0.5334\n",
            "20/86 [=====>........................] - ETA: 1:19 - loss: 2.1563 - regression_loss: 1.6252 - classification_loss: 0.5311\n",
            "21/86 [======>.......................] - ETA: 1:17 - loss: 2.1555 - regression_loss: 1.6242 - classification_loss: 0.5313\n",
            "22/86 [======>.......................] - ETA: 1:15 - loss: 2.1391 - regression_loss: 1.6138 - classification_loss: 0.5253\n",
            "23/86 [=======>......................] - ETA: 1:13 - loss: 2.1253 - regression_loss: 1.6029 - classification_loss: 0.5223\n",
            "24/86 [=======>......................] - ETA: 1:11 - loss: 2.1251 - regression_loss: 1.6027 - classification_loss: 0.5224\n",
            "25/86 [=======>......................] - ETA: 1:09 - loss: 2.1122 - regression_loss: 1.5928 - classification_loss: 0.5195\n",
            "26/86 [========>.....................] - ETA: 1:08 - loss: 2.1202 - regression_loss: 1.5998 - classification_loss: 0.5203\n",
            "27/86 [========>.....................] - ETA: 1:06 - loss: 2.1264 - regression_loss: 1.6016 - classification_loss: 0.5248\n",
            "28/86 [========>.....................] - ETA: 1:06 - loss: 2.1253 - regression_loss: 1.6011 - classification_loss: 0.5242\n",
            "29/86 [=========>....................] - ETA: 1:04 - loss: 2.1202 - regression_loss: 1.5988 - classification_loss: 0.5214\n",
            "30/86 [=========>....................] - ETA: 1:03 - loss: 2.1160 - regression_loss: 1.5954 - classification_loss: 0.5206\n",
            "31/86 [=========>....................] - ETA: 1:02 - loss: 2.1173 - regression_loss: 1.5970 - classification_loss: 0.5203\n",
            "32/86 [==========>...................] - ETA: 1:00 - loss: 2.1235 - regression_loss: 1.6031 - classification_loss: 0.5203\n",
            "33/86 [==========>...................] - ETA: 59s - loss: 2.1223 - regression_loss: 1.6019 - classification_loss: 0.5204 \n",
            "34/86 [==========>...................] - ETA: 57s - loss: 2.1228 - regression_loss: 1.6033 - classification_loss: 0.5196\n",
            "35/86 [===========>..................] - ETA: 56s - loss: 2.1172 - regression_loss: 1.5989 - classification_loss: 0.5183\n",
            "36/86 [===========>..................] - ETA: 55s - loss: 2.1280 - regression_loss: 1.6046 - classification_loss: 0.5234\n",
            "37/86 [===========>..................] - ETA: 53s - loss: 2.1215 - regression_loss: 1.5985 - classification_loss: 0.5230\n",
            "38/86 [============>.................] - ETA: 52s - loss: 2.1247 - regression_loss: 1.6006 - classification_loss: 0.5241\n",
            "39/86 [============>.................] - ETA: 51s - loss: 2.1228 - regression_loss: 1.6004 - classification_loss: 0.5225\n",
            "40/86 [============>.................] - ETA: 49s - loss: 2.1212 - regression_loss: 1.5991 - classification_loss: 0.5221\n",
            "41/86 [=============>................] - ETA: 48s - loss: 2.1182 - regression_loss: 1.5963 - classification_loss: 0.5219\n",
            "42/86 [=============>................] - ETA: 47s - loss: 2.1178 - regression_loss: 1.5958 - classification_loss: 0.5220\n",
            "43/86 [==============>...............] - ETA: 46s - loss: 2.1079 - regression_loss: 1.5891 - classification_loss: 0.5188\n",
            "44/86 [==============>...............] - ETA: 45s - loss: 2.1148 - regression_loss: 1.5959 - classification_loss: 0.5189\n",
            "45/86 [==============>...............] - ETA: 43s - loss: 2.1254 - regression_loss: 1.6045 - classification_loss: 0.5209\n",
            "46/86 [===============>..............] - ETA: 42s - loss: 2.1263 - regression_loss: 1.6047 - classification_loss: 0.5216\n",
            "47/86 [===============>..............] - ETA: 41s - loss: 2.1231 - regression_loss: 1.6024 - classification_loss: 0.5207\n",
            "48/86 [===============>..............] - ETA: 40s - loss: 2.1300 - regression_loss: 1.6077 - classification_loss: 0.5224\n",
            "49/86 [================>.............] - ETA: 39s - loss: 2.1271 - regression_loss: 1.6064 - classification_loss: 0.5207\n",
            "50/86 [================>.............] - ETA: 38s - loss: 2.1256 - regression_loss: 1.6052 - classification_loss: 0.5204\n",
            "51/86 [================>.............] - ETA: 36s - loss: 2.1229 - regression_loss: 1.6038 - classification_loss: 0.5191\n",
            "52/86 [=================>............] - ETA: 36s - loss: 2.1223 - regression_loss: 1.6046 - classification_loss: 0.5178\n",
            "53/86 [=================>............] - ETA: 34s - loss: 2.1169 - regression_loss: 1.6002 - classification_loss: 0.5167\n",
            "54/86 [=================>............] - ETA: 33s - loss: 2.1177 - regression_loss: 1.6011 - classification_loss: 0.5166\n",
            "55/86 [==================>...........] - ETA: 32s - loss: 2.1151 - regression_loss: 1.5989 - classification_loss: 0.5162\n",
            "56/86 [==================>...........] - ETA: 31s - loss: 2.1100 - regression_loss: 1.5945 - classification_loss: 0.5155\n",
            "57/86 [==================>...........] - ETA: 30s - loss: 2.1006 - regression_loss: 1.5874 - classification_loss: 0.5132\n",
            "58/86 [===================>..........] - ETA: 29s - loss: 2.0995 - regression_loss: 1.5828 - classification_loss: 0.5167\n",
            "59/86 [===================>..........] - ETA: 28s - loss: 2.0982 - regression_loss: 1.5816 - classification_loss: 0.5166\n",
            "60/86 [===================>..........] - ETA: 26s - loss: 2.1021 - regression_loss: 1.5845 - classification_loss: 0.5176\n",
            "61/86 [====================>.........] - ETA: 25s - loss: 2.1020 - regression_loss: 1.5840 - classification_loss: 0.5181\n",
            "62/86 [====================>.........] - ETA: 24s - loss: 2.1020 - regression_loss: 1.5834 - classification_loss: 0.5186\n",
            "63/86 [====================>.........] - ETA: 23s - loss: 2.1004 - regression_loss: 1.5832 - classification_loss: 0.5172\n",
            "64/86 [=====================>........] - ETA: 22s - loss: 2.0998 - regression_loss: 1.5823 - classification_loss: 0.5175\n",
            "65/86 [=====================>........] - ETA: 21s - loss: 2.0990 - regression_loss: 1.5823 - classification_loss: 0.5166\n",
            "66/86 [======================>.......] - ETA: 20s - loss: 2.0954 - regression_loss: 1.5796 - classification_loss: 0.5158\n",
            "67/86 [======================>.......] - ETA: 19s - loss: 2.0891 - regression_loss: 1.5750 - classification_loss: 0.5141\n",
            "68/86 [======================>.......] - ETA: 18s - loss: 2.0877 - regression_loss: 1.5742 - classification_loss: 0.5136\n",
            "69/86 [=======================>......] - ETA: 17s - loss: 2.0864 - regression_loss: 1.5735 - classification_loss: 0.5129\n",
            "70/86 [=======================>......] - ETA: 16s - loss: 2.0872 - regression_loss: 1.5740 - classification_loss: 0.5131\n",
            "71/86 [=======================>......] - ETA: 15s - loss: 2.0848 - regression_loss: 1.5721 - classification_loss: 0.5127\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.0850 - regression_loss: 1.5721 - classification_loss: 0.5129\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.0860 - regression_loss: 1.5729 - classification_loss: 0.5131\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.0862 - regression_loss: 1.5738 - classification_loss: 0.5124\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.0887 - regression_loss: 1.5761 - classification_loss: 0.5126\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.0887 - regression_loss: 1.5756 - classification_loss: 0.5131\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.0872 - regression_loss: 1.5747 - classification_loss: 0.5124 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.0874 - regression_loss: 1.5750 - classification_loss: 0.5124\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.0894 - regression_loss: 1.5767 - classification_loss: 0.5127\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.0896 - regression_loss: 1.5769 - classification_loss: 0.5127\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.0899 - regression_loss: 1.5773 - classification_loss: 0.5126\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.0922 - regression_loss: 1.5798 - classification_loss: 0.5123\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.0902 - regression_loss: 1.5784 - classification_loss: 0.5119\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.0862 - regression_loss: 1.5759 - classification_loss: 0.5104\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.0866 - regression_loss: 1.5764 - classification_loss: 0.5102\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0898 - regression_loss: 1.5777 - classification_loss: 0.5120\n",
            "Epoch 33: saving model to ./snapshots\\resnet50_csv_33.h5\n",
            "\n",
            "86/86 [==============================] - 88s 1s/step - loss: 2.0898 - regression_loss: 1.5777 - classification_loss: 0.5120 - lr: 1.0000e-13\n",
            "Epoch 34/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:05 - loss: 1.9426 - regression_loss: 1.4331 - classification_loss: 0.5095\n",
            " 2/86 [..............................] - ETA: 1:05 - loss: 1.9814 - regression_loss: 1.4982 - classification_loss: 0.4832\n",
            " 3/86 [>.............................] - ETA: 1:10 - loss: 2.1162 - regression_loss: 1.6021 - classification_loss: 0.5141\n",
            " 4/86 [>.............................] - ETA: 1:12 - loss: 2.1509 - regression_loss: 1.6132 - classification_loss: 0.5377\n",
            " 5/86 [>.............................] - ETA: 1:19 - loss: 2.1906 - regression_loss: 1.6456 - classification_loss: 0.5450\n",
            " 6/86 [=>............................] - ETA: 1:16 - loss: 2.1688 - regression_loss: 1.6341 - classification_loss: 0.5347\n",
            " 7/86 [=>............................] - ETA: 1:15 - loss: 2.1707 - regression_loss: 1.6395 - classification_loss: 0.5312\n",
            " 8/86 [=>............................] - ETA: 1:13 - loss: 2.1807 - regression_loss: 1.6458 - classification_loss: 0.5350\n",
            " 9/86 [==>...........................] - ETA: 1:12 - loss: 2.2163 - regression_loss: 1.6676 - classification_loss: 0.5487\n",
            "10/86 [==>...........................] - ETA: 1:12 - loss: 2.2181 - regression_loss: 1.6644 - classification_loss: 0.5538\n",
            "11/86 [==>...........................] - ETA: 1:21 - loss: 2.2181 - regression_loss: 1.6587 - classification_loss: 0.5594\n",
            "12/86 [===>..........................] - ETA: 1:25 - loss: 2.2137 - regression_loss: 1.6637 - classification_loss: 0.5501\n",
            "13/86 [===>..........................] - ETA: 1:27 - loss: 2.2039 - regression_loss: 1.6581 - classification_loss: 0.5458\n",
            "14/86 [===>..........................] - ETA: 1:29 - loss: 2.1882 - regression_loss: 1.6492 - classification_loss: 0.5390\n",
            "15/86 [====>.........................] - ETA: 1:30 - loss: 2.1880 - regression_loss: 1.6501 - classification_loss: 0.5379\n",
            "16/86 [====>.........................] - ETA: 1:27 - loss: 2.1876 - regression_loss: 1.6502 - classification_loss: 0.5374\n",
            "17/86 [====>.........................] - ETA: 1:24 - loss: 2.1913 - regression_loss: 1.6529 - classification_loss: 0.5384\n",
            "18/86 [=====>........................] - ETA: 1:23 - loss: 2.1968 - regression_loss: 1.6580 - classification_loss: 0.5388\n",
            "19/86 [=====>........................] - ETA: 1:21 - loss: 2.1894 - regression_loss: 1.6528 - classification_loss: 0.5366\n",
            "20/86 [=====>........................] - ETA: 1:18 - loss: 2.1728 - regression_loss: 1.6407 - classification_loss: 0.5321\n",
            "21/86 [======>.......................] - ETA: 1:16 - loss: 2.1708 - regression_loss: 1.6422 - classification_loss: 0.5286\n",
            "22/86 [======>.......................] - ETA: 1:14 - loss: 2.1670 - regression_loss: 1.6392 - classification_loss: 0.5278\n",
            "23/86 [=======>......................] - ETA: 1:12 - loss: 2.1523 - regression_loss: 1.6283 - classification_loss: 0.5240\n",
            "24/86 [=======>......................] - ETA: 1:11 - loss: 2.1442 - regression_loss: 1.6206 - classification_loss: 0.5236\n",
            "25/86 [=======>......................] - ETA: 1:09 - loss: 2.1339 - regression_loss: 1.6115 - classification_loss: 0.5224\n",
            "26/86 [========>.....................] - ETA: 1:08 - loss: 2.1243 - regression_loss: 1.6053 - classification_loss: 0.5190\n",
            "27/86 [========>.....................] - ETA: 1:06 - loss: 2.1159 - regression_loss: 1.5983 - classification_loss: 0.5176\n",
            "28/86 [========>.....................] - ETA: 1:04 - loss: 2.1181 - regression_loss: 1.5996 - classification_loss: 0.5185\n",
            "29/86 [=========>....................] - ETA: 1:03 - loss: 2.1224 - regression_loss: 1.6019 - classification_loss: 0.5206\n",
            "30/86 [=========>....................] - ETA: 1:01 - loss: 2.1161 - regression_loss: 1.5963 - classification_loss: 0.5199\n",
            "31/86 [=========>....................] - ETA: 1:00 - loss: 2.1229 - regression_loss: 1.5984 - classification_loss: 0.5244\n",
            "32/86 [==========>...................] - ETA: 58s - loss: 2.1145 - regression_loss: 1.5924 - classification_loss: 0.5222 \n",
            "33/86 [==========>...................] - ETA: 57s - loss: 2.1107 - regression_loss: 1.5897 - classification_loss: 0.5210\n",
            "34/86 [==========>...................] - ETA: 56s - loss: 2.1151 - regression_loss: 1.5926 - classification_loss: 0.5225\n",
            "35/86 [===========>..................] - ETA: 54s - loss: 2.1116 - regression_loss: 1.5898 - classification_loss: 0.5218\n",
            "36/86 [===========>..................] - ETA: 53s - loss: 2.1049 - regression_loss: 1.5857 - classification_loss: 0.5191\n",
            "37/86 [===========>..................] - ETA: 52s - loss: 2.1042 - regression_loss: 1.5858 - classification_loss: 0.5184\n",
            "38/86 [============>.................] - ETA: 50s - loss: 2.1034 - regression_loss: 1.5850 - classification_loss: 0.5184\n",
            "39/86 [============>.................] - ETA: 49s - loss: 2.1084 - regression_loss: 1.5899 - classification_loss: 0.5185\n",
            "40/86 [============>.................] - ETA: 48s - loss: 2.1040 - regression_loss: 1.5863 - classification_loss: 0.5177\n",
            "41/86 [=============>................] - ETA: 47s - loss: 2.1014 - regression_loss: 1.5834 - classification_loss: 0.5181\n",
            "42/86 [=============>................] - ETA: 46s - loss: 2.0956 - regression_loss: 1.5798 - classification_loss: 0.5158\n",
            "43/86 [==============>...............] - ETA: 44s - loss: 2.1008 - regression_loss: 1.5838 - classification_loss: 0.5170\n",
            "44/86 [==============>...............] - ETA: 43s - loss: 2.1056 - regression_loss: 1.5882 - classification_loss: 0.5174\n",
            "45/86 [==============>...............] - ETA: 42s - loss: 2.1081 - regression_loss: 1.5901 - classification_loss: 0.5180\n",
            "46/86 [===============>..............] - ETA: 41s - loss: 2.1141 - regression_loss: 1.5935 - classification_loss: 0.5206\n",
            "47/86 [===============>..............] - ETA: 40s - loss: 2.1138 - regression_loss: 1.5890 - classification_loss: 0.5248\n",
            "48/86 [===============>..............] - ETA: 39s - loss: 2.1024 - regression_loss: 1.5807 - classification_loss: 0.5217\n",
            "49/86 [================>.............] - ETA: 38s - loss: 2.0978 - regression_loss: 1.5765 - classification_loss: 0.5213\n",
            "50/86 [================>.............] - ETA: 37s - loss: 2.0963 - regression_loss: 1.5764 - classification_loss: 0.5199\n",
            "51/86 [================>.............] - ETA: 36s - loss: 2.0961 - regression_loss: 1.5755 - classification_loss: 0.5206\n",
            "52/86 [=================>............] - ETA: 34s - loss: 2.0959 - regression_loss: 1.5752 - classification_loss: 0.5208\n",
            "53/86 [=================>............] - ETA: 33s - loss: 2.0957 - regression_loss: 1.5764 - classification_loss: 0.5193\n",
            "54/86 [=================>............] - ETA: 32s - loss: 2.0919 - regression_loss: 1.5747 - classification_loss: 0.5171\n",
            "55/86 [==================>...........] - ETA: 31s - loss: 2.0999 - regression_loss: 1.5814 - classification_loss: 0.5185\n",
            "56/86 [==================>...........] - ETA: 30s - loss: 2.0985 - regression_loss: 1.5800 - classification_loss: 0.5185\n",
            "57/86 [==================>...........] - ETA: 29s - loss: 2.1006 - regression_loss: 1.5813 - classification_loss: 0.5193\n",
            "58/86 [===================>..........] - ETA: 28s - loss: 2.0923 - regression_loss: 1.5756 - classification_loss: 0.5167\n",
            "59/86 [===================>..........] - ETA: 28s - loss: 2.0922 - regression_loss: 1.5748 - classification_loss: 0.5174\n",
            "60/86 [===================>..........] - ETA: 27s - loss: 2.0894 - regression_loss: 1.5732 - classification_loss: 0.5163\n",
            "61/86 [====================>.........] - ETA: 26s - loss: 2.0903 - regression_loss: 1.5731 - classification_loss: 0.5172\n",
            "62/86 [====================>.........] - ETA: 25s - loss: 2.0867 - regression_loss: 1.5701 - classification_loss: 0.5166\n",
            "63/86 [====================>.........] - ETA: 24s - loss: 2.0889 - regression_loss: 1.5705 - classification_loss: 0.5184\n",
            "64/86 [=====================>........] - ETA: 23s - loss: 2.0864 - regression_loss: 1.5689 - classification_loss: 0.5175\n",
            "65/86 [=====================>........] - ETA: 22s - loss: 2.0915 - regression_loss: 1.5726 - classification_loss: 0.5189\n",
            "66/86 [======================>.......] - ETA: 21s - loss: 2.0920 - regression_loss: 1.5736 - classification_loss: 0.5184\n",
            "67/86 [======================>.......] - ETA: 20s - loss: 2.0925 - regression_loss: 1.5739 - classification_loss: 0.5186\n",
            "68/86 [======================>.......] - ETA: 19s - loss: 2.0961 - regression_loss: 1.5767 - classification_loss: 0.5194\n",
            "69/86 [=======================>......] - ETA: 18s - loss: 2.0991 - regression_loss: 1.5792 - classification_loss: 0.5199\n",
            "70/86 [=======================>......] - ETA: 17s - loss: 2.1023 - regression_loss: 1.5810 - classification_loss: 0.5213\n",
            "71/86 [=======================>......] - ETA: 16s - loss: 2.1064 - regression_loss: 1.5846 - classification_loss: 0.5218\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.1077 - regression_loss: 1.5857 - classification_loss: 0.5220\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.1079 - regression_loss: 1.5859 - classification_loss: 0.5220\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.1048 - regression_loss: 1.5839 - classification_loss: 0.5209\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.1064 - regression_loss: 1.5844 - classification_loss: 0.5220\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.1041 - regression_loss: 1.5825 - classification_loss: 0.5217\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.1060 - regression_loss: 1.5846 - classification_loss: 0.5213 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.1096 - regression_loss: 1.5883 - classification_loss: 0.5213\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.1108 - regression_loss: 1.5891 - classification_loss: 0.5217\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.1140 - regression_loss: 1.5919 - classification_loss: 0.5222\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.1124 - regression_loss: 1.5912 - classification_loss: 0.5211\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.1133 - regression_loss: 1.5919 - classification_loss: 0.5214\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.1118 - regression_loss: 1.5913 - classification_loss: 0.5205\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.1120 - regression_loss: 1.5915 - classification_loss: 0.5205\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.1083 - regression_loss: 1.5891 - classification_loss: 0.5192\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1136 - regression_loss: 1.5935 - classification_loss: 0.5201\n",
            "Epoch 34: saving model to ./snapshots\\resnet50_csv_34.h5\n",
            "\n",
            "Epoch 34: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
            "\n",
            "86/86 [==============================] - 92s 1s/step - loss: 2.1136 - regression_loss: 1.5935 - classification_loss: 0.5201 - lr: 1.0000e-13\n",
            "Epoch 35/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:10 - loss: 2.1598 - regression_loss: 1.6323 - classification_loss: 0.5275\n",
            " 2/86 [..............................] - ETA: 1:31 - loss: 2.2448 - regression_loss: 1.6881 - classification_loss: 0.5568\n",
            " 3/86 [>.............................] - ETA: 1:20 - loss: 2.0214 - regression_loss: 1.5251 - classification_loss: 0.4963\n",
            " 4/86 [>.............................] - ETA: 1:16 - loss: 2.0966 - regression_loss: 1.5800 - classification_loss: 0.5166\n",
            " 5/86 [>.............................] - ETA: 1:16 - loss: 2.0739 - regression_loss: 1.5727 - classification_loss: 0.5012\n",
            " 6/86 [=>............................] - ETA: 1:17 - loss: 2.1336 - regression_loss: 1.5986 - classification_loss: 0.5350\n",
            " 7/86 [=>............................] - ETA: 1:15 - loss: 2.1444 - regression_loss: 1.6148 - classification_loss: 0.5296\n",
            " 8/86 [=>............................] - ETA: 1:15 - loss: 2.1512 - regression_loss: 1.6197 - classification_loss: 0.5315\n",
            " 9/86 [==>...........................] - ETA: 1:14 - loss: 2.1282 - regression_loss: 1.5997 - classification_loss: 0.5285\n",
            "10/86 [==>...........................] - ETA: 1:13 - loss: 2.1318 - regression_loss: 1.6044 - classification_loss: 0.5274\n",
            "11/86 [==>...........................] - ETA: 1:11 - loss: 2.1116 - regression_loss: 1.5879 - classification_loss: 0.5237\n",
            "12/86 [===>..........................] - ETA: 1:10 - loss: 2.1070 - regression_loss: 1.5825 - classification_loss: 0.5245\n",
            "13/86 [===>..........................] - ETA: 1:09 - loss: 2.1118 - regression_loss: 1.5904 - classification_loss: 0.5214\n",
            "14/86 [===>..........................] - ETA: 1:08 - loss: 2.0938 - regression_loss: 1.5732 - classification_loss: 0.5206\n",
            "15/86 [====>.........................] - ETA: 1:08 - loss: 2.0960 - regression_loss: 1.5775 - classification_loss: 0.5185\n",
            "16/86 [====>.........................] - ETA: 1:07 - loss: 2.1211 - regression_loss: 1.5957 - classification_loss: 0.5254\n",
            "17/86 [====>.........................] - ETA: 1:07 - loss: 2.1209 - regression_loss: 1.5991 - classification_loss: 0.5218\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.1035 - regression_loss: 1.5891 - classification_loss: 0.5145\n",
            "19/86 [=====>........................] - ETA: 1:04 - loss: 2.1044 - regression_loss: 1.5880 - classification_loss: 0.5165\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.0949 - regression_loss: 1.5841 - classification_loss: 0.5108\n",
            "21/86 [======>.......................] - ETA: 1:04 - loss: 2.0921 - regression_loss: 1.5842 - classification_loss: 0.5080\n",
            "22/86 [======>.......................] - ETA: 1:03 - loss: 2.0913 - regression_loss: 1.5837 - classification_loss: 0.5076\n",
            "23/86 [=======>......................] - ETA: 1:02 - loss: 2.0907 - regression_loss: 1.5829 - classification_loss: 0.5078\n",
            "24/86 [=======>......................] - ETA: 1:04 - loss: 2.0934 - regression_loss: 1.5856 - classification_loss: 0.5078\n",
            "25/86 [=======>......................] - ETA: 1:07 - loss: 2.0899 - regression_loss: 1.5833 - classification_loss: 0.5067\n",
            "26/86 [========>.....................] - ETA: 1:08 - loss: 2.0879 - regression_loss: 1.5804 - classification_loss: 0.5075\n",
            "27/86 [========>.....................] - ETA: 1:07 - loss: 2.0840 - regression_loss: 1.5788 - classification_loss: 0.5052\n",
            "28/86 [========>.....................] - ETA: 1:05 - loss: 2.0819 - regression_loss: 1.5777 - classification_loss: 0.5042\n",
            "29/86 [=========>....................] - ETA: 1:05 - loss: 2.0904 - regression_loss: 1.5815 - classification_loss: 0.5089\n",
            "30/86 [=========>....................] - ETA: 1:03 - loss: 2.0844 - regression_loss: 1.5774 - classification_loss: 0.5070\n",
            "31/86 [=========>....................] - ETA: 1:01 - loss: 2.0751 - regression_loss: 1.5710 - classification_loss: 0.5041\n",
            "32/86 [==========>...................] - ETA: 1:00 - loss: 2.0753 - regression_loss: 1.5713 - classification_loss: 0.5040\n",
            "33/86 [==========>...................] - ETA: 1:00 - loss: 2.0834 - regression_loss: 1.5792 - classification_loss: 0.5042\n",
            "34/86 [==========>...................] - ETA: 59s - loss: 2.0923 - regression_loss: 1.5837 - classification_loss: 0.5086 \n",
            "35/86 [===========>..................] - ETA: 59s - loss: 2.0977 - regression_loss: 1.5871 - classification_loss: 0.5106\n",
            "36/86 [===========>..................] - ETA: 59s - loss: 2.1048 - regression_loss: 1.5937 - classification_loss: 0.5111\n",
            "37/86 [===========>..................] - ETA: 59s - loss: 2.1026 - regression_loss: 1.5918 - classification_loss: 0.5108\n",
            "38/86 [============>.................] - ETA: 58s - loss: 2.1070 - regression_loss: 1.5945 - classification_loss: 0.5125\n",
            "39/86 [============>.................] - ETA: 56s - loss: 2.1101 - regression_loss: 1.5977 - classification_loss: 0.5124\n",
            "40/86 [============>.................] - ETA: 55s - loss: 2.1130 - regression_loss: 1.5992 - classification_loss: 0.5137\n",
            "41/86 [=============>................] - ETA: 53s - loss: 2.1173 - regression_loss: 1.6031 - classification_loss: 0.5142\n",
            "42/86 [=============>................] - ETA: 52s - loss: 2.1204 - regression_loss: 1.6059 - classification_loss: 0.5145\n",
            "43/86 [==============>...............] - ETA: 50s - loss: 2.1234 - regression_loss: 1.6093 - classification_loss: 0.5141\n",
            "44/86 [==============>...............] - ETA: 49s - loss: 2.1303 - regression_loss: 1.6144 - classification_loss: 0.5159\n",
            "45/86 [==============>...............] - ETA: 48s - loss: 2.1280 - regression_loss: 1.6124 - classification_loss: 0.5155\n",
            "46/86 [===============>..............] - ETA: 46s - loss: 2.1263 - regression_loss: 1.6064 - classification_loss: 0.5199\n",
            "47/86 [===============>..............] - ETA: 45s - loss: 2.1294 - regression_loss: 1.6082 - classification_loss: 0.5212\n",
            "48/86 [===============>..............] - ETA: 43s - loss: 2.1357 - regression_loss: 1.6129 - classification_loss: 0.5228\n",
            "49/86 [================>.............] - ETA: 42s - loss: 2.1243 - regression_loss: 1.6045 - classification_loss: 0.5197\n",
            "50/86 [================>.............] - ETA: 41s - loss: 2.1322 - regression_loss: 1.6102 - classification_loss: 0.5221\n",
            "51/86 [================>.............] - ETA: 39s - loss: 2.1355 - regression_loss: 1.6124 - classification_loss: 0.5231\n",
            "52/86 [=================>............] - ETA: 38s - loss: 2.1351 - regression_loss: 1.6115 - classification_loss: 0.5236\n",
            "53/86 [=================>............] - ETA: 37s - loss: 2.1307 - regression_loss: 1.6081 - classification_loss: 0.5225\n",
            "54/86 [=================>............] - ETA: 36s - loss: 2.1197 - regression_loss: 1.5998 - classification_loss: 0.5199\n",
            "55/86 [==================>...........] - ETA: 34s - loss: 2.1202 - regression_loss: 1.6005 - classification_loss: 0.5197\n",
            "56/86 [==================>...........] - ETA: 33s - loss: 2.1146 - regression_loss: 1.5961 - classification_loss: 0.5185\n",
            "57/86 [==================>...........] - ETA: 32s - loss: 2.1190 - regression_loss: 1.6012 - classification_loss: 0.5178\n",
            "58/86 [===================>..........] - ETA: 31s - loss: 2.1200 - regression_loss: 1.6020 - classification_loss: 0.5180\n",
            "59/86 [===================>..........] - ETA: 29s - loss: 2.1197 - regression_loss: 1.6002 - classification_loss: 0.5195\n",
            "60/86 [===================>..........] - ETA: 28s - loss: 2.1145 - regression_loss: 1.5966 - classification_loss: 0.5178\n",
            "61/86 [====================>.........] - ETA: 27s - loss: 2.1140 - regression_loss: 1.5957 - classification_loss: 0.5182\n",
            "62/86 [====================>.........] - ETA: 26s - loss: 2.1168 - regression_loss: 1.5978 - classification_loss: 0.5191\n",
            "63/86 [====================>.........] - ETA: 25s - loss: 2.1123 - regression_loss: 1.5931 - classification_loss: 0.5192\n",
            "64/86 [=====================>........] - ETA: 24s - loss: 2.1139 - regression_loss: 1.5934 - classification_loss: 0.5205\n",
            "65/86 [=====================>........] - ETA: 23s - loss: 2.1184 - regression_loss: 1.5978 - classification_loss: 0.5206\n",
            "66/86 [======================>.......] - ETA: 21s - loss: 2.1171 - regression_loss: 1.5966 - classification_loss: 0.5205\n",
            "67/86 [======================>.......] - ETA: 20s - loss: 2.1123 - regression_loss: 1.5928 - classification_loss: 0.5195\n",
            "68/86 [======================>.......] - ETA: 19s - loss: 2.1123 - regression_loss: 1.5923 - classification_loss: 0.5199\n",
            "69/86 [=======================>......] - ETA: 18s - loss: 2.1117 - regression_loss: 1.5908 - classification_loss: 0.5208\n",
            "70/86 [=======================>......] - ETA: 17s - loss: 2.1161 - regression_loss: 1.5941 - classification_loss: 0.5220\n",
            "71/86 [=======================>......] - ETA: 16s - loss: 2.1187 - regression_loss: 1.5958 - classification_loss: 0.5229\n",
            "72/86 [========================>.....] - ETA: 15s - loss: 2.1195 - regression_loss: 1.5977 - classification_loss: 0.5217\n",
            "73/86 [========================>.....] - ETA: 14s - loss: 2.1176 - regression_loss: 1.5969 - classification_loss: 0.5207\n",
            "74/86 [========================>.....] - ETA: 13s - loss: 2.1173 - regression_loss: 1.5959 - classification_loss: 0.5214\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.1225 - regression_loss: 1.6003 - classification_loss: 0.5222\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.1292 - regression_loss: 1.6059 - classification_loss: 0.5232\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.1290 - regression_loss: 1.6063 - classification_loss: 0.5228 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.1273 - regression_loss: 1.6048 - classification_loss: 0.5225\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.1294 - regression_loss: 1.6053 - classification_loss: 0.5241\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.1316 - regression_loss: 1.6070 - classification_loss: 0.5246\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.1312 - regression_loss: 1.6066 - classification_loss: 0.5246\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.1277 - regression_loss: 1.6036 - classification_loss: 0.5242\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.1287 - regression_loss: 1.6049 - classification_loss: 0.5239\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.1279 - regression_loss: 1.6049 - classification_loss: 0.5230\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.1266 - regression_loss: 1.6034 - classification_loss: 0.5232\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1276 - regression_loss: 1.6030 - classification_loss: 0.5246\n",
            "Epoch 35: saving model to ./snapshots\\resnet50_csv_35.h5\n",
            "\n",
            "86/86 [==============================] - 94s 1s/step - loss: 2.1276 - regression_loss: 1.6030 - classification_loss: 0.5246 - lr: 1.0000e-14\n",
            "Epoch 36/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:52 - loss: 2.0056 - regression_loss: 1.4592 - classification_loss: 0.5464\n",
            " 2/86 [..............................] - ETA: 1:33 - loss: 2.2322 - regression_loss: 1.6407 - classification_loss: 0.5915\n",
            " 3/86 [>.............................] - ETA: 1:35 - loss: 2.2642 - regression_loss: 1.7084 - classification_loss: 0.5557\n",
            " 4/86 [>.............................] - ETA: 1:26 - loss: 2.2210 - regression_loss: 1.6702 - classification_loss: 0.5507\n",
            " 5/86 [>.............................] - ETA: 1:25 - loss: 2.2668 - regression_loss: 1.6835 - classification_loss: 0.5833\n",
            " 6/86 [=>............................] - ETA: 1:21 - loss: 2.1964 - regression_loss: 1.6283 - classification_loss: 0.5681\n",
            " 7/86 [=>............................] - ETA: 1:19 - loss: 2.2010 - regression_loss: 1.6413 - classification_loss: 0.5596\n",
            " 8/86 [=>............................] - ETA: 1:18 - loss: 2.2252 - regression_loss: 1.6597 - classification_loss: 0.5655\n",
            " 9/86 [==>...........................] - ETA: 1:20 - loss: 2.2094 - regression_loss: 1.6474 - classification_loss: 0.5621\n",
            "10/86 [==>...........................] - ETA: 1:18 - loss: 2.2184 - regression_loss: 1.6664 - classification_loss: 0.5520\n",
            "11/86 [==>...........................] - ETA: 1:16 - loss: 2.2110 - regression_loss: 1.6595 - classification_loss: 0.5515\n",
            "12/86 [===>..........................] - ETA: 1:14 - loss: 2.1789 - regression_loss: 1.6325 - classification_loss: 0.5464\n",
            "13/86 [===>..........................] - ETA: 1:13 - loss: 2.1714 - regression_loss: 1.6285 - classification_loss: 0.5429\n",
            "14/86 [===>..........................] - ETA: 1:13 - loss: 2.1534 - regression_loss: 1.6173 - classification_loss: 0.5361\n",
            "15/86 [====>.........................] - ETA: 1:11 - loss: 2.1554 - regression_loss: 1.6220 - classification_loss: 0.5334\n",
            "16/86 [====>.........................] - ETA: 1:13 - loss: 2.1451 - regression_loss: 1.6103 - classification_loss: 0.5348\n",
            "17/86 [====>.........................] - ETA: 1:15 - loss: 2.1396 - regression_loss: 1.6062 - classification_loss: 0.5334\n",
            "18/86 [=====>........................] - ETA: 1:19 - loss: 2.1513 - regression_loss: 1.6146 - classification_loss: 0.5366\n",
            "19/86 [=====>........................] - ETA: 1:18 - loss: 2.1400 - regression_loss: 1.6088 - classification_loss: 0.5311\n",
            "20/86 [=====>........................] - ETA: 1:17 - loss: 2.1547 - regression_loss: 1.6214 - classification_loss: 0.5333\n",
            "21/86 [======>.......................] - ETA: 1:15 - loss: 2.1488 - regression_loss: 1.6202 - classification_loss: 0.5286\n",
            "22/86 [======>.......................] - ETA: 1:13 - loss: 2.1484 - regression_loss: 1.6198 - classification_loss: 0.5287\n",
            "23/86 [=======>......................] - ETA: 1:12 - loss: 2.1465 - regression_loss: 1.6179 - classification_loss: 0.5287\n",
            "24/86 [=======>......................] - ETA: 1:10 - loss: 2.1504 - regression_loss: 1.6200 - classification_loss: 0.5304\n",
            "25/86 [=======>......................] - ETA: 1:08 - loss: 2.1522 - regression_loss: 1.6211 - classification_loss: 0.5312\n",
            "26/86 [========>.....................] - ETA: 1:07 - loss: 2.1628 - regression_loss: 1.6286 - classification_loss: 0.5342\n",
            "27/86 [========>.....................] - ETA: 1:07 - loss: 2.1470 - regression_loss: 1.6185 - classification_loss: 0.5285\n",
            "28/86 [========>.....................] - ETA: 1:08 - loss: 2.1504 - regression_loss: 1.6217 - classification_loss: 0.5287\n",
            "29/86 [=========>....................] - ETA: 1:10 - loss: 2.1568 - regression_loss: 1.6270 - classification_loss: 0.5298\n",
            "30/86 [=========>....................] - ETA: 1:08 - loss: 2.1538 - regression_loss: 1.6262 - classification_loss: 0.5276\n",
            "31/86 [=========>....................] - ETA: 1:07 - loss: 2.1462 - regression_loss: 1.6216 - classification_loss: 0.5247\n",
            "32/86 [==========>...................] - ETA: 1:05 - loss: 2.1449 - regression_loss: 1.6187 - classification_loss: 0.5261\n",
            "33/86 [==========>...................] - ETA: 1:03 - loss: 2.1437 - regression_loss: 1.6181 - classification_loss: 0.5257\n",
            "34/86 [==========>...................] - ETA: 1:02 - loss: 2.1412 - regression_loss: 1.6162 - classification_loss: 0.5250\n",
            "35/86 [===========>..................] - ETA: 1:01 - loss: 2.1357 - regression_loss: 1.6113 - classification_loss: 0.5244\n",
            "36/86 [===========>..................] - ETA: 59s - loss: 2.1255 - regression_loss: 1.6025 - classification_loss: 0.5229 \n",
            "37/86 [===========>..................] - ETA: 57s - loss: 2.1187 - regression_loss: 1.5975 - classification_loss: 0.5212\n",
            "38/86 [============>.................] - ETA: 56s - loss: 2.1139 - regression_loss: 1.5926 - classification_loss: 0.5214\n",
            "39/86 [============>.................] - ETA: 54s - loss: 2.1064 - regression_loss: 1.5882 - classification_loss: 0.5182\n",
            "40/86 [============>.................] - ETA: 53s - loss: 2.1000 - regression_loss: 1.5822 - classification_loss: 0.5178\n",
            "41/86 [=============>................] - ETA: 52s - loss: 2.0962 - regression_loss: 1.5805 - classification_loss: 0.5157\n",
            "42/86 [=============>................] - ETA: 50s - loss: 2.0973 - regression_loss: 1.5812 - classification_loss: 0.5161\n",
            "43/86 [==============>...............] - ETA: 49s - loss: 2.0967 - regression_loss: 1.5812 - classification_loss: 0.5155\n",
            "44/86 [==============>...............] - ETA: 47s - loss: 2.0962 - regression_loss: 1.5814 - classification_loss: 0.5148\n",
            "45/86 [==============>...............] - ETA: 47s - loss: 2.0941 - regression_loss: 1.5801 - classification_loss: 0.5139\n",
            "46/86 [===============>..............] - ETA: 45s - loss: 2.0933 - regression_loss: 1.5786 - classification_loss: 0.5148\n",
            "47/86 [===============>..............] - ETA: 44s - loss: 2.0906 - regression_loss: 1.5784 - classification_loss: 0.5122\n",
            "48/86 [===============>..............] - ETA: 43s - loss: 2.0913 - regression_loss: 1.5799 - classification_loss: 0.5115\n",
            "49/86 [================>.............] - ETA: 41s - loss: 2.0957 - regression_loss: 1.5843 - classification_loss: 0.5114\n",
            "50/86 [================>.............] - ETA: 40s - loss: 2.0901 - regression_loss: 1.5800 - classification_loss: 0.5101\n",
            "51/86 [================>.............] - ETA: 39s - loss: 2.0884 - regression_loss: 1.5788 - classification_loss: 0.5096\n",
            "52/86 [=================>............] - ETA: 37s - loss: 2.0929 - regression_loss: 1.5826 - classification_loss: 0.5104\n",
            "53/86 [=================>............] - ETA: 36s - loss: 2.0971 - regression_loss: 1.5861 - classification_loss: 0.5110\n",
            "54/86 [=================>............] - ETA: 35s - loss: 2.1000 - regression_loss: 1.5888 - classification_loss: 0.5113\n",
            "55/86 [==================>...........] - ETA: 34s - loss: 2.1085 - regression_loss: 1.5957 - classification_loss: 0.5129\n",
            "56/86 [==================>...........] - ETA: 33s - loss: 2.1047 - regression_loss: 1.5929 - classification_loss: 0.5118\n",
            "57/86 [==================>...........] - ETA: 31s - loss: 2.1024 - regression_loss: 1.5910 - classification_loss: 0.5114\n",
            "58/86 [===================>..........] - ETA: 30s - loss: 2.0980 - regression_loss: 1.5864 - classification_loss: 0.5116\n",
            "59/86 [===================>..........] - ETA: 29s - loss: 2.0992 - regression_loss: 1.5861 - classification_loss: 0.5131\n",
            "60/86 [===================>..........] - ETA: 28s - loss: 2.0999 - regression_loss: 1.5878 - classification_loss: 0.5121\n",
            "61/86 [====================>.........] - ETA: 27s - loss: 2.0958 - regression_loss: 1.5847 - classification_loss: 0.5111\n",
            "62/86 [====================>.........] - ETA: 25s - loss: 2.0974 - regression_loss: 1.5853 - classification_loss: 0.5121\n",
            "63/86 [====================>.........] - ETA: 24s - loss: 2.1014 - regression_loss: 1.5879 - classification_loss: 0.5134\n",
            "64/86 [=====================>........] - ETA: 23s - loss: 2.0985 - regression_loss: 1.5858 - classification_loss: 0.5127\n",
            "65/86 [=====================>........] - ETA: 22s - loss: 2.0979 - regression_loss: 1.5859 - classification_loss: 0.5120\n",
            "66/86 [======================>.......] - ETA: 21s - loss: 2.0991 - regression_loss: 1.5870 - classification_loss: 0.5121\n",
            "67/86 [======================>.......] - ETA: 20s - loss: 2.0936 - regression_loss: 1.5830 - classification_loss: 0.5106\n",
            "68/86 [======================>.......] - ETA: 19s - loss: 2.0946 - regression_loss: 1.5842 - classification_loss: 0.5104\n",
            "69/86 [=======================>......] - ETA: 18s - loss: 2.0960 - regression_loss: 1.5856 - classification_loss: 0.5103\n",
            "70/86 [=======================>......] - ETA: 17s - loss: 2.0913 - regression_loss: 1.5821 - classification_loss: 0.5092\n",
            "71/86 [=======================>......] - ETA: 16s - loss: 2.0892 - regression_loss: 1.5802 - classification_loss: 0.5089\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.0862 - regression_loss: 1.5782 - classification_loss: 0.5080\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.0861 - regression_loss: 1.5771 - classification_loss: 0.5090\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.0872 - regression_loss: 1.5781 - classification_loss: 0.5091\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.0829 - regression_loss: 1.5746 - classification_loss: 0.5083\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.0871 - regression_loss: 1.5786 - classification_loss: 0.5085\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.0839 - regression_loss: 1.5763 - classification_loss: 0.5075 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.0891 - regression_loss: 1.5807 - classification_loss: 0.5083\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.0908 - regression_loss: 1.5820 - classification_loss: 0.5087\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.0925 - regression_loss: 1.5835 - classification_loss: 0.5090\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.0949 - regression_loss: 1.5851 - classification_loss: 0.5098\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.1018 - regression_loss: 1.5907 - classification_loss: 0.5111\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.1056 - regression_loss: 1.5925 - classification_loss: 0.5131\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.1042 - regression_loss: 1.5920 - classification_loss: 0.5121\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.0999 - regression_loss: 1.5891 - classification_loss: 0.5108\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1028 - regression_loss: 1.5911 - classification_loss: 0.5117\n",
            "Epoch 36: saving model to ./snapshots\\resnet50_csv_36.h5\n",
            "\n",
            "Epoch 36: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
            "\n",
            "86/86 [==============================] - 92s 1s/step - loss: 2.1028 - regression_loss: 1.5911 - classification_loss: 0.5117 - lr: 1.0000e-14\n",
            "Epoch 37/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:51 - loss: 2.1111 - regression_loss: 1.5333 - classification_loss: 0.5778\n",
            " 2/86 [..............................] - ETA: 1:28 - loss: 2.0031 - regression_loss: 1.4995 - classification_loss: 0.5036\n",
            " 3/86 [>.............................] - ETA: 1:41 - loss: 2.0362 - regression_loss: 1.5273 - classification_loss: 0.5089\n",
            " 4/86 [>.............................] - ETA: 1:33 - loss: 2.1036 - regression_loss: 1.5984 - classification_loss: 0.5052\n",
            " 5/86 [>.............................] - ETA: 1:27 - loss: 2.1241 - regression_loss: 1.6160 - classification_loss: 0.5081\n",
            " 6/86 [=>............................] - ETA: 1:24 - loss: 2.1169 - regression_loss: 1.6123 - classification_loss: 0.5046\n",
            " 7/86 [=>............................] - ETA: 1:20 - loss: 2.1448 - regression_loss: 1.6302 - classification_loss: 0.5146\n",
            " 8/86 [=>............................] - ETA: 1:26 - loss: 2.1306 - regression_loss: 1.6168 - classification_loss: 0.5138\n",
            " 9/86 [==>...........................] - ETA: 1:35 - loss: 2.1328 - regression_loss: 1.6244 - classification_loss: 0.5085\n",
            "10/86 [==>...........................] - ETA: 1:45 - loss: 2.1147 - regression_loss: 1.6168 - classification_loss: 0.4979\n",
            "11/86 [==>...........................] - ETA: 1:42 - loss: 2.1128 - regression_loss: 1.6113 - classification_loss: 0.5015\n",
            "12/86 [===>..........................] - ETA: 1:36 - loss: 2.1056 - regression_loss: 1.6033 - classification_loss: 0.5023\n",
            "13/86 [===>..........................] - ETA: 1:33 - loss: 2.0999 - regression_loss: 1.6002 - classification_loss: 0.4997\n",
            "14/86 [===>..........................] - ETA: 1:31 - loss: 2.0951 - regression_loss: 1.5919 - classification_loss: 0.5032\n",
            "15/86 [====>.........................] - ETA: 1:29 - loss: 2.0963 - regression_loss: 1.5923 - classification_loss: 0.5040\n",
            "16/86 [====>.........................] - ETA: 1:27 - loss: 2.0933 - regression_loss: 1.5905 - classification_loss: 0.5028\n",
            "17/86 [====>.........................] - ETA: 1:24 - loss: 2.0968 - regression_loss: 1.5914 - classification_loss: 0.5053\n",
            "18/86 [=====>........................] - ETA: 1:23 - loss: 2.0908 - regression_loss: 1.5862 - classification_loss: 0.5046\n",
            "19/86 [=====>........................] - ETA: 1:20 - loss: 2.0876 - regression_loss: 1.5840 - classification_loss: 0.5035\n",
            "20/86 [=====>........................] - ETA: 1:18 - loss: 2.0777 - regression_loss: 1.5725 - classification_loss: 0.5052\n",
            "21/86 [======>.......................] - ETA: 1:16 - loss: 2.0796 - regression_loss: 1.5743 - classification_loss: 0.5053\n",
            "22/86 [======>.......................] - ETA: 1:14 - loss: 2.0895 - regression_loss: 1.5839 - classification_loss: 0.5055\n",
            "23/86 [=======>......................] - ETA: 1:13 - loss: 2.0909 - regression_loss: 1.5844 - classification_loss: 0.5066\n",
            "24/86 [=======>......................] - ETA: 1:11 - loss: 2.0974 - regression_loss: 1.5866 - classification_loss: 0.5108\n",
            "25/86 [=======>......................] - ETA: 1:10 - loss: 2.0967 - regression_loss: 1.5869 - classification_loss: 0.5099\n",
            "26/86 [========>.....................] - ETA: 1:12 - loss: 2.1126 - regression_loss: 1.5995 - classification_loss: 0.5130\n",
            "27/86 [========>.....................] - ETA: 1:14 - loss: 2.1060 - regression_loss: 1.5945 - classification_loss: 0.5115\n",
            "28/86 [========>.....................] - ETA: 1:13 - loss: 2.1100 - regression_loss: 1.5983 - classification_loss: 0.5117\n",
            "29/86 [=========>....................] - ETA: 1:10 - loss: 2.1001 - regression_loss: 1.5906 - classification_loss: 0.5095\n",
            "30/86 [=========>....................] - ETA: 1:09 - loss: 2.1005 - regression_loss: 1.5904 - classification_loss: 0.5101\n",
            "31/86 [=========>....................] - ETA: 1:07 - loss: 2.1193 - regression_loss: 1.6058 - classification_loss: 0.5135\n",
            "32/86 [==========>...................] - ETA: 1:05 - loss: 2.1171 - regression_loss: 1.6042 - classification_loss: 0.5129\n",
            "33/86 [==========>...................] - ETA: 1:03 - loss: 2.1154 - regression_loss: 1.6038 - classification_loss: 0.5116\n",
            "34/86 [==========>...................] - ETA: 1:02 - loss: 2.1181 - regression_loss: 1.6066 - classification_loss: 0.5115\n",
            "35/86 [===========>..................] - ETA: 1:00 - loss: 2.1242 - regression_loss: 1.6086 - classification_loss: 0.5156\n",
            "36/86 [===========>..................] - ETA: 59s - loss: 2.1172 - regression_loss: 1.6033 - classification_loss: 0.5139 \n",
            "37/86 [===========>..................] - ETA: 57s - loss: 2.1167 - regression_loss: 1.6016 - classification_loss: 0.5152\n",
            "38/86 [============>.................] - ETA: 55s - loss: 2.1098 - regression_loss: 1.5962 - classification_loss: 0.5136\n",
            "39/86 [============>.................] - ETA: 54s - loss: 2.1041 - regression_loss: 1.5913 - classification_loss: 0.5128\n",
            "40/86 [============>.................] - ETA: 53s - loss: 2.1026 - regression_loss: 1.5898 - classification_loss: 0.5128\n",
            "41/86 [=============>................] - ETA: 51s - loss: 2.1025 - regression_loss: 1.5900 - classification_loss: 0.5124\n",
            "42/86 [=============>................] - ETA: 50s - loss: 2.1045 - regression_loss: 1.5913 - classification_loss: 0.5132\n",
            "43/86 [==============>...............] - ETA: 49s - loss: 2.1144 - regression_loss: 1.5982 - classification_loss: 0.5162\n",
            "44/86 [==============>...............] - ETA: 47s - loss: 2.1225 - regression_loss: 1.6046 - classification_loss: 0.5179\n",
            "45/86 [==============>...............] - ETA: 46s - loss: 2.1170 - regression_loss: 1.6006 - classification_loss: 0.5164\n",
            "46/86 [===============>..............] - ETA: 45s - loss: 2.1101 - regression_loss: 1.5952 - classification_loss: 0.5149\n",
            "47/86 [===============>..............] - ETA: 43s - loss: 2.1065 - regression_loss: 1.5923 - classification_loss: 0.5142\n",
            "48/86 [===============>..............] - ETA: 42s - loss: 2.1046 - regression_loss: 1.5912 - classification_loss: 0.5134\n",
            "49/86 [================>.............] - ETA: 41s - loss: 2.0958 - regression_loss: 1.5849 - classification_loss: 0.5109\n",
            "50/86 [================>.............] - ETA: 40s - loss: 2.0976 - regression_loss: 1.5865 - classification_loss: 0.5111\n",
            "51/86 [================>.............] - ETA: 39s - loss: 2.0967 - regression_loss: 1.5860 - classification_loss: 0.5106\n",
            "52/86 [=================>............] - ETA: 37s - loss: 2.0947 - regression_loss: 1.5853 - classification_loss: 0.5094\n",
            "53/86 [=================>............] - ETA: 36s - loss: 2.0948 - regression_loss: 1.5861 - classification_loss: 0.5087\n",
            "54/86 [=================>............] - ETA: 35s - loss: 2.0930 - regression_loss: 1.5839 - classification_loss: 0.5091\n",
            "55/86 [==================>...........] - ETA: 34s - loss: 2.0988 - regression_loss: 1.5862 - classification_loss: 0.5126\n",
            "56/86 [==================>...........] - ETA: 32s - loss: 2.0939 - regression_loss: 1.5833 - classification_loss: 0.5106\n",
            "57/86 [==================>...........] - ETA: 31s - loss: 2.0945 - regression_loss: 1.5834 - classification_loss: 0.5111\n",
            "58/86 [===================>..........] - ETA: 30s - loss: 2.0967 - regression_loss: 1.5855 - classification_loss: 0.5112\n",
            "59/86 [===================>..........] - ETA: 29s - loss: 2.0990 - regression_loss: 1.5860 - classification_loss: 0.5130\n",
            "60/86 [===================>..........] - ETA: 28s - loss: 2.1026 - regression_loss: 1.5889 - classification_loss: 0.5137\n",
            "61/86 [====================>.........] - ETA: 27s - loss: 2.0982 - regression_loss: 1.5855 - classification_loss: 0.5127\n",
            "62/86 [====================>.........] - ETA: 26s - loss: 2.1012 - regression_loss: 1.5888 - classification_loss: 0.5124\n",
            "63/86 [====================>.........] - ETA: 25s - loss: 2.1039 - regression_loss: 1.5906 - classification_loss: 0.5134\n",
            "64/86 [=====================>........] - ETA: 23s - loss: 2.1022 - regression_loss: 1.5898 - classification_loss: 0.5124\n",
            "65/86 [=====================>........] - ETA: 22s - loss: 2.1069 - regression_loss: 1.5923 - classification_loss: 0.5146\n",
            "66/86 [======================>.......] - ETA: 21s - loss: 2.1050 - regression_loss: 1.5916 - classification_loss: 0.5134\n",
            "67/86 [======================>.......] - ETA: 20s - loss: 2.1016 - regression_loss: 1.5895 - classification_loss: 0.5121\n",
            "68/86 [======================>.......] - ETA: 19s - loss: 2.1001 - regression_loss: 1.5885 - classification_loss: 0.5117\n",
            "69/86 [=======================>......] - ETA: 18s - loss: 2.1023 - regression_loss: 1.5904 - classification_loss: 0.5119\n",
            "70/86 [=======================>......] - ETA: 17s - loss: 2.1010 - regression_loss: 1.5893 - classification_loss: 0.5117\n",
            "71/86 [=======================>......] - ETA: 16s - loss: 2.0964 - regression_loss: 1.5864 - classification_loss: 0.5100\n",
            "72/86 [========================>.....] - ETA: 15s - loss: 2.0982 - regression_loss: 1.5880 - classification_loss: 0.5101\n",
            "73/86 [========================>.....] - ETA: 14s - loss: 2.0969 - regression_loss: 1.5875 - classification_loss: 0.5095\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.0971 - regression_loss: 1.5875 - classification_loss: 0.5095\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.0965 - regression_loss: 1.5876 - classification_loss: 0.5089\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.0932 - regression_loss: 1.5844 - classification_loss: 0.5088\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.0924 - regression_loss: 1.5844 - classification_loss: 0.5080 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.0895 - regression_loss: 1.5818 - classification_loss: 0.5077\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.0850 - regression_loss: 1.5788 - classification_loss: 0.5063\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.0857 - regression_loss: 1.5800 - classification_loss: 0.5058\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.0877 - regression_loss: 1.5811 - classification_loss: 0.5066\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.0849 - regression_loss: 1.5793 - classification_loss: 0.5056\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.0837 - regression_loss: 1.5783 - classification_loss: 0.5054\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.0840 - regression_loss: 1.5785 - classification_loss: 0.5055\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.0862 - regression_loss: 1.5798 - classification_loss: 0.5064\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0873 - regression_loss: 1.5806 - classification_loss: 0.5067\n",
            "Epoch 37: saving model to ./snapshots\\resnet50_csv_37.h5\n",
            "\n",
            "86/86 [==============================] - 93s 1s/step - loss: 2.0873 - regression_loss: 1.5806 - classification_loss: 0.5067 - lr: 1.0000e-15\n",
            "Epoch 38/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:37 - loss: 1.5214 - regression_loss: 1.1433 - classification_loss: 0.3781\n",
            " 2/86 [..............................] - ETA: 1:12 - loss: 1.7278 - regression_loss: 1.2934 - classification_loss: 0.4345\n",
            " 3/86 [>.............................] - ETA: 1:09 - loss: 1.7927 - regression_loss: 1.3413 - classification_loss: 0.4514\n",
            " 4/86 [>.............................] - ETA: 1:13 - loss: 1.9087 - regression_loss: 1.4429 - classification_loss: 0.4658\n",
            " 5/86 [>.............................] - ETA: 1:17 - loss: 1.9612 - regression_loss: 1.4878 - classification_loss: 0.4735\n",
            " 6/86 [=>............................] - ETA: 1:11 - loss: 1.9782 - regression_loss: 1.5011 - classification_loss: 0.4770\n",
            " 7/86 [=>............................] - ETA: 1:14 - loss: 2.0154 - regression_loss: 1.5313 - classification_loss: 0.4841\n",
            " 8/86 [=>............................] - ETA: 1:12 - loss: 1.9976 - regression_loss: 1.5118 - classification_loss: 0.4857\n",
            " 9/86 [==>...........................] - ETA: 1:10 - loss: 2.0333 - regression_loss: 1.5427 - classification_loss: 0.4906\n",
            "10/86 [==>...........................] - ETA: 1:09 - loss: 2.0129 - regression_loss: 1.5260 - classification_loss: 0.4869\n",
            "11/86 [==>...........................] - ETA: 1:09 - loss: 2.0433 - regression_loss: 1.5465 - classification_loss: 0.4968\n",
            "12/86 [===>..........................] - ETA: 1:09 - loss: 2.0290 - regression_loss: 1.5354 - classification_loss: 0.4936\n",
            "13/86 [===>..........................] - ETA: 1:10 - loss: 2.0326 - regression_loss: 1.5394 - classification_loss: 0.4932\n",
            "14/86 [===>..........................] - ETA: 1:08 - loss: 2.0325 - regression_loss: 1.5397 - classification_loss: 0.4928\n",
            "15/86 [====>.........................] - ETA: 1:08 - loss: 2.0649 - regression_loss: 1.5661 - classification_loss: 0.4988\n",
            "16/86 [====>.........................] - ETA: 1:06 - loss: 2.0620 - regression_loss: 1.5661 - classification_loss: 0.4960\n",
            "17/86 [====>.........................] - ETA: 1:05 - loss: 2.0696 - regression_loss: 1.5717 - classification_loss: 0.4980\n",
            "18/86 [=====>........................] - ETA: 1:04 - loss: 2.0661 - regression_loss: 1.5730 - classification_loss: 0.4931\n",
            "19/86 [=====>........................] - ETA: 1:03 - loss: 2.0640 - regression_loss: 1.5721 - classification_loss: 0.4919\n",
            "20/86 [=====>........................] - ETA: 1:02 - loss: 2.0664 - regression_loss: 1.5728 - classification_loss: 0.4936\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.0645 - regression_loss: 1.5690 - classification_loss: 0.4955\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.0546 - regression_loss: 1.5624 - classification_loss: 0.4923\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.0703 - regression_loss: 1.5756 - classification_loss: 0.4946\n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.0725 - regression_loss: 1.5772 - classification_loss: 0.4952 \n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.0782 - regression_loss: 1.5805 - classification_loss: 0.4977\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.0742 - regression_loss: 1.5762 - classification_loss: 0.4980\n",
            "27/86 [========>.....................] - ETA: 55s - loss: 2.0782 - regression_loss: 1.5796 - classification_loss: 0.4985\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.0768 - regression_loss: 1.5796 - classification_loss: 0.4972\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.0862 - regression_loss: 1.5844 - classification_loss: 0.5018\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.0829 - regression_loss: 1.5820 - classification_loss: 0.5009\n",
            "31/86 [=========>....................] - ETA: 51s - loss: 2.0738 - regression_loss: 1.5744 - classification_loss: 0.4994\n",
            "32/86 [==========>...................] - ETA: 50s - loss: 2.0821 - regression_loss: 1.5776 - classification_loss: 0.5045\n",
            "33/86 [==========>...................] - ETA: 49s - loss: 2.0828 - regression_loss: 1.5786 - classification_loss: 0.5042\n",
            "34/86 [==========>...................] - ETA: 48s - loss: 2.0877 - regression_loss: 1.5828 - classification_loss: 0.5048\n",
            "35/86 [===========>..................] - ETA: 47s - loss: 2.0899 - regression_loss: 1.5839 - classification_loss: 0.5060\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.0925 - regression_loss: 1.5862 - classification_loss: 0.5063\n",
            "37/86 [===========>..................] - ETA: 45s - loss: 2.0928 - regression_loss: 1.5850 - classification_loss: 0.5078\n",
            "38/86 [============>.................] - ETA: 44s - loss: 2.0793 - regression_loss: 1.5753 - classification_loss: 0.5040\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.0820 - regression_loss: 1.5764 - classification_loss: 0.5056\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.0783 - regression_loss: 1.5743 - classification_loss: 0.5040\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.0825 - regression_loss: 1.5787 - classification_loss: 0.5039\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.0907 - regression_loss: 1.5828 - classification_loss: 0.5080\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.0971 - regression_loss: 1.5857 - classification_loss: 0.5114\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.0957 - regression_loss: 1.5842 - classification_loss: 0.5116\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.0983 - regression_loss: 1.5847 - classification_loss: 0.5136\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.0928 - regression_loss: 1.5799 - classification_loss: 0.5129\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.0949 - regression_loss: 1.5826 - classification_loss: 0.5122\n",
            "48/86 [===============>..............] - ETA: 37s - loss: 2.0905 - regression_loss: 1.5781 - classification_loss: 0.5123\n",
            "49/86 [================>.............] - ETA: 37s - loss: 2.0952 - regression_loss: 1.5840 - classification_loss: 0.5113\n",
            "50/86 [================>.............] - ETA: 36s - loss: 2.0945 - regression_loss: 1.5842 - classification_loss: 0.5103\n",
            "51/86 [================>.............] - ETA: 35s - loss: 2.0989 - regression_loss: 1.5879 - classification_loss: 0.5110\n",
            "52/86 [=================>............] - ETA: 34s - loss: 2.0979 - regression_loss: 1.5878 - classification_loss: 0.5101\n",
            "53/86 [=================>............] - ETA: 33s - loss: 2.0993 - regression_loss: 1.5889 - classification_loss: 0.5104\n",
            "54/86 [=================>............] - ETA: 32s - loss: 2.1014 - regression_loss: 1.5909 - classification_loss: 0.5105\n",
            "55/86 [==================>...........] - ETA: 31s - loss: 2.1080 - regression_loss: 1.5960 - classification_loss: 0.5120\n",
            "56/86 [==================>...........] - ETA: 30s - loss: 2.1067 - regression_loss: 1.5947 - classification_loss: 0.5121\n",
            "57/86 [==================>...........] - ETA: 29s - loss: 2.1105 - regression_loss: 1.5971 - classification_loss: 0.5134\n",
            "58/86 [===================>..........] - ETA: 28s - loss: 2.1120 - regression_loss: 1.5977 - classification_loss: 0.5144\n",
            "59/86 [===================>..........] - ETA: 27s - loss: 2.1090 - regression_loss: 1.5950 - classification_loss: 0.5139\n",
            "60/86 [===================>..........] - ETA: 26s - loss: 2.1145 - regression_loss: 1.5996 - classification_loss: 0.5150\n",
            "61/86 [====================>.........] - ETA: 25s - loss: 2.1088 - regression_loss: 1.5956 - classification_loss: 0.5132\n",
            "62/86 [====================>.........] - ETA: 24s - loss: 2.1129 - regression_loss: 1.5986 - classification_loss: 0.5143\n",
            "63/86 [====================>.........] - ETA: 23s - loss: 2.1136 - regression_loss: 1.5989 - classification_loss: 0.5147\n",
            "64/86 [=====================>........] - ETA: 22s - loss: 2.1092 - regression_loss: 1.5949 - classification_loss: 0.5143\n",
            "65/86 [=====================>........] - ETA: 21s - loss: 2.1071 - regression_loss: 1.5931 - classification_loss: 0.5140\n",
            "66/86 [======================>.......] - ETA: 20s - loss: 2.1073 - regression_loss: 1.5936 - classification_loss: 0.5138\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1049 - regression_loss: 1.5913 - classification_loss: 0.5135\n",
            "68/86 [======================>.......] - ETA: 18s - loss: 2.1031 - regression_loss: 1.5894 - classification_loss: 0.5137\n",
            "69/86 [=======================>......] - ETA: 17s - loss: 2.1033 - regression_loss: 1.5899 - classification_loss: 0.5134\n",
            "70/86 [=======================>......] - ETA: 16s - loss: 2.1028 - regression_loss: 1.5892 - classification_loss: 0.5135\n",
            "71/86 [=======================>......] - ETA: 15s - loss: 2.1010 - regression_loss: 1.5879 - classification_loss: 0.5132\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.1005 - regression_loss: 1.5883 - classification_loss: 0.5122\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.1002 - regression_loss: 1.5877 - classification_loss: 0.5125\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.1001 - regression_loss: 1.5878 - classification_loss: 0.5123\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1014 - regression_loss: 1.5894 - classification_loss: 0.5120\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.0994 - regression_loss: 1.5881 - classification_loss: 0.5113\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.0949 - regression_loss: 1.5849 - classification_loss: 0.5099 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.0940 - regression_loss: 1.5838 - classification_loss: 0.5102\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.0943 - regression_loss: 1.5839 - classification_loss: 0.5104\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.0978 - regression_loss: 1.5862 - classification_loss: 0.5116\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.0977 - regression_loss: 1.5865 - classification_loss: 0.5113\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.0972 - regression_loss: 1.5859 - classification_loss: 0.5113\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.0982 - regression_loss: 1.5871 - classification_loss: 0.5111\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.0961 - regression_loss: 1.5863 - classification_loss: 0.5098\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.0933 - regression_loss: 1.5840 - classification_loss: 0.5093\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0922 - regression_loss: 1.5833 - classification_loss: 0.5089\n",
            "Epoch 38: saving model to ./snapshots\\resnet50_csv_38.h5\n",
            "\n",
            "Epoch 38: ReduceLROnPlateau reducing learning rate to 9.999998977483754e-17.\n",
            "\n",
            "86/86 [==============================] - 88s 1s/step - loss: 2.0922 - regression_loss: 1.5833 - classification_loss: 0.5089 - lr: 1.0000e-15\n",
            "Epoch 39/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:13 - loss: 2.2089 - regression_loss: 1.6412 - classification_loss: 0.5676\n",
            " 2/86 [..............................] - ETA: 1:10 - loss: 2.1851 - regression_loss: 1.6486 - classification_loss: 0.5365\n",
            " 3/86 [>.............................] - ETA: 1:13 - loss: 2.0316 - regression_loss: 1.5405 - classification_loss: 0.4911\n",
            " 4/86 [>.............................] - ETA: 1:11 - loss: 2.0916 - regression_loss: 1.5777 - classification_loss: 0.5140\n",
            " 5/86 [>.............................] - ETA: 1:15 - loss: 2.1111 - regression_loss: 1.5874 - classification_loss: 0.5237\n",
            " 6/86 [=>............................] - ETA: 1:12 - loss: 2.1363 - regression_loss: 1.6105 - classification_loss: 0.5259\n",
            " 7/86 [=>............................] - ETA: 1:17 - loss: 2.1491 - regression_loss: 1.6153 - classification_loss: 0.5339\n",
            " 8/86 [=>............................] - ETA: 1:13 - loss: 2.1540 - regression_loss: 1.6131 - classification_loss: 0.5409\n",
            " 9/86 [==>...........................] - ETA: 1:14 - loss: 2.1579 - regression_loss: 1.6108 - classification_loss: 0.5472\n",
            "10/86 [==>...........................] - ETA: 1:15 - loss: 2.1345 - regression_loss: 1.5898 - classification_loss: 0.5447\n",
            "11/86 [==>...........................] - ETA: 1:19 - loss: 2.1099 - regression_loss: 1.5726 - classification_loss: 0.5372\n",
            "12/86 [===>..........................] - ETA: 1:27 - loss: 2.1527 - regression_loss: 1.6092 - classification_loss: 0.5435\n",
            "13/86 [===>..........................] - ETA: 1:27 - loss: 2.1668 - regression_loss: 1.6224 - classification_loss: 0.5444\n",
            "14/86 [===>..........................] - ETA: 1:25 - loss: 2.1597 - regression_loss: 1.6155 - classification_loss: 0.5442\n",
            "15/86 [====>.........................] - ETA: 1:22 - loss: 2.1407 - regression_loss: 1.6026 - classification_loss: 0.5381\n",
            "16/86 [====>.........................] - ETA: 1:20 - loss: 2.1292 - regression_loss: 1.6007 - classification_loss: 0.5286\n",
            "17/86 [====>.........................] - ETA: 1:20 - loss: 2.1448 - regression_loss: 1.6080 - classification_loss: 0.5368\n",
            "18/86 [=====>........................] - ETA: 1:18 - loss: 2.1493 - regression_loss: 1.6087 - classification_loss: 0.5406\n",
            "19/86 [=====>........................] - ETA: 1:16 - loss: 2.1496 - regression_loss: 1.6061 - classification_loss: 0.5434\n",
            "20/86 [=====>........................] - ETA: 1:14 - loss: 2.1496 - regression_loss: 1.6069 - classification_loss: 0.5427\n",
            "21/86 [======>.......................] - ETA: 1:12 - loss: 2.1397 - regression_loss: 1.6007 - classification_loss: 0.5391\n",
            "22/86 [======>.......................] - ETA: 1:10 - loss: 2.1320 - regression_loss: 1.5958 - classification_loss: 0.5363\n",
            "23/86 [=======>......................] - ETA: 1:08 - loss: 2.1486 - regression_loss: 1.6087 - classification_loss: 0.5399\n",
            "24/86 [=======>......................] - ETA: 1:07 - loss: 2.1534 - regression_loss: 1.6127 - classification_loss: 0.5407\n",
            "25/86 [=======>......................] - ETA: 1:06 - loss: 2.1400 - regression_loss: 1.6041 - classification_loss: 0.5359\n",
            "26/86 [========>.....................] - ETA: 1:04 - loss: 2.1164 - regression_loss: 1.5865 - classification_loss: 0.5299\n",
            "27/86 [========>.....................] - ETA: 1:03 - loss: 2.1237 - regression_loss: 1.5892 - classification_loss: 0.5345\n",
            "28/86 [========>.....................] - ETA: 1:02 - loss: 2.1232 - regression_loss: 1.5888 - classification_loss: 0.5344\n",
            "29/86 [=========>....................] - ETA: 1:00 - loss: 2.1238 - regression_loss: 1.5906 - classification_loss: 0.5332\n",
            "30/86 [=========>....................] - ETA: 59s - loss: 2.1178 - regression_loss: 1.5891 - classification_loss: 0.5287 \n",
            "31/86 [=========>....................] - ETA: 58s - loss: 2.1228 - regression_loss: 1.5936 - classification_loss: 0.5292\n",
            "32/86 [==========>...................] - ETA: 57s - loss: 2.1183 - regression_loss: 1.5899 - classification_loss: 0.5284\n",
            "33/86 [==========>...................] - ETA: 55s - loss: 2.1174 - regression_loss: 1.5893 - classification_loss: 0.5281\n",
            "34/86 [==========>...................] - ETA: 55s - loss: 2.1261 - regression_loss: 1.5981 - classification_loss: 0.5280\n",
            "35/86 [===========>..................] - ETA: 53s - loss: 2.1301 - regression_loss: 1.6023 - classification_loss: 0.5278\n",
            "36/86 [===========>..................] - ETA: 53s - loss: 2.1261 - regression_loss: 1.6008 - classification_loss: 0.5253\n",
            "37/86 [===========>..................] - ETA: 51s - loss: 2.1333 - regression_loss: 1.6063 - classification_loss: 0.5269\n",
            "38/86 [============>.................] - ETA: 50s - loss: 2.1403 - regression_loss: 1.6131 - classification_loss: 0.5272\n",
            "39/86 [============>.................] - ETA: 49s - loss: 2.1322 - regression_loss: 1.6063 - classification_loss: 0.5259\n",
            "40/86 [============>.................] - ETA: 48s - loss: 2.1339 - regression_loss: 1.6075 - classification_loss: 0.5264\n",
            "41/86 [=============>................] - ETA: 46s - loss: 2.1349 - regression_loss: 1.6088 - classification_loss: 0.5260\n",
            "42/86 [=============>................] - ETA: 46s - loss: 2.1391 - regression_loss: 1.6125 - classification_loss: 0.5267\n",
            "43/86 [==============>...............] - ETA: 44s - loss: 2.1389 - regression_loss: 1.6124 - classification_loss: 0.5265\n",
            "44/86 [==============>...............] - ETA: 43s - loss: 2.1480 - regression_loss: 1.6194 - classification_loss: 0.5285\n",
            "45/86 [==============>...............] - ETA: 42s - loss: 2.1461 - regression_loss: 1.6133 - classification_loss: 0.5328\n",
            "46/86 [===============>..............] - ETA: 41s - loss: 2.1457 - regression_loss: 1.6132 - classification_loss: 0.5325\n",
            "47/86 [===============>..............] - ETA: 40s - loss: 2.1418 - regression_loss: 1.6100 - classification_loss: 0.5318\n",
            "48/86 [===============>..............] - ETA: 39s - loss: 2.1382 - regression_loss: 1.6078 - classification_loss: 0.5304\n",
            "49/86 [================>.............] - ETA: 38s - loss: 2.1383 - regression_loss: 1.6084 - classification_loss: 0.5299\n",
            "50/86 [================>.............] - ETA: 37s - loss: 2.1407 - regression_loss: 1.6102 - classification_loss: 0.5305\n",
            "51/86 [================>.............] - ETA: 36s - loss: 2.1465 - regression_loss: 1.6157 - classification_loss: 0.5308\n",
            "52/86 [=================>............] - ETA: 34s - loss: 2.1493 - regression_loss: 1.6177 - classification_loss: 0.5316\n",
            "53/86 [=================>............] - ETA: 33s - loss: 2.1470 - regression_loss: 1.6161 - classification_loss: 0.5309\n",
            "54/86 [=================>............] - ETA: 32s - loss: 2.1423 - regression_loss: 1.6129 - classification_loss: 0.5294\n",
            "55/86 [==================>...........] - ETA: 31s - loss: 2.1363 - regression_loss: 1.6090 - classification_loss: 0.5273\n",
            "56/86 [==================>...........] - ETA: 30s - loss: 2.1299 - regression_loss: 1.6052 - classification_loss: 0.5247\n",
            "57/86 [==================>...........] - ETA: 29s - loss: 2.1272 - regression_loss: 1.6041 - classification_loss: 0.5232\n",
            "58/86 [===================>..........] - ETA: 28s - loss: 2.1256 - regression_loss: 1.6038 - classification_loss: 0.5218\n",
            "59/86 [===================>..........] - ETA: 27s - loss: 2.1243 - regression_loss: 1.6024 - classification_loss: 0.5219\n",
            "60/86 [===================>..........] - ETA: 26s - loss: 2.1246 - regression_loss: 1.6034 - classification_loss: 0.5213\n",
            "61/86 [====================>.........] - ETA: 25s - loss: 2.1279 - regression_loss: 1.6062 - classification_loss: 0.5217\n",
            "62/86 [====================>.........] - ETA: 24s - loss: 2.1286 - regression_loss: 1.6064 - classification_loss: 0.5222\n",
            "63/86 [====================>.........] - ETA: 23s - loss: 2.1285 - regression_loss: 1.6070 - classification_loss: 0.5215\n",
            "64/86 [=====================>........] - ETA: 22s - loss: 2.1273 - regression_loss: 1.6056 - classification_loss: 0.5218\n",
            "65/86 [=====================>........] - ETA: 21s - loss: 2.1263 - regression_loss: 1.6039 - classification_loss: 0.5225\n",
            "66/86 [======================>.......] - ETA: 20s - loss: 2.1280 - regression_loss: 1.6057 - classification_loss: 0.5224\n",
            "67/86 [======================>.......] - ETA: 19s - loss: 2.1275 - regression_loss: 1.6056 - classification_loss: 0.5219\n",
            "68/86 [======================>.......] - ETA: 18s - loss: 2.1311 - regression_loss: 1.6096 - classification_loss: 0.5215\n",
            "69/86 [=======================>......] - ETA: 17s - loss: 2.1310 - regression_loss: 1.6093 - classification_loss: 0.5218\n",
            "70/86 [=======================>......] - ETA: 16s - loss: 2.1321 - regression_loss: 1.6108 - classification_loss: 0.5213\n",
            "71/86 [=======================>......] - ETA: 15s - loss: 2.1325 - regression_loss: 1.6111 - classification_loss: 0.5214\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.1285 - regression_loss: 1.6080 - classification_loss: 0.5206\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.1294 - regression_loss: 1.6072 - classification_loss: 0.5222\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.1277 - regression_loss: 1.6059 - classification_loss: 0.5218\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.1272 - regression_loss: 1.6061 - classification_loss: 0.5210\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.1304 - regression_loss: 1.6086 - classification_loss: 0.5218\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.1310 - regression_loss: 1.6090 - classification_loss: 0.5220 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.1314 - regression_loss: 1.6098 - classification_loss: 0.5217\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.1338 - regression_loss: 1.6115 - classification_loss: 0.5224\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.1358 - regression_loss: 1.6133 - classification_loss: 0.5225\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1333 - regression_loss: 1.6114 - classification_loss: 0.5219\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1294 - regression_loss: 1.6079 - classification_loss: 0.5215\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1311 - regression_loss: 1.6096 - classification_loss: 0.5216\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1336 - regression_loss: 1.6109 - classification_loss: 0.5227\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1334 - regression_loss: 1.6107 - classification_loss: 0.5228\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1340 - regression_loss: 1.6112 - classification_loss: 0.5228\n",
            "Epoch 39: saving model to ./snapshots\\resnet50_csv_39.h5\n",
            "\n",
            "86/86 [==============================] - 87s 1s/step - loss: 2.1340 - regression_loss: 1.6112 - classification_loss: 0.5228 - lr: 1.0000e-16\n",
            "Epoch 40/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:34 - loss: 2.2443 - regression_loss: 1.7550 - classification_loss: 0.4894\n",
            " 2/86 [..............................] - ETA: 1:42 - loss: 2.1938 - regression_loss: 1.6754 - classification_loss: 0.5184\n",
            " 3/86 [>.............................] - ETA: 1:19 - loss: 2.0944 - regression_loss: 1.6140 - classification_loss: 0.4804\n",
            " 4/86 [>.............................] - ETA: 1:17 - loss: 2.1398 - regression_loss: 1.6445 - classification_loss: 0.4953\n",
            " 5/86 [>.............................] - ETA: 1:14 - loss: 2.1557 - regression_loss: 1.6544 - classification_loss: 0.5013\n",
            " 6/86 [=>............................] - ETA: 1:17 - loss: 2.0873 - regression_loss: 1.6039 - classification_loss: 0.4834\n",
            " 7/86 [=>............................] - ETA: 1:11 - loss: 2.1027 - regression_loss: 1.6215 - classification_loss: 0.4812\n",
            " 8/86 [=>............................] - ETA: 1:21 - loss: 2.1120 - regression_loss: 1.6212 - classification_loss: 0.4909\n",
            " 9/86 [==>...........................] - ETA: 1:29 - loss: 2.0891 - regression_loss: 1.6044 - classification_loss: 0.4847\n",
            "10/86 [==>...........................] - ETA: 1:30 - loss: 2.0919 - regression_loss: 1.6098 - classification_loss: 0.4821\n",
            "11/86 [==>...........................] - ETA: 1:24 - loss: 2.1290 - regression_loss: 1.6354 - classification_loss: 0.4936\n",
            "12/86 [===>..........................] - ETA: 1:26 - loss: 2.1206 - regression_loss: 1.6254 - classification_loss: 0.4952\n",
            "13/86 [===>..........................] - ETA: 1:22 - loss: 2.1281 - regression_loss: 1.6314 - classification_loss: 0.4968\n",
            "14/86 [===>..........................] - ETA: 1:22 - loss: 2.1131 - regression_loss: 1.6238 - classification_loss: 0.4893\n",
            "15/86 [====>.........................] - ETA: 1:18 - loss: 2.1020 - regression_loss: 1.6133 - classification_loss: 0.4887\n",
            "16/86 [====>.........................] - ETA: 1:19 - loss: 2.0962 - regression_loss: 1.6106 - classification_loss: 0.4856\n",
            "17/86 [====>.........................] - ETA: 1:17 - loss: 2.1048 - regression_loss: 1.6159 - classification_loss: 0.4889\n",
            "18/86 [=====>........................] - ETA: 1:16 - loss: 2.1186 - regression_loss: 1.6178 - classification_loss: 0.5008\n",
            "19/86 [=====>........................] - ETA: 1:13 - loss: 2.0857 - regression_loss: 1.5924 - classification_loss: 0.4933\n",
            "20/86 [=====>........................] - ETA: 1:11 - loss: 2.0842 - regression_loss: 1.5895 - classification_loss: 0.4947\n",
            "21/86 [======>.......................] - ETA: 1:10 - loss: 2.0805 - regression_loss: 1.5872 - classification_loss: 0.4933\n",
            "22/86 [======>.......................] - ETA: 1:09 - loss: 2.1042 - regression_loss: 1.6057 - classification_loss: 0.4985\n",
            "23/86 [=======>......................] - ETA: 1:07 - loss: 2.0995 - regression_loss: 1.5999 - classification_loss: 0.4996\n",
            "24/86 [=======>......................] - ETA: 1:07 - loss: 2.1057 - regression_loss: 1.6029 - classification_loss: 0.5028\n",
            "25/86 [=======>......................] - ETA: 1:06 - loss: 2.1052 - regression_loss: 1.6030 - classification_loss: 0.5021\n",
            "26/86 [========>.....................] - ETA: 1:03 - loss: 2.0936 - regression_loss: 1.5935 - classification_loss: 0.5001\n",
            "27/86 [========>.....................] - ETA: 1:03 - loss: 2.1102 - regression_loss: 1.6036 - classification_loss: 0.5066\n",
            "28/86 [========>.....................] - ETA: 1:02 - loss: 2.1156 - regression_loss: 1.6079 - classification_loss: 0.5076\n",
            "29/86 [=========>....................] - ETA: 1:00 - loss: 2.1195 - regression_loss: 1.6127 - classification_loss: 0.5067\n",
            "30/86 [=========>....................] - ETA: 58s - loss: 2.1170 - regression_loss: 1.6104 - classification_loss: 0.5066 \n",
            "31/86 [=========>....................] - ETA: 57s - loss: 2.1193 - regression_loss: 1.6123 - classification_loss: 0.5069\n",
            "32/86 [==========>...................] - ETA: 56s - loss: 2.1174 - regression_loss: 1.6107 - classification_loss: 0.5067\n",
            "33/86 [==========>...................] - ETA: 56s - loss: 2.1200 - regression_loss: 1.6124 - classification_loss: 0.5076\n",
            "34/86 [==========>...................] - ETA: 56s - loss: 2.1262 - regression_loss: 1.6178 - classification_loss: 0.5085\n",
            "35/86 [===========>..................] - ETA: 56s - loss: 2.1347 - regression_loss: 1.6231 - classification_loss: 0.5116\n",
            "36/86 [===========>..................] - ETA: 55s - loss: 2.1347 - regression_loss: 1.6221 - classification_loss: 0.5126\n",
            "37/86 [===========>..................] - ETA: 54s - loss: 2.1309 - regression_loss: 1.6192 - classification_loss: 0.5117\n",
            "38/86 [============>.................] - ETA: 53s - loss: 2.1275 - regression_loss: 1.6170 - classification_loss: 0.5104\n",
            "39/86 [============>.................] - ETA: 52s - loss: 2.1295 - regression_loss: 1.6178 - classification_loss: 0.5116\n",
            "40/86 [============>.................] - ETA: 50s - loss: 2.1293 - regression_loss: 1.6168 - classification_loss: 0.5124\n",
            "41/86 [=============>................] - ETA: 49s - loss: 2.1216 - regression_loss: 1.6113 - classification_loss: 0.5103\n",
            "42/86 [=============>................] - ETA: 47s - loss: 2.1184 - regression_loss: 1.6085 - classification_loss: 0.5099\n",
            "43/86 [==============>...............] - ETA: 46s - loss: 2.1132 - regression_loss: 1.6036 - classification_loss: 0.5096\n",
            "44/86 [==============>...............] - ETA: 45s - loss: 2.1142 - regression_loss: 1.6042 - classification_loss: 0.5100\n",
            "45/86 [==============>...............] - ETA: 44s - loss: 2.1067 - regression_loss: 1.5984 - classification_loss: 0.5083\n",
            "46/86 [===============>..............] - ETA: 43s - loss: 2.1096 - regression_loss: 1.6009 - classification_loss: 0.5087\n",
            "47/86 [===============>..............] - ETA: 42s - loss: 2.1119 - regression_loss: 1.6030 - classification_loss: 0.5089\n",
            "48/86 [===============>..............] - ETA: 41s - loss: 2.1175 - regression_loss: 1.6076 - classification_loss: 0.5099\n",
            "49/86 [================>.............] - ETA: 39s - loss: 2.1131 - regression_loss: 1.6037 - classification_loss: 0.5094\n",
            "50/86 [================>.............] - ETA: 38s - loss: 2.1142 - regression_loss: 1.6042 - classification_loss: 0.5099\n",
            "51/86 [================>.............] - ETA: 37s - loss: 2.1132 - regression_loss: 1.6033 - classification_loss: 0.5099\n",
            "52/86 [=================>............] - ETA: 36s - loss: 2.1146 - regression_loss: 1.6043 - classification_loss: 0.5103\n",
            "53/86 [=================>............] - ETA: 35s - loss: 2.1155 - regression_loss: 1.6051 - classification_loss: 0.5104\n",
            "54/86 [=================>............] - ETA: 33s - loss: 2.1146 - regression_loss: 1.6032 - classification_loss: 0.5114\n",
            "55/86 [==================>...........] - ETA: 32s - loss: 2.1160 - regression_loss: 1.6046 - classification_loss: 0.5115\n",
            "56/86 [==================>...........] - ETA: 31s - loss: 2.1169 - regression_loss: 1.6056 - classification_loss: 0.5112\n",
            "57/86 [==================>...........] - ETA: 30s - loss: 2.1127 - regression_loss: 1.6027 - classification_loss: 0.5100\n",
            "58/86 [===================>..........] - ETA: 29s - loss: 2.1137 - regression_loss: 1.6048 - classification_loss: 0.5089\n",
            "59/86 [===================>..........] - ETA: 28s - loss: 2.1146 - regression_loss: 1.6062 - classification_loss: 0.5084\n",
            "60/86 [===================>..........] - ETA: 27s - loss: 2.1129 - regression_loss: 1.6052 - classification_loss: 0.5077\n",
            "61/86 [====================>.........] - ETA: 26s - loss: 2.1085 - regression_loss: 1.6020 - classification_loss: 0.5065\n",
            "62/86 [====================>.........] - ETA: 25s - loss: 2.1126 - regression_loss: 1.6050 - classification_loss: 0.5076\n",
            "63/86 [====================>.........] - ETA: 24s - loss: 2.1097 - regression_loss: 1.6029 - classification_loss: 0.5069\n",
            "64/86 [=====================>........] - ETA: 23s - loss: 2.1066 - regression_loss: 1.5998 - classification_loss: 0.5069\n",
            "65/86 [=====================>........] - ETA: 21s - loss: 2.1019 - regression_loss: 1.5964 - classification_loss: 0.5055\n",
            "66/86 [======================>.......] - ETA: 20s - loss: 2.1004 - regression_loss: 1.5949 - classification_loss: 0.5055\n",
            "67/86 [======================>.......] - ETA: 19s - loss: 2.1012 - regression_loss: 1.5956 - classification_loss: 0.5056\n",
            "68/86 [======================>.......] - ETA: 18s - loss: 2.1005 - regression_loss: 1.5952 - classification_loss: 0.5052\n",
            "69/86 [=======================>......] - ETA: 17s - loss: 2.1039 - regression_loss: 1.5974 - classification_loss: 0.5065\n",
            "70/86 [=======================>......] - ETA: 16s - loss: 2.1055 - regression_loss: 1.5983 - classification_loss: 0.5073\n",
            "71/86 [=======================>......] - ETA: 15s - loss: 2.1057 - regression_loss: 1.5985 - classification_loss: 0.5072\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.1085 - regression_loss: 1.6015 - classification_loss: 0.5070\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.1051 - regression_loss: 1.5982 - classification_loss: 0.5070\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.1070 - regression_loss: 1.5991 - classification_loss: 0.5079\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.1066 - regression_loss: 1.5981 - classification_loss: 0.5085\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.1123 - regression_loss: 1.6026 - classification_loss: 0.5097\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.1185 - regression_loss: 1.6064 - classification_loss: 0.5121 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.1209 - regression_loss: 1.6078 - classification_loss: 0.5131\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.1236 - regression_loss: 1.6098 - classification_loss: 0.5138\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.1245 - regression_loss: 1.6097 - classification_loss: 0.5148\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.1246 - regression_loss: 1.6096 - classification_loss: 0.5150\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.1238 - regression_loss: 1.6090 - classification_loss: 0.5148\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.1277 - regression_loss: 1.6124 - classification_loss: 0.5153\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.1281 - regression_loss: 1.6127 - classification_loss: 0.5154\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.1243 - regression_loss: 1.6098 - classification_loss: 0.5145\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1235 - regression_loss: 1.6091 - classification_loss: 0.5144\n",
            "Epoch 40: saving model to ./snapshots\\resnet50_csv_40.h5\n",
            "\n",
            "Epoch 40: ReduceLROnPlateau reducing learning rate to 9.999998845134856e-18.\n",
            "\n",
            "86/86 [==============================] - 90s 1s/step - loss: 2.1235 - regression_loss: 1.6091 - classification_loss: 0.5144 - lr: 1.0000e-16\n",
            "Epoch 41/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:18 - loss: 1.8731 - regression_loss: 1.3915 - classification_loss: 0.4816\n",
            " 2/86 [..............................] - ETA: 1:38 - loss: 2.1000 - regression_loss: 1.5722 - classification_loss: 0.5278\n",
            " 3/86 [>.............................] - ETA: 1:34 - loss: 2.1631 - regression_loss: 1.6165 - classification_loss: 0.5466\n",
            " 4/86 [>.............................] - ETA: 1:25 - loss: 2.1853 - regression_loss: 1.6434 - classification_loss: 0.5419\n",
            " 5/86 [>.............................] - ETA: 1:22 - loss: 2.1771 - regression_loss: 1.6427 - classification_loss: 0.5344\n",
            " 6/86 [=>............................] - ETA: 1:18 - loss: 2.1777 - regression_loss: 1.6457 - classification_loss: 0.5321\n",
            " 7/86 [=>............................] - ETA: 1:17 - loss: 2.1406 - regression_loss: 1.6147 - classification_loss: 0.5259\n",
            " 8/86 [=>............................] - ETA: 1:14 - loss: 2.1015 - regression_loss: 1.5804 - classification_loss: 0.5211\n",
            " 9/86 [==>...........................] - ETA: 1:17 - loss: 2.1091 - regression_loss: 1.5836 - classification_loss: 0.5255\n",
            "10/86 [==>...........................] - ETA: 1:14 - loss: 2.0906 - regression_loss: 1.5686 - classification_loss: 0.5220\n",
            "11/86 [==>...........................] - ETA: 1:13 - loss: 2.0766 - regression_loss: 1.5582 - classification_loss: 0.5184\n",
            "12/86 [===>..........................] - ETA: 1:12 - loss: 2.0851 - regression_loss: 1.5646 - classification_loss: 0.5206\n",
            "13/86 [===>..........................] - ETA: 1:11 - loss: 2.0678 - regression_loss: 1.5521 - classification_loss: 0.5157\n",
            "14/86 [===>..........................] - ETA: 1:15 - loss: 2.0816 - regression_loss: 1.5659 - classification_loss: 0.5156\n",
            "15/86 [====>.........................] - ETA: 1:21 - loss: 2.0781 - regression_loss: 1.5611 - classification_loss: 0.5170\n",
            "16/86 [====>.........................] - ETA: 1:20 - loss: 2.0708 - regression_loss: 1.5571 - classification_loss: 0.5137\n",
            "17/86 [====>.........................] - ETA: 1:17 - loss: 2.0708 - regression_loss: 1.5556 - classification_loss: 0.5153\n",
            "18/86 [=====>........................] - ETA: 1:16 - loss: 2.0783 - regression_loss: 1.5638 - classification_loss: 0.5145\n",
            "19/86 [=====>........................] - ETA: 1:13 - loss: 2.0681 - regression_loss: 1.5543 - classification_loss: 0.5137\n",
            "20/86 [=====>........................] - ETA: 1:12 - loss: 2.0859 - regression_loss: 1.5697 - classification_loss: 0.5162\n",
            "21/86 [======>.......................] - ETA: 1:10 - loss: 2.0893 - regression_loss: 1.5723 - classification_loss: 0.5170\n",
            "22/86 [======>.......................] - ETA: 1:09 - loss: 2.0911 - regression_loss: 1.5755 - classification_loss: 0.5156\n",
            "23/86 [=======>......................] - ETA: 1:09 - loss: 2.0801 - regression_loss: 1.5670 - classification_loss: 0.5131\n",
            "24/86 [=======>......................] - ETA: 1:10 - loss: 2.0806 - regression_loss: 1.5676 - classification_loss: 0.5130\n",
            "25/86 [=======>......................] - ETA: 1:10 - loss: 2.0764 - regression_loss: 1.5641 - classification_loss: 0.5124\n",
            "26/86 [========>.....................] - ETA: 1:09 - loss: 2.0703 - regression_loss: 1.5589 - classification_loss: 0.5114\n",
            "27/86 [========>.....................] - ETA: 1:07 - loss: 2.0723 - regression_loss: 1.5598 - classification_loss: 0.5125\n",
            "28/86 [========>.....................] - ETA: 1:08 - loss: 2.0785 - regression_loss: 1.5660 - classification_loss: 0.5125\n",
            "29/86 [=========>....................] - ETA: 1:06 - loss: 2.0750 - regression_loss: 1.5631 - classification_loss: 0.5119\n",
            "30/86 [=========>....................] - ETA: 1:04 - loss: 2.0730 - regression_loss: 1.5612 - classification_loss: 0.5117\n",
            "31/86 [=========>....................] - ETA: 1:04 - loss: 2.0764 - regression_loss: 1.5641 - classification_loss: 0.5122\n",
            "32/86 [==========>...................] - ETA: 1:03 - loss: 2.0837 - regression_loss: 1.5694 - classification_loss: 0.5142\n",
            "33/86 [==========>...................] - ETA: 1:02 - loss: 2.0849 - regression_loss: 1.5712 - classification_loss: 0.5137\n",
            "34/86 [==========>...................] - ETA: 1:01 - loss: 2.0865 - regression_loss: 1.5714 - classification_loss: 0.5150\n",
            "35/86 [===========>..................] - ETA: 59s - loss: 2.1004 - regression_loss: 1.5819 - classification_loss: 0.5185 \n",
            "36/86 [===========>..................] - ETA: 58s - loss: 2.1078 - regression_loss: 1.5872 - classification_loss: 0.5206\n",
            "37/86 [===========>..................] - ETA: 57s - loss: 2.1032 - regression_loss: 1.5823 - classification_loss: 0.5209\n",
            "38/86 [============>.................] - ETA: 55s - loss: 2.0952 - regression_loss: 1.5768 - classification_loss: 0.5185\n",
            "39/86 [============>.................] - ETA: 54s - loss: 2.0985 - regression_loss: 1.5808 - classification_loss: 0.5176\n",
            "40/86 [============>.................] - ETA: 52s - loss: 2.1007 - regression_loss: 1.5845 - classification_loss: 0.5162\n",
            "41/86 [=============>................] - ETA: 51s - loss: 2.1033 - regression_loss: 1.5874 - classification_loss: 0.5159\n",
            "42/86 [=============>................] - ETA: 50s - loss: 2.1078 - regression_loss: 1.5893 - classification_loss: 0.5185\n",
            "43/86 [==============>...............] - ETA: 48s - loss: 2.1070 - regression_loss: 1.5883 - classification_loss: 0.5188\n",
            "44/86 [==============>...............] - ETA: 47s - loss: 2.1197 - regression_loss: 1.5987 - classification_loss: 0.5210\n",
            "45/86 [==============>...............] - ETA: 46s - loss: 2.1173 - regression_loss: 1.5982 - classification_loss: 0.5190\n",
            "46/86 [===============>..............] - ETA: 45s - loss: 2.1223 - regression_loss: 1.6029 - classification_loss: 0.5194\n",
            "47/86 [===============>..............] - ETA: 44s - loss: 2.1279 - regression_loss: 1.6081 - classification_loss: 0.5198\n",
            "48/86 [===============>..............] - ETA: 43s - loss: 2.1282 - regression_loss: 1.6093 - classification_loss: 0.5190\n",
            "49/86 [================>.............] - ETA: 41s - loss: 2.1301 - regression_loss: 1.6109 - classification_loss: 0.5192\n",
            "50/86 [================>.............] - ETA: 40s - loss: 2.1223 - regression_loss: 1.6057 - classification_loss: 0.5166\n",
            "51/86 [================>.............] - ETA: 39s - loss: 2.1231 - regression_loss: 1.6061 - classification_loss: 0.5171\n",
            "52/86 [=================>............] - ETA: 38s - loss: 2.1162 - regression_loss: 1.6016 - classification_loss: 0.5146\n",
            "53/86 [=================>............] - ETA: 36s - loss: 2.1149 - regression_loss: 1.6001 - classification_loss: 0.5149\n",
            "54/86 [=================>............] - ETA: 35s - loss: 2.1167 - regression_loss: 1.6018 - classification_loss: 0.5149\n",
            "55/86 [==================>...........] - ETA: 34s - loss: 2.1207 - regression_loss: 1.6048 - classification_loss: 0.5159\n",
            "56/86 [==================>...........] - ETA: 32s - loss: 2.1157 - regression_loss: 1.6014 - classification_loss: 0.5143\n",
            "57/86 [==================>...........] - ETA: 31s - loss: 2.1153 - regression_loss: 1.6010 - classification_loss: 0.5143\n",
            "58/86 [===================>..........] - ETA: 30s - loss: 2.1136 - regression_loss: 1.5994 - classification_loss: 0.5142\n",
            "59/86 [===================>..........] - ETA: 29s - loss: 2.1169 - regression_loss: 1.6030 - classification_loss: 0.5139\n",
            "60/86 [===================>..........] - ETA: 28s - loss: 2.1138 - regression_loss: 1.6005 - classification_loss: 0.5133\n",
            "61/86 [====================>.........] - ETA: 27s - loss: 2.1143 - regression_loss: 1.6009 - classification_loss: 0.5134\n",
            "62/86 [====================>.........] - ETA: 26s - loss: 2.1226 - regression_loss: 1.6075 - classification_loss: 0.5151\n",
            "63/86 [====================>.........] - ETA: 24s - loss: 2.1163 - regression_loss: 1.6023 - classification_loss: 0.5140\n",
            "64/86 [=====================>........] - ETA: 23s - loss: 2.1143 - regression_loss: 1.6016 - classification_loss: 0.5127\n",
            "65/86 [=====================>........] - ETA: 23s - loss: 2.1119 - regression_loss: 1.6003 - classification_loss: 0.5116\n",
            "66/86 [======================>.......] - ETA: 22s - loss: 2.1117 - regression_loss: 1.6003 - classification_loss: 0.5114\n",
            "67/86 [======================>.......] - ETA: 20s - loss: 2.1150 - regression_loss: 1.6042 - classification_loss: 0.5108\n",
            "68/86 [======================>.......] - ETA: 19s - loss: 2.1185 - regression_loss: 1.6074 - classification_loss: 0.5111\n",
            "69/86 [=======================>......] - ETA: 18s - loss: 2.1175 - regression_loss: 1.6036 - classification_loss: 0.5139\n",
            "70/86 [=======================>......] - ETA: 17s - loss: 2.1157 - regression_loss: 1.6034 - classification_loss: 0.5123\n",
            "71/86 [=======================>......] - ETA: 16s - loss: 2.1188 - regression_loss: 1.6059 - classification_loss: 0.5129\n",
            "72/86 [========================>.....] - ETA: 15s - loss: 2.1147 - regression_loss: 1.6035 - classification_loss: 0.5112\n",
            "73/86 [========================>.....] - ETA: 14s - loss: 2.1157 - regression_loss: 1.6038 - classification_loss: 0.5119\n",
            "74/86 [========================>.....] - ETA: 13s - loss: 2.1157 - regression_loss: 1.6047 - classification_loss: 0.5110\n",
            "75/86 [=========================>....] - ETA: 12s - loss: 2.1143 - regression_loss: 1.6035 - classification_loss: 0.5108\n",
            "76/86 [=========================>....] - ETA: 11s - loss: 2.1152 - regression_loss: 1.6044 - classification_loss: 0.5108\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.1156 - regression_loss: 1.6049 - classification_loss: 0.5107 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.1178 - regression_loss: 1.6065 - classification_loss: 0.5112\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.1207 - regression_loss: 1.6076 - classification_loss: 0.5131\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.1205 - regression_loss: 1.6071 - classification_loss: 0.5134\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.1221 - regression_loss: 1.6080 - classification_loss: 0.5141\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.1220 - regression_loss: 1.6085 - classification_loss: 0.5135\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.1270 - regression_loss: 1.6127 - classification_loss: 0.5143\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.1251 - regression_loss: 1.6110 - classification_loss: 0.5141\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.1253 - regression_loss: 1.6115 - classification_loss: 0.5137\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1252 - regression_loss: 1.6110 - classification_loss: 0.5142\n",
            "Epoch 41: saving model to ./snapshots\\resnet50_csv_41.h5\n",
            "\n",
            "86/86 [==============================] - 96s 1s/step - loss: 2.1252 - regression_loss: 1.6110 - classification_loss: 0.5142 - lr: 1.0000e-17\n",
            "Epoch 42/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:19 - loss: 2.2068 - regression_loss: 1.6780 - classification_loss: 0.5288\n",
            " 2/86 [..............................] - ETA: 58s - loss: 1.9998 - regression_loss: 1.5098 - classification_loss: 0.4900 \n",
            " 3/86 [>.............................] - ETA: 1:10 - loss: 1.9695 - regression_loss: 1.4713 - classification_loss: 0.4982\n",
            " 4/86 [>.............................] - ETA: 1:51 - loss: 1.9958 - regression_loss: 1.4899 - classification_loss: 0.5059\n",
            " 5/86 [>.............................] - ETA: 1:42 - loss: 2.0276 - regression_loss: 1.5136 - classification_loss: 0.5140\n",
            " 6/86 [=>............................] - ETA: 1:35 - loss: 2.0675 - regression_loss: 1.5570 - classification_loss: 0.5105\n",
            " 7/86 [=>............................] - ETA: 1:31 - loss: 2.0744 - regression_loss: 1.5640 - classification_loss: 0.5104\n",
            " 8/86 [=>............................] - ETA: 1:28 - loss: 2.1009 - regression_loss: 1.5905 - classification_loss: 0.5104\n",
            " 9/86 [==>...........................] - ETA: 1:25 - loss: 2.0756 - regression_loss: 1.5632 - classification_loss: 0.5124\n",
            "10/86 [==>...........................] - ETA: 1:23 - loss: 2.0786 - regression_loss: 1.5640 - classification_loss: 0.5147\n",
            "11/86 [==>...........................] - ETA: 1:20 - loss: 2.1183 - regression_loss: 1.5947 - classification_loss: 0.5236\n",
            "12/86 [===>..........................] - ETA: 1:18 - loss: 2.0867 - regression_loss: 1.5726 - classification_loss: 0.5141\n",
            "13/86 [===>..........................] - ETA: 1:22 - loss: 2.0795 - regression_loss: 1.5690 - classification_loss: 0.5105\n",
            "14/86 [===>..........................] - ETA: 1:22 - loss: 2.0802 - regression_loss: 1.5690 - classification_loss: 0.5112\n",
            "15/86 [====>.........................] - ETA: 1:21 - loss: 2.0787 - regression_loss: 1.5669 - classification_loss: 0.5119\n",
            "16/86 [====>.........................] - ETA: 1:19 - loss: 2.0855 - regression_loss: 1.5661 - classification_loss: 0.5193\n",
            "17/86 [====>.........................] - ETA: 1:17 - loss: 2.0953 - regression_loss: 1.5755 - classification_loss: 0.5198\n",
            "18/86 [=====>........................] - ETA: 1:15 - loss: 2.0979 - regression_loss: 1.5777 - classification_loss: 0.5201\n",
            "19/86 [=====>........................] - ETA: 1:13 - loss: 2.0932 - regression_loss: 1.5763 - classification_loss: 0.5169\n",
            "20/86 [=====>........................] - ETA: 1:12 - loss: 2.1080 - regression_loss: 1.5866 - classification_loss: 0.5214\n",
            "21/86 [======>.......................] - ETA: 1:12 - loss: 2.1105 - regression_loss: 1.5891 - classification_loss: 0.5214\n",
            "22/86 [======>.......................] - ETA: 1:13 - loss: 2.1076 - regression_loss: 1.5864 - classification_loss: 0.5212\n",
            "23/86 [=======>......................] - ETA: 1:11 - loss: 2.1075 - regression_loss: 1.5872 - classification_loss: 0.5203\n",
            "24/86 [=======>......................] - ETA: 1:09 - loss: 2.1121 - regression_loss: 1.5903 - classification_loss: 0.5218\n",
            "25/86 [=======>......................] - ETA: 1:08 - loss: 2.1129 - regression_loss: 1.5916 - classification_loss: 0.5214\n",
            "26/86 [========>.....................] - ETA: 1:06 - loss: 2.1232 - regression_loss: 1.6009 - classification_loss: 0.5223\n",
            "27/86 [========>.....................] - ETA: 1:05 - loss: 2.1187 - regression_loss: 1.5978 - classification_loss: 0.5208\n",
            "28/86 [========>.....................] - ETA: 1:03 - loss: 2.1324 - regression_loss: 1.6054 - classification_loss: 0.5270\n",
            "29/86 [=========>....................] - ETA: 1:01 - loss: 2.1110 - regression_loss: 1.5904 - classification_loss: 0.5205\n",
            "30/86 [=========>....................] - ETA: 1:00 - loss: 2.1189 - regression_loss: 1.5966 - classification_loss: 0.5223\n",
            "31/86 [=========>....................] - ETA: 59s - loss: 2.1146 - regression_loss: 1.5931 - classification_loss: 0.5214 \n",
            "32/86 [==========>...................] - ETA: 58s - loss: 2.1131 - regression_loss: 1.5919 - classification_loss: 0.5212\n",
            "33/86 [==========>...................] - ETA: 57s - loss: 2.1292 - regression_loss: 1.6049 - classification_loss: 0.5243\n",
            "34/86 [==========>...................] - ETA: 56s - loss: 2.1265 - regression_loss: 1.6031 - classification_loss: 0.5234\n",
            "35/86 [===========>..................] - ETA: 54s - loss: 2.1250 - regression_loss: 1.6011 - classification_loss: 0.5239\n",
            "36/86 [===========>..................] - ETA: 54s - loss: 2.1269 - regression_loss: 1.6018 - classification_loss: 0.5251\n",
            "37/86 [===========>..................] - ETA: 52s - loss: 2.1323 - regression_loss: 1.6068 - classification_loss: 0.5255\n",
            "38/86 [============>.................] - ETA: 51s - loss: 2.1275 - regression_loss: 1.6030 - classification_loss: 0.5245\n",
            "39/86 [============>.................] - ETA: 50s - loss: 2.1280 - regression_loss: 1.6035 - classification_loss: 0.5246\n",
            "40/86 [============>.................] - ETA: 49s - loss: 2.1316 - regression_loss: 1.6057 - classification_loss: 0.5259\n",
            "41/86 [=============>................] - ETA: 48s - loss: 2.1412 - regression_loss: 1.6125 - classification_loss: 0.5287\n",
            "42/86 [=============>................] - ETA: 46s - loss: 2.1455 - regression_loss: 1.6169 - classification_loss: 0.5286\n",
            "43/86 [==============>...............] - ETA: 45s - loss: 2.1399 - regression_loss: 1.6143 - classification_loss: 0.5255\n",
            "44/86 [==============>...............] - ETA: 44s - loss: 2.1342 - regression_loss: 1.6095 - classification_loss: 0.5248\n",
            "45/86 [==============>...............] - ETA: 43s - loss: 2.1288 - regression_loss: 1.6066 - classification_loss: 0.5222\n",
            "46/86 [===============>..............] - ETA: 42s - loss: 2.1225 - regression_loss: 1.6017 - classification_loss: 0.5209\n",
            "47/86 [===============>..............] - ETA: 42s - loss: 2.1175 - regression_loss: 1.5982 - classification_loss: 0.5193\n",
            "48/86 [===============>..............] - ETA: 41s - loss: 2.1192 - regression_loss: 1.6003 - classification_loss: 0.5189\n",
            "49/86 [================>.............] - ETA: 40s - loss: 2.1207 - regression_loss: 1.6022 - classification_loss: 0.5186\n",
            "50/86 [================>.............] - ETA: 39s - loss: 2.1173 - regression_loss: 1.5997 - classification_loss: 0.5176\n",
            "51/86 [================>.............] - ETA: 38s - loss: 2.1151 - regression_loss: 1.5984 - classification_loss: 0.5166\n",
            "52/86 [=================>............] - ETA: 36s - loss: 2.1135 - regression_loss: 1.5970 - classification_loss: 0.5165\n",
            "53/86 [=================>............] - ETA: 35s - loss: 2.1108 - regression_loss: 1.5947 - classification_loss: 0.5161\n",
            "54/86 [=================>............] - ETA: 34s - loss: 2.1067 - regression_loss: 1.5920 - classification_loss: 0.5146\n",
            "55/86 [==================>...........] - ETA: 33s - loss: 2.1130 - regression_loss: 1.5964 - classification_loss: 0.5166\n",
            "56/86 [==================>...........] - ETA: 32s - loss: 2.1150 - regression_loss: 1.5982 - classification_loss: 0.5168\n",
            "57/86 [==================>...........] - ETA: 31s - loss: 2.1146 - regression_loss: 1.5987 - classification_loss: 0.5160\n",
            "58/86 [===================>..........] - ETA: 30s - loss: 2.1189 - regression_loss: 1.6006 - classification_loss: 0.5183\n",
            "59/86 [===================>..........] - ETA: 28s - loss: 2.1117 - regression_loss: 1.5955 - classification_loss: 0.5162\n",
            "60/86 [===================>..........] - ETA: 27s - loss: 2.1113 - regression_loss: 1.5957 - classification_loss: 0.5156\n",
            "61/86 [====================>.........] - ETA: 26s - loss: 2.1107 - regression_loss: 1.5952 - classification_loss: 0.5155\n",
            "62/86 [====================>.........] - ETA: 25s - loss: 2.1085 - regression_loss: 1.5935 - classification_loss: 0.5150\n",
            "63/86 [====================>.........] - ETA: 24s - loss: 2.1029 - regression_loss: 1.5896 - classification_loss: 0.5133\n",
            "64/86 [=====================>........] - ETA: 23s - loss: 2.1035 - regression_loss: 1.5902 - classification_loss: 0.5133\n",
            "65/86 [=====================>........] - ETA: 22s - loss: 2.0997 - regression_loss: 1.5874 - classification_loss: 0.5123\n",
            "66/86 [======================>.......] - ETA: 21s - loss: 2.0991 - regression_loss: 1.5864 - classification_loss: 0.5127\n",
            "67/86 [======================>.......] - ETA: 20s - loss: 2.0986 - regression_loss: 1.5853 - classification_loss: 0.5133\n",
            "68/86 [======================>.......] - ETA: 18s - loss: 2.0953 - regression_loss: 1.5835 - classification_loss: 0.5117\n",
            "69/86 [=======================>......] - ETA: 17s - loss: 2.0925 - regression_loss: 1.5820 - classification_loss: 0.5104\n",
            "70/86 [=======================>......] - ETA: 16s - loss: 2.0888 - regression_loss: 1.5788 - classification_loss: 0.5100\n",
            "71/86 [=======================>......] - ETA: 15s - loss: 2.0904 - regression_loss: 1.5794 - classification_loss: 0.5109\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.0919 - regression_loss: 1.5811 - classification_loss: 0.5108\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.0914 - regression_loss: 1.5818 - classification_loss: 0.5096\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.0883 - regression_loss: 1.5797 - classification_loss: 0.5086\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.0859 - regression_loss: 1.5780 - classification_loss: 0.5079\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.0867 - regression_loss: 1.5783 - classification_loss: 0.5085\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.0863 - regression_loss: 1.5782 - classification_loss: 0.5080 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.0867 - regression_loss: 1.5787 - classification_loss: 0.5080\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.0833 - regression_loss: 1.5756 - classification_loss: 0.5077\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.0848 - regression_loss: 1.5763 - classification_loss: 0.5085\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.0835 - regression_loss: 1.5748 - classification_loss: 0.5087\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.0856 - regression_loss: 1.5772 - classification_loss: 0.5084\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.0837 - regression_loss: 1.5761 - classification_loss: 0.5076\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.0867 - regression_loss: 1.5781 - classification_loss: 0.5086\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.0897 - regression_loss: 1.5802 - classification_loss: 0.5094\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0913 - regression_loss: 1.5822 - classification_loss: 0.5091\n",
            "Epoch 42: saving model to ./snapshots\\resnet50_csv_42.h5\n",
            "\n",
            "Epoch 42: ReduceLROnPlateau reducing learning rate to 9.999999010570977e-19.\n",
            "\n",
            "86/86 [==============================] - 91s 1s/step - loss: 2.0913 - regression_loss: 1.5822 - classification_loss: 0.5091 - lr: 1.0000e-17\n",
            "Epoch 43/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:29 - loss: 2.2553 - regression_loss: 1.6762 - classification_loss: 0.5790\n",
            " 2/86 [..............................] - ETA: 1:10 - loss: 2.1938 - regression_loss: 1.6635 - classification_loss: 0.5303\n",
            " 3/86 [>.............................] - ETA: 1:12 - loss: 2.1610 - regression_loss: 1.6299 - classification_loss: 0.5312\n",
            " 4/86 [>.............................] - ETA: 1:12 - loss: 2.1176 - regression_loss: 1.6028 - classification_loss: 0.5148\n",
            " 5/86 [>.............................] - ETA: 1:15 - loss: 1.9938 - regression_loss: 1.5075 - classification_loss: 0.4863\n",
            " 6/86 [=>............................] - ETA: 1:15 - loss: 2.0151 - regression_loss: 1.5256 - classification_loss: 0.4895\n",
            " 7/86 [=>............................] - ETA: 1:14 - loss: 1.9871 - regression_loss: 1.5041 - classification_loss: 0.4830\n",
            " 8/86 [=>............................] - ETA: 1:12 - loss: 1.9623 - regression_loss: 1.4751 - classification_loss: 0.4872\n",
            " 9/86 [==>...........................] - ETA: 1:13 - loss: 2.0186 - regression_loss: 1.5175 - classification_loss: 0.5011\n",
            "10/86 [==>...........................] - ETA: 1:18 - loss: 2.0056 - regression_loss: 1.5106 - classification_loss: 0.4950\n",
            "11/86 [==>...........................] - ETA: 1:15 - loss: 1.9802 - regression_loss: 1.4938 - classification_loss: 0.4864\n",
            "12/86 [===>..........................] - ETA: 1:14 - loss: 1.9752 - regression_loss: 1.4911 - classification_loss: 0.4841\n",
            "13/86 [===>..........................] - ETA: 1:12 - loss: 1.9817 - regression_loss: 1.4994 - classification_loss: 0.4823\n",
            "14/86 [===>..........................] - ETA: 1:11 - loss: 2.0059 - regression_loss: 1.5210 - classification_loss: 0.4849\n",
            "15/86 [====>.........................] - ETA: 1:12 - loss: 1.9873 - regression_loss: 1.5075 - classification_loss: 0.4798\n",
            "16/86 [====>.........................] - ETA: 1:11 - loss: 2.0020 - regression_loss: 1.5165 - classification_loss: 0.4855\n",
            "17/86 [====>.........................] - ETA: 1:09 - loss: 1.9967 - regression_loss: 1.5116 - classification_loss: 0.4851\n",
            "18/86 [=====>........................] - ETA: 1:11 - loss: 1.9914 - regression_loss: 1.5102 - classification_loss: 0.4812\n",
            "19/86 [=====>........................] - ETA: 1:09 - loss: 1.9796 - regression_loss: 1.5027 - classification_loss: 0.4768\n",
            "20/86 [=====>........................] - ETA: 1:09 - loss: 2.0012 - regression_loss: 1.5220 - classification_loss: 0.4792\n",
            "21/86 [======>.......................] - ETA: 1:07 - loss: 1.9980 - regression_loss: 1.5203 - classification_loss: 0.4777\n",
            "22/86 [======>.......................] - ETA: 1:06 - loss: 2.0024 - regression_loss: 1.5227 - classification_loss: 0.4796\n",
            "23/86 [=======>......................] - ETA: 1:06 - loss: 2.0130 - regression_loss: 1.5290 - classification_loss: 0.4840\n",
            "24/86 [=======>......................] - ETA: 1:04 - loss: 2.0074 - regression_loss: 1.5225 - classification_loss: 0.4849\n",
            "25/86 [=======>......................] - ETA: 1:03 - loss: 2.0110 - regression_loss: 1.5265 - classification_loss: 0.4846\n",
            "26/86 [========>.....................] - ETA: 1:04 - loss: 2.0288 - regression_loss: 1.5401 - classification_loss: 0.4887\n",
            "27/86 [========>.....................] - ETA: 1:03 - loss: 2.0398 - regression_loss: 1.5512 - classification_loss: 0.4886\n",
            "28/86 [========>.....................] - ETA: 1:01 - loss: 2.0460 - regression_loss: 1.5548 - classification_loss: 0.4912\n",
            "29/86 [=========>....................] - ETA: 1:00 - loss: 2.0510 - regression_loss: 1.5590 - classification_loss: 0.4920\n",
            "30/86 [=========>....................] - ETA: 58s - loss: 2.0552 - regression_loss: 1.5630 - classification_loss: 0.4922 \n",
            "31/86 [=========>....................] - ETA: 57s - loss: 2.0622 - regression_loss: 1.5700 - classification_loss: 0.4922\n",
            "32/86 [==========>...................] - ETA: 55s - loss: 2.0624 - regression_loss: 1.5676 - classification_loss: 0.4948\n",
            "33/86 [==========>...................] - ETA: 54s - loss: 2.0642 - regression_loss: 1.5687 - classification_loss: 0.4956\n",
            "34/86 [==========>...................] - ETA: 53s - loss: 2.0622 - regression_loss: 1.5682 - classification_loss: 0.4940\n",
            "35/86 [===========>..................] - ETA: 51s - loss: 2.0600 - regression_loss: 1.5671 - classification_loss: 0.4929\n",
            "36/86 [===========>..................] - ETA: 50s - loss: 2.0633 - regression_loss: 1.5697 - classification_loss: 0.4936\n",
            "37/86 [===========>..................] - ETA: 49s - loss: 2.0634 - regression_loss: 1.5697 - classification_loss: 0.4938\n",
            "38/86 [============>.................] - ETA: 48s - loss: 2.0638 - regression_loss: 1.5700 - classification_loss: 0.4938\n",
            "39/86 [============>.................] - ETA: 47s - loss: 2.0756 - regression_loss: 1.5793 - classification_loss: 0.4963\n",
            "40/86 [============>.................] - ETA: 46s - loss: 2.0764 - regression_loss: 1.5746 - classification_loss: 0.5018\n",
            "41/86 [=============>................] - ETA: 45s - loss: 2.0807 - regression_loss: 1.5774 - classification_loss: 0.5034\n",
            "42/86 [=============>................] - ETA: 44s - loss: 2.0769 - regression_loss: 1.5762 - classification_loss: 0.5007\n",
            "43/86 [==============>...............] - ETA: 43s - loss: 2.0759 - regression_loss: 1.5765 - classification_loss: 0.4994\n",
            "44/86 [==============>...............] - ETA: 42s - loss: 2.0749 - regression_loss: 1.5763 - classification_loss: 0.4986\n",
            "45/86 [==============>...............] - ETA: 41s - loss: 2.0727 - regression_loss: 1.5739 - classification_loss: 0.4988\n",
            "46/86 [===============>..............] - ETA: 40s - loss: 2.0781 - regression_loss: 1.5787 - classification_loss: 0.4994\n",
            "47/86 [===============>..............] - ETA: 39s - loss: 2.0805 - regression_loss: 1.5803 - classification_loss: 0.5002\n",
            "48/86 [===============>..............] - ETA: 38s - loss: 2.0804 - regression_loss: 1.5802 - classification_loss: 0.5002\n",
            "49/86 [================>.............] - ETA: 37s - loss: 2.0841 - regression_loss: 1.5839 - classification_loss: 0.5002\n",
            "50/86 [================>.............] - ETA: 36s - loss: 2.0876 - regression_loss: 1.5852 - classification_loss: 0.5023\n",
            "51/86 [================>.............] - ETA: 35s - loss: 2.0886 - regression_loss: 1.5863 - classification_loss: 0.5023\n",
            "52/86 [=================>............] - ETA: 34s - loss: 2.0846 - regression_loss: 1.5834 - classification_loss: 0.5012\n",
            "53/86 [=================>............] - ETA: 33s - loss: 2.0883 - regression_loss: 1.5855 - classification_loss: 0.5028\n",
            "54/86 [=================>............] - ETA: 32s - loss: 2.0871 - regression_loss: 1.5850 - classification_loss: 0.5021\n",
            "55/86 [==================>...........] - ETA: 31s - loss: 2.0885 - regression_loss: 1.5855 - classification_loss: 0.5030\n",
            "56/86 [==================>...........] - ETA: 30s - loss: 2.0891 - regression_loss: 1.5846 - classification_loss: 0.5045\n",
            "57/86 [==================>...........] - ETA: 29s - loss: 2.0876 - regression_loss: 1.5837 - classification_loss: 0.5039\n",
            "58/86 [===================>..........] - ETA: 28s - loss: 2.0903 - regression_loss: 1.5840 - classification_loss: 0.5063\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.0874 - regression_loss: 1.5823 - classification_loss: 0.5051\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.0836 - regression_loss: 1.5784 - classification_loss: 0.5052\n",
            "61/86 [====================>.........] - ETA: 25s - loss: 2.0872 - regression_loss: 1.5825 - classification_loss: 0.5047\n",
            "62/86 [====================>.........] - ETA: 24s - loss: 2.0808 - regression_loss: 1.5777 - classification_loss: 0.5031\n",
            "63/86 [====================>.........] - ETA: 23s - loss: 2.0821 - regression_loss: 1.5785 - classification_loss: 0.5036\n",
            "64/86 [=====================>........] - ETA: 22s - loss: 2.0809 - regression_loss: 1.5774 - classification_loss: 0.5035\n",
            "65/86 [=====================>........] - ETA: 21s - loss: 2.0861 - regression_loss: 1.5810 - classification_loss: 0.5051\n",
            "66/86 [======================>.......] - ETA: 20s - loss: 2.0827 - regression_loss: 1.5787 - classification_loss: 0.5040\n",
            "67/86 [======================>.......] - ETA: 19s - loss: 2.0828 - regression_loss: 1.5789 - classification_loss: 0.5039\n",
            "68/86 [======================>.......] - ETA: 18s - loss: 2.0852 - regression_loss: 1.5803 - classification_loss: 0.5049\n",
            "69/86 [=======================>......] - ETA: 17s - loss: 2.0845 - regression_loss: 1.5799 - classification_loss: 0.5046\n",
            "70/86 [=======================>......] - ETA: 16s - loss: 2.0831 - regression_loss: 1.5788 - classification_loss: 0.5043\n",
            "71/86 [=======================>......] - ETA: 15s - loss: 2.0854 - regression_loss: 1.5806 - classification_loss: 0.5048\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.0823 - regression_loss: 1.5786 - classification_loss: 0.5037\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.0822 - regression_loss: 1.5781 - classification_loss: 0.5041\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.0857 - regression_loss: 1.5803 - classification_loss: 0.5053\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.0835 - regression_loss: 1.5789 - classification_loss: 0.5046\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.0809 - regression_loss: 1.5759 - classification_loss: 0.5050\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.0802 - regression_loss: 1.5751 - classification_loss: 0.5050 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.0797 - regression_loss: 1.5746 - classification_loss: 0.5052\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.0827 - regression_loss: 1.5768 - classification_loss: 0.5059\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.0808 - regression_loss: 1.5753 - classification_loss: 0.5055\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.0831 - regression_loss: 1.5774 - classification_loss: 0.5058\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.0854 - regression_loss: 1.5796 - classification_loss: 0.5059\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.0782 - regression_loss: 1.5744 - classification_loss: 0.5039\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.0782 - regression_loss: 1.5751 - classification_loss: 0.5031\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0788 - regression_loss: 1.5758 - classification_loss: 0.5030\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0765 - regression_loss: 1.5738 - classification_loss: 0.5027\n",
            "Epoch 43: saving model to ./snapshots\\resnet50_csv_43.h5\n",
            "\n",
            "86/86 [==============================] - 88s 1s/step - loss: 2.0765 - regression_loss: 1.5738 - classification_loss: 0.5027 - lr: 1.0000e-18\n",
            "Epoch 44/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:31 - loss: 2.2047 - regression_loss: 1.6117 - classification_loss: 0.5930\n",
            " 2/86 [..............................] - ETA: 1:46 - loss: 2.2861 - regression_loss: 1.6594 - classification_loss: 0.6267\n",
            " 3/86 [>.............................] - ETA: 1:39 - loss: 2.1947 - regression_loss: 1.6167 - classification_loss: 0.5780\n",
            " 4/86 [>.............................] - ETA: 1:28 - loss: 2.1117 - regression_loss: 1.5583 - classification_loss: 0.5534\n",
            " 5/86 [>.............................] - ETA: 1:21 - loss: 2.0974 - regression_loss: 1.5539 - classification_loss: 0.5436\n",
            " 6/86 [=>............................] - ETA: 1:35 - loss: 2.1246 - regression_loss: 1.5900 - classification_loss: 0.5346\n",
            " 7/86 [=>............................] - ETA: 1:32 - loss: 2.1203 - regression_loss: 1.5846 - classification_loss: 0.5357\n",
            " 8/86 [=>............................] - ETA: 1:25 - loss: 2.1351 - regression_loss: 1.6046 - classification_loss: 0.5305\n",
            " 9/86 [==>...........................] - ETA: 1:22 - loss: 2.1478 - regression_loss: 1.6182 - classification_loss: 0.5296\n",
            "10/86 [==>...........................] - ETA: 1:18 - loss: 2.1278 - regression_loss: 1.5999 - classification_loss: 0.5279\n",
            "11/86 [==>...........................] - ETA: 1:20 - loss: 2.1186 - regression_loss: 1.5959 - classification_loss: 0.5227\n",
            "12/86 [===>..........................] - ETA: 1:16 - loss: 2.1143 - regression_loss: 1.5923 - classification_loss: 0.5221\n",
            "13/86 [===>..........................] - ETA: 1:16 - loss: 2.0689 - regression_loss: 1.5579 - classification_loss: 0.5110\n",
            "14/86 [===>..........................] - ETA: 1:14 - loss: 2.0596 - regression_loss: 1.5422 - classification_loss: 0.5174\n",
            "15/86 [====>.........................] - ETA: 1:16 - loss: 2.0810 - regression_loss: 1.5580 - classification_loss: 0.5230\n",
            "16/86 [====>.........................] - ETA: 1:14 - loss: 2.0766 - regression_loss: 1.5522 - classification_loss: 0.5244\n",
            "17/86 [====>.........................] - ETA: 1:12 - loss: 2.0828 - regression_loss: 1.5587 - classification_loss: 0.5241\n",
            "18/86 [=====>........................] - ETA: 1:10 - loss: 2.0663 - regression_loss: 1.5456 - classification_loss: 0.5206\n",
            "19/86 [=====>........................] - ETA: 1:10 - loss: 2.0774 - regression_loss: 1.5585 - classification_loss: 0.5189\n",
            "20/86 [=====>........................] - ETA: 1:08 - loss: 2.0762 - regression_loss: 1.5562 - classification_loss: 0.5201\n",
            "21/86 [======>.......................] - ETA: 1:07 - loss: 2.0805 - regression_loss: 1.5591 - classification_loss: 0.5214\n",
            "22/86 [======>.......................] - ETA: 1:05 - loss: 2.0732 - regression_loss: 1.5543 - classification_loss: 0.5189\n",
            "23/86 [=======>......................] - ETA: 1:06 - loss: 2.0704 - regression_loss: 1.5508 - classification_loss: 0.5196\n",
            "24/86 [=======>......................] - ETA: 1:06 - loss: 2.0653 - regression_loss: 1.5479 - classification_loss: 0.5174\n",
            "25/86 [=======>......................] - ETA: 1:04 - loss: 2.0600 - regression_loss: 1.5445 - classification_loss: 0.5155\n",
            "26/86 [========>.....................] - ETA: 1:03 - loss: 2.0662 - regression_loss: 1.5518 - classification_loss: 0.5144\n",
            "27/86 [========>.....................] - ETA: 1:03 - loss: 2.0784 - regression_loss: 1.5639 - classification_loss: 0.5145\n",
            "28/86 [========>.....................] - ETA: 1:01 - loss: 2.0771 - regression_loss: 1.5617 - classification_loss: 0.5154\n",
            "29/86 [=========>....................] - ETA: 1:01 - loss: 2.0818 - regression_loss: 1.5668 - classification_loss: 0.5149\n",
            "30/86 [=========>....................] - ETA: 59s - loss: 2.0844 - regression_loss: 1.5695 - classification_loss: 0.5149 \n",
            "31/86 [=========>....................] - ETA: 57s - loss: 2.0834 - regression_loss: 1.5686 - classification_loss: 0.5148\n",
            "32/86 [==========>...................] - ETA: 56s - loss: 2.0821 - regression_loss: 1.5681 - classification_loss: 0.5141\n",
            "33/86 [==========>...................] - ETA: 55s - loss: 2.0795 - regression_loss: 1.5665 - classification_loss: 0.5131\n",
            "34/86 [==========>...................] - ETA: 54s - loss: 2.0822 - regression_loss: 1.5699 - classification_loss: 0.5123\n",
            "35/86 [===========>..................] - ETA: 52s - loss: 2.0840 - regression_loss: 1.5712 - classification_loss: 0.5128\n",
            "36/86 [===========>..................] - ETA: 52s - loss: 2.0847 - regression_loss: 1.5732 - classification_loss: 0.5115\n",
            "37/86 [===========>..................] - ETA: 50s - loss: 2.0742 - regression_loss: 1.5660 - classification_loss: 0.5082\n",
            "38/86 [============>.................] - ETA: 50s - loss: 2.0775 - regression_loss: 1.5678 - classification_loss: 0.5097\n",
            "39/86 [============>.................] - ETA: 48s - loss: 2.0779 - regression_loss: 1.5692 - classification_loss: 0.5087\n",
            "40/86 [============>.................] - ETA: 48s - loss: 2.0866 - regression_loss: 1.5754 - classification_loss: 0.5112\n",
            "41/86 [=============>................] - ETA: 47s - loss: 2.0860 - regression_loss: 1.5747 - classification_loss: 0.5113\n",
            "42/86 [=============>................] - ETA: 46s - loss: 2.0830 - regression_loss: 1.5711 - classification_loss: 0.5118\n",
            "43/86 [==============>...............] - ETA: 45s - loss: 2.0867 - regression_loss: 1.5744 - classification_loss: 0.5123\n",
            "44/86 [==============>...............] - ETA: 44s - loss: 2.0898 - regression_loss: 1.5748 - classification_loss: 0.5150\n",
            "45/86 [==============>...............] - ETA: 43s - loss: 2.0854 - regression_loss: 1.5705 - classification_loss: 0.5149\n",
            "46/86 [===============>..............] - ETA: 41s - loss: 2.0836 - regression_loss: 1.5703 - classification_loss: 0.5133\n",
            "47/86 [===============>..............] - ETA: 40s - loss: 2.0865 - regression_loss: 1.5728 - classification_loss: 0.5137\n",
            "48/86 [===============>..............] - ETA: 39s - loss: 2.0848 - regression_loss: 1.5719 - classification_loss: 0.5129\n",
            "49/86 [================>.............] - ETA: 38s - loss: 2.0927 - regression_loss: 1.5781 - classification_loss: 0.5146\n",
            "50/86 [================>.............] - ETA: 37s - loss: 2.0967 - regression_loss: 1.5815 - classification_loss: 0.5152\n",
            "51/86 [================>.............] - ETA: 36s - loss: 2.0911 - regression_loss: 1.5766 - classification_loss: 0.5145\n",
            "52/86 [=================>............] - ETA: 35s - loss: 2.0853 - regression_loss: 1.5722 - classification_loss: 0.5132\n",
            "53/86 [=================>............] - ETA: 34s - loss: 2.0906 - regression_loss: 1.5767 - classification_loss: 0.5140\n",
            "54/86 [=================>............] - ETA: 33s - loss: 2.0938 - regression_loss: 1.5789 - classification_loss: 0.5149\n",
            "55/86 [==================>...........] - ETA: 32s - loss: 2.0900 - regression_loss: 1.5769 - classification_loss: 0.5131\n",
            "56/86 [==================>...........] - ETA: 31s - loss: 2.0836 - regression_loss: 1.5720 - classification_loss: 0.5115\n",
            "57/86 [==================>...........] - ETA: 30s - loss: 2.0884 - regression_loss: 1.5775 - classification_loss: 0.5109\n",
            "58/86 [===================>..........] - ETA: 29s - loss: 2.0910 - regression_loss: 1.5787 - classification_loss: 0.5122\n",
            "59/86 [===================>..........] - ETA: 28s - loss: 2.0877 - regression_loss: 1.5755 - classification_loss: 0.5122\n",
            "60/86 [===================>..........] - ETA: 27s - loss: 2.0899 - regression_loss: 1.5778 - classification_loss: 0.5121\n",
            "61/86 [====================>.........] - ETA: 26s - loss: 2.0879 - regression_loss: 1.5772 - classification_loss: 0.5107\n",
            "62/86 [====================>.........] - ETA: 25s - loss: 2.0917 - regression_loss: 1.5807 - classification_loss: 0.5110\n",
            "63/86 [====================>.........] - ETA: 24s - loss: 2.0905 - regression_loss: 1.5799 - classification_loss: 0.5107\n",
            "64/86 [=====================>........] - ETA: 22s - loss: 2.0893 - regression_loss: 1.5790 - classification_loss: 0.5103\n",
            "65/86 [=====================>........] - ETA: 22s - loss: 2.0891 - regression_loss: 1.5788 - classification_loss: 0.5103\n",
            "66/86 [======================>.......] - ETA: 21s - loss: 2.0881 - regression_loss: 1.5788 - classification_loss: 0.5093\n",
            "67/86 [======================>.......] - ETA: 20s - loss: 2.0898 - regression_loss: 1.5806 - classification_loss: 0.5092\n",
            "68/86 [======================>.......] - ETA: 18s - loss: 2.0934 - regression_loss: 1.5841 - classification_loss: 0.5094\n",
            "69/86 [=======================>......] - ETA: 17s - loss: 2.0909 - regression_loss: 1.5819 - classification_loss: 0.5090\n",
            "70/86 [=======================>......] - ETA: 16s - loss: 2.0934 - regression_loss: 1.5838 - classification_loss: 0.5097\n",
            "71/86 [=======================>......] - ETA: 15s - loss: 2.0901 - regression_loss: 1.5813 - classification_loss: 0.5088\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.0963 - regression_loss: 1.5866 - classification_loss: 0.5097\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.0966 - regression_loss: 1.5866 - classification_loss: 0.5100\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.0926 - regression_loss: 1.5824 - classification_loss: 0.5102\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.0932 - regression_loss: 1.5827 - classification_loss: 0.5105\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.0988 - regression_loss: 1.5869 - classification_loss: 0.5119\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.0971 - regression_loss: 1.5855 - classification_loss: 0.5115 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.1016 - regression_loss: 1.5894 - classification_loss: 0.5121\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.1055 - regression_loss: 1.5915 - classification_loss: 0.5140\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.1054 - regression_loss: 1.5910 - classification_loss: 0.5144\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.1053 - regression_loss: 1.5902 - classification_loss: 0.5151\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.1033 - regression_loss: 1.5889 - classification_loss: 0.5144\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.1036 - regression_loss: 1.5885 - classification_loss: 0.5151\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.1063 - regression_loss: 1.5903 - classification_loss: 0.5160\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.1049 - regression_loss: 1.5893 - classification_loss: 0.5156\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1067 - regression_loss: 1.5910 - classification_loss: 0.5157\n",
            "Epoch 44: saving model to ./snapshots\\resnet50_csv_44.h5\n",
            "\n",
            "Epoch 44: ReduceLROnPlateau reducing learning rate to 9.999999424161285e-20.\n",
            "\n",
            "86/86 [==============================] - 91s 1s/step - loss: 2.1067 - regression_loss: 1.5910 - classification_loss: 0.5157 - lr: 1.0000e-18\n",
            "Epoch 45/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:03 - loss: 1.9603 - regression_loss: 1.4875 - classification_loss: 0.4727\n",
            " 2/86 [..............................] - ETA: 1:24 - loss: 1.9020 - regression_loss: 1.4319 - classification_loss: 0.4701\n",
            " 3/86 [>.............................] - ETA: 1:26 - loss: 1.9831 - regression_loss: 1.4727 - classification_loss: 0.5105\n",
            " 4/86 [>.............................] - ETA: 2:13 - loss: 2.0365 - regression_loss: 1.5307 - classification_loss: 0.5059\n",
            " 5/86 [>.............................] - ETA: 1:56 - loss: 2.0215 - regression_loss: 1.5304 - classification_loss: 0.4911\n",
            " 6/86 [=>............................] - ETA: 1:50 - loss: 2.0091 - regression_loss: 1.5112 - classification_loss: 0.4979\n",
            " 7/86 [=>............................] - ETA: 1:40 - loss: 1.9874 - regression_loss: 1.4902 - classification_loss: 0.4972\n",
            " 8/86 [=>............................] - ETA: 1:35 - loss: 1.9917 - regression_loss: 1.4998 - classification_loss: 0.4920\n",
            " 9/86 [==>...........................] - ETA: 1:31 - loss: 2.0012 - regression_loss: 1.5108 - classification_loss: 0.4904\n",
            "10/86 [==>...........................] - ETA: 1:30 - loss: 2.0080 - regression_loss: 1.5136 - classification_loss: 0.4944\n",
            "11/86 [==>...........................] - ETA: 1:27 - loss: 2.0326 - regression_loss: 1.5344 - classification_loss: 0.4981\n",
            "12/86 [===>..........................] - ETA: 1:29 - loss: 2.0104 - regression_loss: 1.5184 - classification_loss: 0.4920\n",
            "13/86 [===>..........................] - ETA: 1:28 - loss: 2.0213 - regression_loss: 1.5276 - classification_loss: 0.4937\n",
            "14/86 [===>..........................] - ETA: 1:26 - loss: 2.0047 - regression_loss: 1.5162 - classification_loss: 0.4885\n",
            "15/86 [====>.........................] - ETA: 1:23 - loss: 2.0075 - regression_loss: 1.5174 - classification_loss: 0.4901\n",
            "16/86 [====>.........................] - ETA: 1:20 - loss: 2.0267 - regression_loss: 1.5321 - classification_loss: 0.4946\n",
            "17/86 [====>.........................] - ETA: 1:18 - loss: 2.0284 - regression_loss: 1.5329 - classification_loss: 0.4955\n",
            "18/86 [=====>........................] - ETA: 1:16 - loss: 2.0289 - regression_loss: 1.5314 - classification_loss: 0.4976\n",
            "19/86 [=====>........................] - ETA: 1:14 - loss: 2.0291 - regression_loss: 1.5317 - classification_loss: 0.4975\n",
            "20/86 [=====>........................] - ETA: 1:12 - loss: 2.0259 - regression_loss: 1.5284 - classification_loss: 0.4976\n",
            "21/86 [======>.......................] - ETA: 1:11 - loss: 2.0280 - regression_loss: 1.5201 - classification_loss: 0.5079\n",
            "22/86 [======>.......................] - ETA: 1:09 - loss: 2.0142 - regression_loss: 1.5122 - classification_loss: 0.5020\n",
            "23/86 [=======>......................] - ETA: 1:08 - loss: 2.0213 - regression_loss: 1.5210 - classification_loss: 0.5003\n",
            "24/86 [=======>......................] - ETA: 1:06 - loss: 2.0255 - regression_loss: 1.5231 - classification_loss: 0.5024\n",
            "25/86 [=======>......................] - ETA: 1:05 - loss: 2.0381 - regression_loss: 1.5321 - classification_loss: 0.5060\n",
            "26/86 [========>.....................] - ETA: 1:03 - loss: 2.0402 - regression_loss: 1.5338 - classification_loss: 0.5064\n",
            "27/86 [========>.....................] - ETA: 1:02 - loss: 2.0482 - regression_loss: 1.5425 - classification_loss: 0.5056\n",
            "28/86 [========>.....................] - ETA: 1:01 - loss: 2.0554 - regression_loss: 1.5482 - classification_loss: 0.5072\n",
            "29/86 [=========>....................] - ETA: 59s - loss: 2.0531 - regression_loss: 1.5464 - classification_loss: 0.5067 \n",
            "30/86 [=========>....................] - ETA: 59s - loss: 2.0587 - regression_loss: 1.5518 - classification_loss: 0.5070\n",
            "31/86 [=========>....................] - ETA: 58s - loss: 2.0567 - regression_loss: 1.5512 - classification_loss: 0.5055\n",
            "32/86 [==========>...................] - ETA: 56s - loss: 2.0611 - regression_loss: 1.5559 - classification_loss: 0.5053\n",
            "33/86 [==========>...................] - ETA: 55s - loss: 2.0657 - regression_loss: 1.5583 - classification_loss: 0.5074\n",
            "34/86 [==========>...................] - ETA: 54s - loss: 2.0680 - regression_loss: 1.5592 - classification_loss: 0.5087\n",
            "35/86 [===========>..................] - ETA: 53s - loss: 2.0679 - regression_loss: 1.5582 - classification_loss: 0.5096\n",
            "36/86 [===========>..................] - ETA: 51s - loss: 2.0690 - regression_loss: 1.5591 - classification_loss: 0.5098\n",
            "37/86 [===========>..................] - ETA: 51s - loss: 2.0675 - regression_loss: 1.5581 - classification_loss: 0.5094\n",
            "38/86 [============>.................] - ETA: 49s - loss: 2.0619 - regression_loss: 1.5540 - classification_loss: 0.5080\n",
            "39/86 [============>.................] - ETA: 48s - loss: 2.0636 - regression_loss: 1.5550 - classification_loss: 0.5086\n",
            "40/86 [============>.................] - ETA: 47s - loss: 2.0700 - regression_loss: 1.5595 - classification_loss: 0.5106\n",
            "41/86 [=============>................] - ETA: 46s - loss: 2.0593 - regression_loss: 1.5519 - classification_loss: 0.5074\n",
            "42/86 [=============>................] - ETA: 45s - loss: 2.0559 - regression_loss: 1.5500 - classification_loss: 0.5059\n",
            "43/86 [==============>...............] - ETA: 44s - loss: 2.0571 - regression_loss: 1.5507 - classification_loss: 0.5064\n",
            "44/86 [==============>...............] - ETA: 43s - loss: 2.0518 - regression_loss: 1.5463 - classification_loss: 0.5055\n",
            "45/86 [==============>...............] - ETA: 42s - loss: 2.0639 - regression_loss: 1.5559 - classification_loss: 0.5080\n",
            "46/86 [===============>..............] - ETA: 41s - loss: 2.0686 - regression_loss: 1.5592 - classification_loss: 0.5094\n",
            "47/86 [===============>..............] - ETA: 40s - loss: 2.0676 - regression_loss: 1.5587 - classification_loss: 0.5089\n",
            "48/86 [===============>..............] - ETA: 39s - loss: 2.0726 - regression_loss: 1.5628 - classification_loss: 0.5098\n",
            "49/86 [================>.............] - ETA: 38s - loss: 2.0737 - regression_loss: 1.5631 - classification_loss: 0.5105\n",
            "50/86 [================>.............] - ETA: 37s - loss: 2.0763 - regression_loss: 1.5646 - classification_loss: 0.5117\n",
            "51/86 [================>.............] - ETA: 35s - loss: 2.0711 - regression_loss: 1.5608 - classification_loss: 0.5103\n",
            "52/86 [=================>............] - ETA: 34s - loss: 2.0670 - regression_loss: 1.5586 - classification_loss: 0.5084\n",
            "53/86 [=================>............] - ETA: 34s - loss: 2.0663 - regression_loss: 1.5586 - classification_loss: 0.5076\n",
            "54/86 [=================>............] - ETA: 33s - loss: 2.0636 - regression_loss: 1.5573 - classification_loss: 0.5063\n",
            "55/86 [==================>...........] - ETA: 32s - loss: 2.0675 - regression_loss: 1.5599 - classification_loss: 0.5075\n",
            "56/86 [==================>...........] - ETA: 31s - loss: 2.0726 - regression_loss: 1.5648 - classification_loss: 0.5078\n",
            "57/86 [==================>...........] - ETA: 30s - loss: 2.0751 - regression_loss: 1.5675 - classification_loss: 0.5077\n",
            "58/86 [===================>..........] - ETA: 29s - loss: 2.0814 - regression_loss: 1.5704 - classification_loss: 0.5110\n",
            "59/86 [===================>..........] - ETA: 27s - loss: 2.0792 - regression_loss: 1.5685 - classification_loss: 0.5107\n",
            "60/86 [===================>..........] - ETA: 26s - loss: 2.0759 - regression_loss: 1.5649 - classification_loss: 0.5109\n",
            "61/86 [====================>.........] - ETA: 25s - loss: 2.0761 - regression_loss: 1.5643 - classification_loss: 0.5118\n",
            "62/86 [====================>.........] - ETA: 24s - loss: 2.0714 - regression_loss: 1.5613 - classification_loss: 0.5100\n",
            "63/86 [====================>.........] - ETA: 23s - loss: 2.0754 - regression_loss: 1.5647 - classification_loss: 0.5107\n",
            "64/86 [=====================>........] - ETA: 22s - loss: 2.0793 - regression_loss: 1.5673 - classification_loss: 0.5120\n",
            "65/86 [=====================>........] - ETA: 21s - loss: 2.0851 - regression_loss: 1.5713 - classification_loss: 0.5139\n",
            "66/86 [======================>.......] - ETA: 20s - loss: 2.0855 - regression_loss: 1.5711 - classification_loss: 0.5144\n",
            "67/86 [======================>.......] - ETA: 19s - loss: 2.0809 - regression_loss: 1.5675 - classification_loss: 0.5133\n",
            "68/86 [======================>.......] - ETA: 18s - loss: 2.0799 - regression_loss: 1.5670 - classification_loss: 0.5129\n",
            "69/86 [=======================>......] - ETA: 17s - loss: 2.0809 - regression_loss: 1.5678 - classification_loss: 0.5130\n",
            "70/86 [=======================>......] - ETA: 16s - loss: 2.0842 - regression_loss: 1.5692 - classification_loss: 0.5149\n",
            "71/86 [=======================>......] - ETA: 15s - loss: 2.0852 - regression_loss: 1.5697 - classification_loss: 0.5155\n",
            "72/86 [========================>.....] - ETA: 14s - loss: 2.0831 - regression_loss: 1.5680 - classification_loss: 0.5151\n",
            "73/86 [========================>.....] - ETA: 13s - loss: 2.0846 - regression_loss: 1.5694 - classification_loss: 0.5153\n",
            "74/86 [========================>.....] - ETA: 12s - loss: 2.0831 - regression_loss: 1.5684 - classification_loss: 0.5147\n",
            "75/86 [=========================>....] - ETA: 11s - loss: 2.0870 - regression_loss: 1.5711 - classification_loss: 0.5158\n",
            "76/86 [=========================>....] - ETA: 10s - loss: 2.0856 - regression_loss: 1.5708 - classification_loss: 0.5147\n",
            "77/86 [=========================>....] - ETA: 9s - loss: 2.0818 - regression_loss: 1.5686 - classification_loss: 0.5132 \n",
            "78/86 [==========================>...] - ETA: 8s - loss: 2.0829 - regression_loss: 1.5694 - classification_loss: 0.5135\n",
            "79/86 [==========================>...] - ETA: 7s - loss: 2.0836 - regression_loss: 1.5701 - classification_loss: 0.5135\n",
            "80/86 [==========================>...] - ETA: 6s - loss: 2.0819 - regression_loss: 1.5692 - classification_loss: 0.5127\n",
            "81/86 [===========================>..] - ETA: 5s - loss: 2.0830 - regression_loss: 1.5700 - classification_loss: 0.5130\n",
            "82/86 [===========================>..] - ETA: 4s - loss: 2.0833 - regression_loss: 1.5706 - classification_loss: 0.5126\n",
            "83/86 [===========================>..] - ETA: 3s - loss: 2.0793 - regression_loss: 1.5672 - classification_loss: 0.5121\n",
            "84/86 [============================>.] - ETA: 2s - loss: 2.0766 - regression_loss: 1.5648 - classification_loss: 0.5117\n",
            "85/86 [============================>.] - ETA: 1s - loss: 2.0746 - regression_loss: 1.5636 - classification_loss: 0.5110\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0750 - regression_loss: 1.5642 - classification_loss: 0.5108\n",
            "Epoch 45: saving model to ./snapshots\\resnet50_csv_45.h5\n",
            "\n",
            "86/86 [==============================] - 90s 1s/step - loss: 2.0750 - regression_loss: 1.5642 - classification_loss: 0.5108 - lr: 1.0000e-19\n",
            "Epoch 46/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:24 - loss: 2.0863 - regression_loss: 1.5758 - classification_loss: 0.5105\n",
            " 2/86 [..............................] - ETA: 1:11 - loss: 2.2932 - regression_loss: 1.7438 - classification_loss: 0.5494\n",
            " 3/86 [>.............................] - ETA: 1:03 - loss: 2.3451 - regression_loss: 1.7746 - classification_loss: 0.5705\n",
            " 4/86 [>.............................] - ETA: 1:15 - loss: 2.3246 - regression_loss: 1.7356 - classification_loss: 0.5891\n",
            " 5/86 [>.............................] - ETA: 1:13 - loss: 2.3159 - regression_loss: 1.7288 - classification_loss: 0.5872\n",
            " 6/86 [=>............................] - ETA: 1:12 - loss: 2.2667 - regression_loss: 1.6962 - classification_loss: 0.5705\n",
            " 7/86 [=>............................] - ETA: 1:10 - loss: 2.3009 - regression_loss: 1.7223 - classification_loss: 0.5786\n",
            " 8/86 [=>............................] - ETA: 1:08 - loss: 2.2690 - regression_loss: 1.7000 - classification_loss: 0.5690\n",
            " 9/86 [==>...........................] - ETA: 1:07 - loss: 2.2173 - regression_loss: 1.6664 - classification_loss: 0.5509\n",
            "10/86 [==>...........................] - ETA: 1:08 - loss: 2.1819 - regression_loss: 1.6442 - classification_loss: 0.5378\n",
            "11/86 [==>...........................] - ETA: 1:05 - loss: 2.1418 - regression_loss: 1.6134 - classification_loss: 0.5284\n",
            "12/86 [===>..........................] - ETA: 1:08 - loss: 2.1454 - regression_loss: 1.6190 - classification_loss: 0.5264\n",
            "13/86 [===>..........................] - ETA: 1:07 - loss: 2.1440 - regression_loss: 1.6197 - classification_loss: 0.5243\n",
            "14/86 [===>..........................] - ETA: 1:05 - loss: 2.1155 - regression_loss: 1.6012 - classification_loss: 0.5143\n",
            "15/86 [====>.........................] - ETA: 1:05 - loss: 2.1357 - regression_loss: 1.6194 - classification_loss: 0.5163\n",
            "16/86 [====>.........................] - ETA: 1:05 - loss: 2.1364 - regression_loss: 1.6207 - classification_loss: 0.5157\n",
            "17/86 [====>.........................] - ETA: 1:04 - loss: 2.1393 - regression_loss: 1.6226 - classification_loss: 0.5167\n",
            "18/86 [=====>........................] - ETA: 1:03 - loss: 2.1608 - regression_loss: 1.6356 - classification_loss: 0.5252\n",
            "19/86 [=====>........................] - ETA: 1:02 - loss: 2.1574 - regression_loss: 1.6337 - classification_loss: 0.5237\n",
            "20/86 [=====>........................] - ETA: 1:01 - loss: 2.1364 - regression_loss: 1.6189 - classification_loss: 0.5175\n",
            "21/86 [======>.......................] - ETA: 1:00 - loss: 2.1320 - regression_loss: 1.6149 - classification_loss: 0.5171\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.1390 - regression_loss: 1.6202 - classification_loss: 0.5188\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.1404 - regression_loss: 1.6214 - classification_loss: 0.5190\n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.1373 - regression_loss: 1.6169 - classification_loss: 0.5205 \n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.1364 - regression_loss: 1.6163 - classification_loss: 0.5202\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.1368 - regression_loss: 1.6163 - classification_loss: 0.5205\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.1402 - regression_loss: 1.6171 - classification_loss: 0.5230\n",
            "28/86 [========>.....................] - ETA: 56s - loss: 2.1360 - regression_loss: 1.6143 - classification_loss: 0.5217\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.1329 - regression_loss: 1.6120 - classification_loss: 0.5209\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.1346 - regression_loss: 1.6135 - classification_loss: 0.5211\n",
            "31/86 [=========>....................] - ETA: 53s - loss: 2.1421 - regression_loss: 1.6194 - classification_loss: 0.5227\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.1493 - regression_loss: 1.6276 - classification_loss: 0.5217\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.1394 - regression_loss: 1.6202 - classification_loss: 0.5193\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.1480 - regression_loss: 1.6255 - classification_loss: 0.5226\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.1492 - regression_loss: 1.6276 - classification_loss: 0.5216\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.1566 - regression_loss: 1.6332 - classification_loss: 0.5234\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.1611 - regression_loss: 1.6365 - classification_loss: 0.5246\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.1602 - regression_loss: 1.6361 - classification_loss: 0.5241\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.1586 - regression_loss: 1.6334 - classification_loss: 0.5251\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.1603 - regression_loss: 1.6350 - classification_loss: 0.5253\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.1658 - regression_loss: 1.6401 - classification_loss: 0.5257\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.1614 - regression_loss: 1.6361 - classification_loss: 0.5253\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.1560 - regression_loss: 1.6318 - classification_loss: 0.5243\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.1616 - regression_loss: 1.6367 - classification_loss: 0.5249\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.1630 - regression_loss: 1.6380 - classification_loss: 0.5249\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.1670 - regression_loss: 1.6408 - classification_loss: 0.5262\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1678 - regression_loss: 1.6422 - classification_loss: 0.5255\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1680 - regression_loss: 1.6431 - classification_loss: 0.5250\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1661 - regression_loss: 1.6417 - classification_loss: 0.5244\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1687 - regression_loss: 1.6444 - classification_loss: 0.5243\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1719 - regression_loss: 1.6450 - classification_loss: 0.5269\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1755 - regression_loss: 1.6475 - classification_loss: 0.5281\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1730 - regression_loss: 1.6451 - classification_loss: 0.5279\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1751 - regression_loss: 1.6480 - classification_loss: 0.5271\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1716 - regression_loss: 1.6461 - classification_loss: 0.5255\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1687 - regression_loss: 1.6435 - classification_loss: 0.5252\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1749 - regression_loss: 1.6486 - classification_loss: 0.5263\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1735 - regression_loss: 1.6477 - classification_loss: 0.5257\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1707 - regression_loss: 1.6449 - classification_loss: 0.5258\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1707 - regression_loss: 1.6452 - classification_loss: 0.5255\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1696 - regression_loss: 1.6439 - classification_loss: 0.5256\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1766 - regression_loss: 1.6498 - classification_loss: 0.5268\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1775 - regression_loss: 1.6502 - classification_loss: 0.5272\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1755 - regression_loss: 1.6480 - classification_loss: 0.5274\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1732 - regression_loss: 1.6459 - classification_loss: 0.5273\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.1675 - regression_loss: 1.6414 - classification_loss: 0.5262\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.1651 - regression_loss: 1.6399 - classification_loss: 0.5252\n",
            "68/86 [======================>.......] - ETA: 16s - loss: 2.1598 - regression_loss: 1.6357 - classification_loss: 0.5241\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1589 - regression_loss: 1.6360 - classification_loss: 0.5229\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1567 - regression_loss: 1.6345 - classification_loss: 0.5223\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1528 - regression_loss: 1.6314 - classification_loss: 0.5214\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1523 - regression_loss: 1.6299 - classification_loss: 0.5223\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1498 - regression_loss: 1.6281 - classification_loss: 0.5217\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1554 - regression_loss: 1.6314 - classification_loss: 0.5241\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1566 - regression_loss: 1.6329 - classification_loss: 0.5238\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1560 - regression_loss: 1.6328 - classification_loss: 0.5231 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1546 - regression_loss: 1.6317 - classification_loss: 0.5228\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1543 - regression_loss: 1.6314 - classification_loss: 0.5229\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1527 - regression_loss: 1.6300 - classification_loss: 0.5227\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1503 - regression_loss: 1.6282 - classification_loss: 0.5221\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1488 - regression_loss: 1.6274 - classification_loss: 0.5215\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1496 - regression_loss: 1.6287 - classification_loss: 0.5210\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1485 - regression_loss: 1.6278 - classification_loss: 0.5207\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1518 - regression_loss: 1.6302 - classification_loss: 0.5216\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1508 - regression_loss: 1.6288 - classification_loss: 0.5220\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1461 - regression_loss: 1.6253 - classification_loss: 0.5208\n",
            "Epoch 46: saving model to ./snapshots\\resnet50_csv_46.h5\n",
            "\n",
            "Epoch 46: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-21.\n",
            "\n",
            "86/86 [==============================] - 82s 947ms/step - loss: 2.1461 - regression_loss: 1.6253 - classification_loss: 0.5208 - lr: 1.0000e-19\n",
            "Epoch 47/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:24 - loss: 2.2588 - regression_loss: 1.7471 - classification_loss: 0.5117\n",
            " 2/86 [..............................] - ETA: 1:20 - loss: 2.0690 - regression_loss: 1.5731 - classification_loss: 0.4959\n",
            " 3/86 [>.............................] - ETA: 1:18 - loss: 2.1501 - regression_loss: 1.6350 - classification_loss: 0.5150\n",
            " 4/86 [>.............................] - ETA: 1:19 - loss: 2.1095 - regression_loss: 1.5958 - classification_loss: 0.5138\n",
            " 5/86 [>.............................] - ETA: 1:30 - loss: 2.1036 - regression_loss: 1.6030 - classification_loss: 0.5006\n",
            " 6/86 [=>............................] - ETA: 1:30 - loss: 2.1010 - regression_loss: 1.6130 - classification_loss: 0.4880\n",
            " 7/86 [=>............................] - ETA: 1:27 - loss: 2.1118 - regression_loss: 1.6230 - classification_loss: 0.4888\n",
            " 8/86 [=>............................] - ETA: 1:24 - loss: 2.1145 - regression_loss: 1.6200 - classification_loss: 0.4945\n",
            " 9/86 [==>...........................] - ETA: 1:26 - loss: 2.1210 - regression_loss: 1.6266 - classification_loss: 0.4944\n",
            "10/86 [==>...........................] - ETA: 1:23 - loss: 2.0927 - regression_loss: 1.6042 - classification_loss: 0.4885\n",
            "11/86 [==>...........................] - ETA: 1:21 - loss: 2.0686 - regression_loss: 1.5804 - classification_loss: 0.4882\n",
            "12/86 [===>..........................] - ETA: 1:19 - loss: 2.0625 - regression_loss: 1.5709 - classification_loss: 0.4916\n",
            "13/86 [===>..........................] - ETA: 1:20 - loss: 2.0566 - regression_loss: 1.5622 - classification_loss: 0.4944\n",
            "14/86 [===>..........................] - ETA: 1:18 - loss: 2.0595 - regression_loss: 1.5666 - classification_loss: 0.4929\n",
            "15/86 [====>.........................] - ETA: 1:16 - loss: 2.0568 - regression_loss: 1.5615 - classification_loss: 0.4953\n",
            "16/86 [====>.........................] - ETA: 1:16 - loss: 2.0620 - regression_loss: 1.5672 - classification_loss: 0.4948\n",
            "17/86 [====>.........................] - ETA: 1:13 - loss: 2.0519 - regression_loss: 1.5590 - classification_loss: 0.4929\n",
            "18/86 [=====>........................] - ETA: 1:12 - loss: 2.0811 - regression_loss: 1.5827 - classification_loss: 0.4984\n",
            "19/86 [=====>........................] - ETA: 1:11 - loss: 2.0878 - regression_loss: 1.5905 - classification_loss: 0.4972\n",
            "20/86 [=====>........................] - ETA: 1:09 - loss: 2.0818 - regression_loss: 1.5867 - classification_loss: 0.4950\n",
            "21/86 [======>.......................] - ETA: 1:09 - loss: 2.0823 - regression_loss: 1.5846 - classification_loss: 0.4977\n",
            "22/86 [======>.......................] - ETA: 1:08 - loss: 2.0795 - regression_loss: 1.5819 - classification_loss: 0.4976\n",
            "23/86 [=======>......................] - ETA: 1:07 - loss: 2.0795 - regression_loss: 1.5824 - classification_loss: 0.4971\n",
            "24/86 [=======>......................] - ETA: 1:05 - loss: 2.0858 - regression_loss: 1.5889 - classification_loss: 0.4969\n",
            "25/86 [=======>......................] - ETA: 1:04 - loss: 2.0935 - regression_loss: 1.5921 - classification_loss: 0.5015\n",
            "26/86 [========>.....................] - ETA: 1:02 - loss: 2.0951 - regression_loss: 1.5928 - classification_loss: 0.5023\n",
            "27/86 [========>.....................] - ETA: 1:01 - loss: 2.1018 - regression_loss: 1.5974 - classification_loss: 0.5044\n",
            "28/86 [========>.....................] - ETA: 1:00 - loss: 2.1019 - regression_loss: 1.5961 - classification_loss: 0.5058\n",
            "29/86 [=========>....................] - ETA: 59s - loss: 2.1026 - regression_loss: 1.5960 - classification_loss: 0.5066 \n",
            "30/86 [=========>....................] - ETA: 57s - loss: 2.0981 - regression_loss: 1.5926 - classification_loss: 0.5055\n",
            "31/86 [=========>....................] - ETA: 56s - loss: 2.1006 - regression_loss: 1.5938 - classification_loss: 0.5068\n",
            "32/86 [==========>...................] - ETA: 55s - loss: 2.0951 - regression_loss: 1.5917 - classification_loss: 0.5034\n",
            "33/86 [==========>...................] - ETA: 54s - loss: 2.1088 - regression_loss: 1.6013 - classification_loss: 0.5075\n",
            "34/86 [==========>...................] - ETA: 52s - loss: 2.1083 - regression_loss: 1.6004 - classification_loss: 0.5080\n",
            "35/86 [===========>..................] - ETA: 51s - loss: 2.1068 - regression_loss: 1.5972 - classification_loss: 0.5096\n",
            "36/86 [===========>..................] - ETA: 50s - loss: 2.1113 - regression_loss: 1.5997 - classification_loss: 0.5116\n",
            "37/86 [===========>..................] - ETA: 49s - loss: 2.1105 - regression_loss: 1.5995 - classification_loss: 0.5111\n",
            "38/86 [============>.................] - ETA: 48s - loss: 2.1025 - regression_loss: 1.5946 - classification_loss: 0.5080\n",
            "39/86 [============>.................] - ETA: 47s - loss: 2.1055 - regression_loss: 1.5972 - classification_loss: 0.5082\n",
            "40/86 [============>.................] - ETA: 45s - loss: 2.1069 - regression_loss: 1.5986 - classification_loss: 0.5084\n",
            "41/86 [=============>................] - ETA: 44s - loss: 2.1130 - regression_loss: 1.6033 - classification_loss: 0.5097\n",
            "42/86 [=============>................] - ETA: 43s - loss: 2.1181 - regression_loss: 1.6064 - classification_loss: 0.5117\n",
            "43/86 [==============>...............] - ETA: 43s - loss: 2.1199 - regression_loss: 1.6078 - classification_loss: 0.5121\n",
            "44/86 [==============>...............] - ETA: 41s - loss: 2.1255 - regression_loss: 1.6126 - classification_loss: 0.5129\n",
            "45/86 [==============>...............] - ETA: 40s - loss: 2.1153 - regression_loss: 1.6046 - classification_loss: 0.5107\n",
            "46/86 [===============>..............] - ETA: 39s - loss: 2.1134 - regression_loss: 1.6031 - classification_loss: 0.5103\n",
            "47/86 [===============>..............] - ETA: 38s - loss: 2.1026 - regression_loss: 1.5951 - classification_loss: 0.5075\n",
            "48/86 [===============>..............] - ETA: 37s - loss: 2.0996 - regression_loss: 1.5918 - classification_loss: 0.5078\n",
            "49/86 [================>.............] - ETA: 36s - loss: 2.1037 - regression_loss: 1.5943 - classification_loss: 0.5095\n",
            "50/86 [================>.............] - ETA: 35s - loss: 2.1039 - regression_loss: 1.5941 - classification_loss: 0.5098\n",
            "51/86 [================>.............] - ETA: 34s - loss: 2.1023 - regression_loss: 1.5924 - classification_loss: 0.5099\n",
            "52/86 [=================>............] - ETA: 33s - loss: 2.1003 - regression_loss: 1.5910 - classification_loss: 0.5093\n",
            "53/86 [=================>............] - ETA: 32s - loss: 2.0937 - regression_loss: 1.5855 - classification_loss: 0.5082\n",
            "54/86 [=================>............] - ETA: 31s - loss: 2.0941 - regression_loss: 1.5862 - classification_loss: 0.5079\n",
            "55/86 [==================>...........] - ETA: 30s - loss: 2.0952 - regression_loss: 1.5873 - classification_loss: 0.5079\n",
            "56/86 [==================>...........] - ETA: 29s - loss: 2.0997 - regression_loss: 1.5891 - classification_loss: 0.5106\n",
            "57/86 [==================>...........] - ETA: 28s - loss: 2.1006 - regression_loss: 1.5878 - classification_loss: 0.5128\n",
            "58/86 [===================>..........] - ETA: 27s - loss: 2.1039 - regression_loss: 1.5907 - classification_loss: 0.5133\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.1026 - regression_loss: 1.5899 - classification_loss: 0.5127\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.0997 - regression_loss: 1.5875 - classification_loss: 0.5122\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.1051 - regression_loss: 1.5916 - classification_loss: 0.5135\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.1035 - regression_loss: 1.5901 - classification_loss: 0.5134\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.1018 - regression_loss: 1.5891 - classification_loss: 0.5128\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.1044 - regression_loss: 1.5912 - classification_loss: 0.5132\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1051 - regression_loss: 1.5925 - classification_loss: 0.5126\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1058 - regression_loss: 1.5930 - classification_loss: 0.5128\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1056 - regression_loss: 1.5926 - classification_loss: 0.5130\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1055 - regression_loss: 1.5924 - classification_loss: 0.5131\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1076 - regression_loss: 1.5936 - classification_loss: 0.5140\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1083 - regression_loss: 1.5947 - classification_loss: 0.5136\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1071 - regression_loss: 1.5938 - classification_loss: 0.5133\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1060 - regression_loss: 1.5930 - classification_loss: 0.5130\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1074 - regression_loss: 1.5942 - classification_loss: 0.5131\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1072 - regression_loss: 1.5936 - classification_loss: 0.5135\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1025 - regression_loss: 1.5898 - classification_loss: 0.5128\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0980 - regression_loss: 1.5867 - classification_loss: 0.5113 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0979 - regression_loss: 1.5860 - classification_loss: 0.5119\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1018 - regression_loss: 1.5889 - classification_loss: 0.5129\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1047 - regression_loss: 1.5916 - classification_loss: 0.5131\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1071 - regression_loss: 1.5937 - classification_loss: 0.5134\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1070 - regression_loss: 1.5934 - classification_loss: 0.5136\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1001 - regression_loss: 1.5882 - classification_loss: 0.5120\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0985 - regression_loss: 1.5864 - classification_loss: 0.5121\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0991 - regression_loss: 1.5868 - classification_loss: 0.5124\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0984 - regression_loss: 1.5862 - classification_loss: 0.5122\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0988 - regression_loss: 1.5869 - classification_loss: 0.5119\n",
            "Epoch 47: saving model to ./snapshots\\resnet50_csv_47.h5\n",
            "\n",
            "86/86 [==============================] - 84s 973ms/step - loss: 2.0988 - regression_loss: 1.5869 - classification_loss: 0.5119 - lr: 1.0000e-20\n",
            "Epoch 48/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:31 - loss: 2.1088 - regression_loss: 1.5446 - classification_loss: 0.5642\n",
            " 2/86 [..............................] - ETA: 1:08 - loss: 2.0867 - regression_loss: 1.5489 - classification_loss: 0.5378\n",
            " 3/86 [>.............................] - ETA: 1:09 - loss: 2.1626 - regression_loss: 1.6258 - classification_loss: 0.5368\n",
            " 4/86 [>.............................] - ETA: 1:10 - loss: 2.1611 - regression_loss: 1.6343 - classification_loss: 0.5268\n",
            " 5/86 [>.............................] - ETA: 1:11 - loss: 2.1024 - regression_loss: 1.5829 - classification_loss: 0.5195\n",
            " 6/86 [=>............................] - ETA: 1:11 - loss: 2.0525 - regression_loss: 1.5501 - classification_loss: 0.5024\n",
            " 7/86 [=>............................] - ETA: 1:09 - loss: 2.0579 - regression_loss: 1.5561 - classification_loss: 0.5017\n",
            " 8/86 [=>............................] - ETA: 1:09 - loss: 2.0532 - regression_loss: 1.5517 - classification_loss: 0.5016\n",
            " 9/86 [==>...........................] - ETA: 1:10 - loss: 2.0635 - regression_loss: 1.5596 - classification_loss: 0.5039\n",
            "10/86 [==>...........................] - ETA: 1:08 - loss: 2.0451 - regression_loss: 1.5442 - classification_loss: 0.5009\n",
            "11/86 [==>...........................] - ETA: 1:08 - loss: 2.0540 - regression_loss: 1.5464 - classification_loss: 0.5076\n",
            "12/86 [===>..........................] - ETA: 1:06 - loss: 2.0749 - regression_loss: 1.5616 - classification_loss: 0.5133\n",
            "13/86 [===>..........................] - ETA: 1:08 - loss: 2.0749 - regression_loss: 1.5634 - classification_loss: 0.5115\n",
            "14/86 [===>..........................] - ETA: 1:06 - loss: 2.0924 - regression_loss: 1.5772 - classification_loss: 0.5152\n",
            "15/86 [====>.........................] - ETA: 1:05 - loss: 2.0859 - regression_loss: 1.5723 - classification_loss: 0.5136\n",
            "16/86 [====>.........................] - ETA: 1:05 - loss: 2.0987 - regression_loss: 1.5814 - classification_loss: 0.5173\n",
            "17/86 [====>.........................] - ETA: 1:04 - loss: 2.0900 - regression_loss: 1.5764 - classification_loss: 0.5136\n",
            "18/86 [=====>........................] - ETA: 1:02 - loss: 2.1048 - regression_loss: 1.5889 - classification_loss: 0.5160\n",
            "19/86 [=====>........................] - ETA: 1:01 - loss: 2.1034 - regression_loss: 1.5844 - classification_loss: 0.5190\n",
            "20/86 [=====>........................] - ETA: 1:01 - loss: 2.1170 - regression_loss: 1.5961 - classification_loss: 0.5210\n",
            "21/86 [======>.......................] - ETA: 1:00 - loss: 2.1225 - regression_loss: 1.6015 - classification_loss: 0.5210\n",
            "22/86 [======>.......................] - ETA: 59s - loss: 2.1272 - regression_loss: 1.6056 - classification_loss: 0.5216 \n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.1170 - regression_loss: 1.5996 - classification_loss: 0.5174\n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.1185 - regression_loss: 1.5993 - classification_loss: 0.5192\n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.1163 - regression_loss: 1.5977 - classification_loss: 0.5186\n",
            "26/86 [========>.....................] - ETA: 56s - loss: 2.1191 - regression_loss: 1.6009 - classification_loss: 0.5182\n",
            "27/86 [========>.....................] - ETA: 55s - loss: 2.1197 - regression_loss: 1.6015 - classification_loss: 0.5182\n",
            "28/86 [========>.....................] - ETA: 54s - loss: 2.1180 - regression_loss: 1.6001 - classification_loss: 0.5179\n",
            "29/86 [=========>....................] - ETA: 52s - loss: 2.1144 - regression_loss: 1.5965 - classification_loss: 0.5179\n",
            "30/86 [=========>....................] - ETA: 51s - loss: 2.1092 - regression_loss: 1.5912 - classification_loss: 0.5181\n",
            "31/86 [=========>....................] - ETA: 50s - loss: 2.0999 - regression_loss: 1.5847 - classification_loss: 0.5152\n",
            "32/86 [==========>...................] - ETA: 50s - loss: 2.1069 - regression_loss: 1.5929 - classification_loss: 0.5140\n",
            "33/86 [==========>...................] - ETA: 48s - loss: 2.1023 - regression_loss: 1.5893 - classification_loss: 0.5130\n",
            "34/86 [==========>...................] - ETA: 48s - loss: 2.1056 - regression_loss: 1.5931 - classification_loss: 0.5125\n",
            "35/86 [===========>..................] - ETA: 47s - loss: 2.1037 - regression_loss: 1.5926 - classification_loss: 0.5111\n",
            "36/86 [===========>..................] - ETA: 46s - loss: 2.1010 - regression_loss: 1.5902 - classification_loss: 0.5107\n",
            "37/86 [===========>..................] - ETA: 45s - loss: 2.1028 - regression_loss: 1.5899 - classification_loss: 0.5129\n",
            "38/86 [============>.................] - ETA: 44s - loss: 2.1035 - regression_loss: 1.5906 - classification_loss: 0.5129\n",
            "39/86 [============>.................] - ETA: 43s - loss: 2.0993 - regression_loss: 1.5869 - classification_loss: 0.5125\n",
            "40/86 [============>.................] - ETA: 42s - loss: 2.0985 - regression_loss: 1.5845 - classification_loss: 0.5141\n",
            "41/86 [=============>................] - ETA: 41s - loss: 2.0915 - regression_loss: 1.5788 - classification_loss: 0.5127\n",
            "42/86 [=============>................] - ETA: 40s - loss: 2.0838 - regression_loss: 1.5727 - classification_loss: 0.5111\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.0808 - regression_loss: 1.5724 - classification_loss: 0.5084\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.0827 - regression_loss: 1.5736 - classification_loss: 0.5091\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.0893 - regression_loss: 1.5779 - classification_loss: 0.5114\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.0893 - regression_loss: 1.5773 - classification_loss: 0.5120\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.0938 - regression_loss: 1.5796 - classification_loss: 0.5142\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.0858 - regression_loss: 1.5742 - classification_loss: 0.5116\n",
            "49/86 [================>.............] - ETA: 34s - loss: 2.0845 - regression_loss: 1.5734 - classification_loss: 0.5111\n",
            "50/86 [================>.............] - ETA: 33s - loss: 2.0838 - regression_loss: 1.5733 - classification_loss: 0.5105\n",
            "51/86 [================>.............] - ETA: 32s - loss: 2.0895 - regression_loss: 1.5777 - classification_loss: 0.5119\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.0879 - regression_loss: 1.5759 - classification_loss: 0.5120\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0887 - regression_loss: 1.5781 - classification_loss: 0.5106\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0953 - regression_loss: 1.5829 - classification_loss: 0.5124\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0934 - regression_loss: 1.5809 - classification_loss: 0.5124\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0967 - regression_loss: 1.5828 - classification_loss: 0.5139\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0996 - regression_loss: 1.5849 - classification_loss: 0.5147\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1011 - regression_loss: 1.5858 - classification_loss: 0.5154\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1033 - regression_loss: 1.5873 - classification_loss: 0.5160\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1083 - regression_loss: 1.5909 - classification_loss: 0.5174\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1114 - regression_loss: 1.5942 - classification_loss: 0.5173\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1090 - regression_loss: 1.5925 - classification_loss: 0.5165\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1073 - regression_loss: 1.5908 - classification_loss: 0.5165\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1037 - regression_loss: 1.5878 - classification_loss: 0.5159\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1010 - regression_loss: 1.5852 - classification_loss: 0.5158\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.0975 - regression_loss: 1.5830 - classification_loss: 0.5146\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1007 - regression_loss: 1.5851 - classification_loss: 0.5156\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1028 - regression_loss: 1.5865 - classification_loss: 0.5163\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0955 - regression_loss: 1.5813 - classification_loss: 0.5142\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0933 - regression_loss: 1.5804 - classification_loss: 0.5130\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0922 - regression_loss: 1.5801 - classification_loss: 0.5121\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0906 - regression_loss: 1.5781 - classification_loss: 0.5125\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0929 - regression_loss: 1.5796 - classification_loss: 0.5133\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0968 - regression_loss: 1.5815 - classification_loss: 0.5153\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0983 - regression_loss: 1.5824 - classification_loss: 0.5159\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0947 - regression_loss: 1.5788 - classification_loss: 0.5159 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0972 - regression_loss: 1.5808 - classification_loss: 0.5164\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0998 - regression_loss: 1.5827 - classification_loss: 0.5171\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0994 - regression_loss: 1.5821 - classification_loss: 0.5172\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1007 - regression_loss: 1.5824 - classification_loss: 0.5183\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0985 - regression_loss: 1.5805 - classification_loss: 0.5181\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0980 - regression_loss: 1.5800 - classification_loss: 0.5180\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1003 - regression_loss: 1.5827 - classification_loss: 0.5177\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0969 - regression_loss: 1.5799 - classification_loss: 0.5170\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1028 - regression_loss: 1.5837 - classification_loss: 0.5191\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1042 - regression_loss: 1.5852 - classification_loss: 0.5190\n",
            "Epoch 48: saving model to ./snapshots\\resnet50_csv_48.h5\n",
            "\n",
            "Epoch 48: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-22.\n",
            "\n",
            "86/86 [==============================] - 82s 949ms/step - loss: 2.1042 - regression_loss: 1.5852 - classification_loss: 0.5190 - lr: 1.0000e-20\n",
            "Epoch 49/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:58 - loss: 2.1950 - regression_loss: 1.7225 - classification_loss: 0.4725\n",
            " 2/86 [..............................] - ETA: 1:06 - loss: 2.1938 - regression_loss: 1.6950 - classification_loss: 0.4989\n",
            " 3/86 [>.............................] - ETA: 1:10 - loss: 2.2386 - regression_loss: 1.7272 - classification_loss: 0.5113\n",
            " 4/86 [>.............................] - ETA: 1:13 - loss: 2.2510 - regression_loss: 1.7342 - classification_loss: 0.5168\n",
            " 5/86 [>.............................] - ETA: 1:10 - loss: 2.1155 - regression_loss: 1.6297 - classification_loss: 0.4858\n",
            " 6/86 [=>............................] - ETA: 1:09 - loss: 2.0679 - regression_loss: 1.5807 - classification_loss: 0.4873\n",
            " 7/86 [=>............................] - ETA: 1:09 - loss: 2.0741 - regression_loss: 1.5919 - classification_loss: 0.4822\n",
            " 8/86 [=>............................] - ETA: 1:12 - loss: 2.0763 - regression_loss: 1.5925 - classification_loss: 0.4837\n",
            " 9/86 [==>...........................] - ETA: 1:10 - loss: 2.0892 - regression_loss: 1.5999 - classification_loss: 0.4893\n",
            "10/86 [==>...........................] - ETA: 1:09 - loss: 2.0683 - regression_loss: 1.5751 - classification_loss: 0.4933\n",
            "11/86 [==>...........................] - ETA: 1:08 - loss: 2.0995 - regression_loss: 1.5977 - classification_loss: 0.5018\n",
            "12/86 [===>..........................] - ETA: 1:08 - loss: 2.0860 - regression_loss: 1.5907 - classification_loss: 0.4953\n",
            "13/86 [===>..........................] - ETA: 1:06 - loss: 2.0845 - regression_loss: 1.5827 - classification_loss: 0.5018\n",
            "14/86 [===>..........................] - ETA: 1:05 - loss: 2.0661 - regression_loss: 1.5671 - classification_loss: 0.4991\n",
            "15/86 [====>.........................] - ETA: 1:06 - loss: 2.0659 - regression_loss: 1.5672 - classification_loss: 0.4987\n",
            "16/86 [====>.........................] - ETA: 1:04 - loss: 2.0819 - regression_loss: 1.5785 - classification_loss: 0.5034\n",
            "17/86 [====>.........................] - ETA: 1:03 - loss: 2.0771 - regression_loss: 1.5782 - classification_loss: 0.4989\n",
            "18/86 [=====>........................] - ETA: 1:03 - loss: 2.0866 - regression_loss: 1.5880 - classification_loss: 0.4985\n",
            "19/86 [=====>........................] - ETA: 1:02 - loss: 2.0749 - regression_loss: 1.5782 - classification_loss: 0.4967\n",
            "20/86 [=====>........................] - ETA: 1:00 - loss: 2.0837 - regression_loss: 1.5871 - classification_loss: 0.4966\n",
            "21/86 [======>.......................] - ETA: 59s - loss: 2.0590 - regression_loss: 1.5684 - classification_loss: 0.4906 \n",
            "22/86 [======>.......................] - ETA: 58s - loss: 2.0459 - regression_loss: 1.5599 - classification_loss: 0.4859\n",
            "23/86 [=======>......................] - ETA: 57s - loss: 2.0452 - regression_loss: 1.5579 - classification_loss: 0.4873\n",
            "24/86 [=======>......................] - ETA: 57s - loss: 2.0481 - regression_loss: 1.5589 - classification_loss: 0.4892\n",
            "25/86 [=======>......................] - ETA: 56s - loss: 2.0538 - regression_loss: 1.5619 - classification_loss: 0.4919\n",
            "26/86 [========>.....................] - ETA: 55s - loss: 2.0547 - regression_loss: 1.5624 - classification_loss: 0.4923\n",
            "27/86 [========>.....................] - ETA: 54s - loss: 2.0595 - regression_loss: 1.5659 - classification_loss: 0.4936\n",
            "28/86 [========>.....................] - ETA: 54s - loss: 2.0645 - regression_loss: 1.5706 - classification_loss: 0.4938\n",
            "29/86 [=========>....................] - ETA: 53s - loss: 2.0799 - regression_loss: 1.5836 - classification_loss: 0.4964\n",
            "30/86 [=========>....................] - ETA: 52s - loss: 2.0774 - regression_loss: 1.5816 - classification_loss: 0.4958\n",
            "31/86 [=========>....................] - ETA: 51s - loss: 2.0765 - regression_loss: 1.5824 - classification_loss: 0.4940\n",
            "32/86 [==========>...................] - ETA: 50s - loss: 2.0793 - regression_loss: 1.5849 - classification_loss: 0.4944\n",
            "33/86 [==========>...................] - ETA: 49s - loss: 2.0884 - regression_loss: 1.5922 - classification_loss: 0.4962\n",
            "34/86 [==========>...................] - ETA: 48s - loss: 2.0898 - regression_loss: 1.5927 - classification_loss: 0.4971\n",
            "35/86 [===========>..................] - ETA: 47s - loss: 2.0932 - regression_loss: 1.5949 - classification_loss: 0.4983\n",
            "36/86 [===========>..................] - ETA: 46s - loss: 2.0931 - regression_loss: 1.5937 - classification_loss: 0.4994\n",
            "37/86 [===========>..................] - ETA: 45s - loss: 2.0940 - regression_loss: 1.5922 - classification_loss: 0.5018\n",
            "38/86 [============>.................] - ETA: 44s - loss: 2.0981 - regression_loss: 1.5966 - classification_loss: 0.5015\n",
            "39/86 [============>.................] - ETA: 43s - loss: 2.0952 - regression_loss: 1.5964 - classification_loss: 0.4987\n",
            "40/86 [============>.................] - ETA: 42s - loss: 2.1017 - regression_loss: 1.6006 - classification_loss: 0.5011\n",
            "41/86 [=============>................] - ETA: 41s - loss: 2.1033 - regression_loss: 1.6011 - classification_loss: 0.5022\n",
            "42/86 [=============>................] - ETA: 40s - loss: 2.1019 - regression_loss: 1.5992 - classification_loss: 0.5026\n",
            "43/86 [==============>...............] - ETA: 39s - loss: 2.1038 - regression_loss: 1.5995 - classification_loss: 0.5043\n",
            "44/86 [==============>...............] - ETA: 38s - loss: 2.0998 - regression_loss: 1.5962 - classification_loss: 0.5035\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.1002 - regression_loss: 1.5961 - classification_loss: 0.5041\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.0967 - regression_loss: 1.5937 - classification_loss: 0.5030\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.0931 - regression_loss: 1.5914 - classification_loss: 0.5017\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.0929 - regression_loss: 1.5909 - classification_loss: 0.5020\n",
            "49/86 [================>.............] - ETA: 34s - loss: 2.0933 - regression_loss: 1.5914 - classification_loss: 0.5020\n",
            "50/86 [================>.............] - ETA: 33s - loss: 2.0907 - regression_loss: 1.5885 - classification_loss: 0.5022\n",
            "51/86 [================>.............] - ETA: 32s - loss: 2.0877 - regression_loss: 1.5852 - classification_loss: 0.5024\n",
            "52/86 [=================>............] - ETA: 31s - loss: 2.0944 - regression_loss: 1.5902 - classification_loss: 0.5043\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0980 - regression_loss: 1.5923 - classification_loss: 0.5057\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0996 - regression_loss: 1.5921 - classification_loss: 0.5075\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1010 - regression_loss: 1.5932 - classification_loss: 0.5079\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0982 - regression_loss: 1.5910 - classification_loss: 0.5072\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0973 - regression_loss: 1.5901 - classification_loss: 0.5072\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1061 - regression_loss: 1.5972 - classification_loss: 0.5089\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1012 - regression_loss: 1.5938 - classification_loss: 0.5074\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1013 - regression_loss: 1.5929 - classification_loss: 0.5084\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1030 - regression_loss: 1.5946 - classification_loss: 0.5085\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1052 - regression_loss: 1.5959 - classification_loss: 0.5093\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1044 - regression_loss: 1.5951 - classification_loss: 0.5093\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1072 - regression_loss: 1.5969 - classification_loss: 0.5103\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1092 - regression_loss: 1.5976 - classification_loss: 0.5117\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1084 - regression_loss: 1.5971 - classification_loss: 0.5113\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1057 - regression_loss: 1.5953 - classification_loss: 0.5105\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1062 - regression_loss: 1.5954 - classification_loss: 0.5108\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1057 - regression_loss: 1.5952 - classification_loss: 0.5105\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1036 - regression_loss: 1.5949 - classification_loss: 0.5087\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1061 - regression_loss: 1.5965 - classification_loss: 0.5097\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1032 - regression_loss: 1.5940 - classification_loss: 0.5092\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1035 - regression_loss: 1.5943 - classification_loss: 0.5092\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1029 - regression_loss: 1.5934 - classification_loss: 0.5095\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0983 - regression_loss: 1.5904 - classification_loss: 0.5079\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1004 - regression_loss: 1.5930 - classification_loss: 0.5074 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0982 - regression_loss: 1.5905 - classification_loss: 0.5077\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0983 - regression_loss: 1.5909 - classification_loss: 0.5074\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1045 - regression_loss: 1.5961 - classification_loss: 0.5083\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1051 - regression_loss: 1.5970 - classification_loss: 0.5081\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1013 - regression_loss: 1.5946 - classification_loss: 0.5067\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1022 - regression_loss: 1.5953 - classification_loss: 0.5070\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1015 - regression_loss: 1.5945 - classification_loss: 0.5071\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1015 - regression_loss: 1.5951 - classification_loss: 0.5064\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0979 - regression_loss: 1.5924 - classification_loss: 0.5055\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0976 - regression_loss: 1.5920 - classification_loss: 0.5057\n",
            "Epoch 49: saving model to ./snapshots\\resnet50_csv_49.h5\n",
            "\n",
            "86/86 [==============================] - 82s 949ms/step - loss: 2.0976 - regression_loss: 1.5920 - classification_loss: 0.5057 - lr: 1.0000e-21\n",
            "Epoch 50/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:55 - loss: 2.0404 - regression_loss: 1.5396 - classification_loss: 0.5008\n",
            " 2/86 [..............................] - ETA: 1:23 - loss: 2.2405 - regression_loss: 1.6955 - classification_loss: 0.5450\n",
            " 3/86 [>.............................] - ETA: 1:20 - loss: 2.1355 - regression_loss: 1.5935 - classification_loss: 0.5420\n",
            " 4/86 [>.............................] - ETA: 1:19 - loss: 2.1275 - regression_loss: 1.6034 - classification_loss: 0.5241\n",
            " 5/86 [>.............................] - ETA: 1:16 - loss: 2.1305 - regression_loss: 1.6062 - classification_loss: 0.5242\n",
            " 6/86 [=>............................] - ETA: 1:14 - loss: 2.1039 - regression_loss: 1.5834 - classification_loss: 0.5205\n",
            " 7/86 [=>............................] - ETA: 1:13 - loss: 2.0562 - regression_loss: 1.5510 - classification_loss: 0.5052\n",
            " 8/86 [=>............................] - ETA: 1:12 - loss: 2.0555 - regression_loss: 1.5549 - classification_loss: 0.5006\n",
            " 9/86 [==>...........................] - ETA: 1:11 - loss: 2.0312 - regression_loss: 1.5368 - classification_loss: 0.4943\n",
            "10/86 [==>...........................] - ETA: 1:10 - loss: 2.0374 - regression_loss: 1.5406 - classification_loss: 0.4968\n",
            "11/86 [==>...........................] - ETA: 1:09 - loss: 2.0369 - regression_loss: 1.5398 - classification_loss: 0.4971\n",
            "12/86 [===>..........................] - ETA: 1:08 - loss: 2.0724 - regression_loss: 1.5625 - classification_loss: 0.5099\n",
            "13/86 [===>..........................] - ETA: 1:07 - loss: 2.0766 - regression_loss: 1.5685 - classification_loss: 0.5081\n",
            "14/86 [===>..........................] - ETA: 1:07 - loss: 2.0606 - regression_loss: 1.5558 - classification_loss: 0.5048\n",
            "15/86 [====>.........................] - ETA: 1:06 - loss: 2.0650 - regression_loss: 1.5579 - classification_loss: 0.5071\n",
            "16/86 [====>.........................] - ETA: 1:05 - loss: 2.0525 - regression_loss: 1.5507 - classification_loss: 0.5018\n",
            "17/86 [====>.........................] - ETA: 1:04 - loss: 2.0460 - regression_loss: 1.5481 - classification_loss: 0.4979\n",
            "18/86 [=====>........................] - ETA: 1:03 - loss: 2.0651 - regression_loss: 1.5643 - classification_loss: 0.5008\n",
            "19/86 [=====>........................] - ETA: 1:01 - loss: 2.0409 - regression_loss: 1.5464 - classification_loss: 0.4945\n",
            "20/86 [=====>........................] - ETA: 1:00 - loss: 2.0480 - regression_loss: 1.5520 - classification_loss: 0.4960\n",
            "21/86 [======>.......................] - ETA: 1:00 - loss: 2.0532 - regression_loss: 1.5559 - classification_loss: 0.4972\n",
            "22/86 [======>.......................] - ETA: 59s - loss: 2.0414 - regression_loss: 1.5473 - classification_loss: 0.4942 \n",
            "23/86 [=======>......................] - ETA: 57s - loss: 2.0479 - regression_loss: 1.5521 - classification_loss: 0.4959\n",
            "24/86 [=======>......................] - ETA: 56s - loss: 2.0505 - regression_loss: 1.5505 - classification_loss: 0.4999\n",
            "25/86 [=======>......................] - ETA: 56s - loss: 2.0626 - regression_loss: 1.5576 - classification_loss: 0.5050\n",
            "26/86 [========>.....................] - ETA: 54s - loss: 2.0606 - regression_loss: 1.5577 - classification_loss: 0.5029\n",
            "27/86 [========>.....................] - ETA: 53s - loss: 2.0557 - regression_loss: 1.5539 - classification_loss: 0.5018\n",
            "28/86 [========>.....................] - ETA: 52s - loss: 2.0491 - regression_loss: 1.5487 - classification_loss: 0.5003\n",
            "29/86 [=========>....................] - ETA: 52s - loss: 2.0450 - regression_loss: 1.5464 - classification_loss: 0.4985\n",
            "30/86 [=========>....................] - ETA: 51s - loss: 2.0420 - regression_loss: 1.5443 - classification_loss: 0.4977\n",
            "31/86 [=========>....................] - ETA: 50s - loss: 2.0458 - regression_loss: 1.5470 - classification_loss: 0.4989\n",
            "32/86 [==========>...................] - ETA: 49s - loss: 2.0475 - regression_loss: 1.5478 - classification_loss: 0.4997\n",
            "33/86 [==========>...................] - ETA: 48s - loss: 2.0431 - regression_loss: 1.5437 - classification_loss: 0.4994\n",
            "34/86 [==========>...................] - ETA: 47s - loss: 2.0405 - regression_loss: 1.5409 - classification_loss: 0.4996\n",
            "35/86 [===========>..................] - ETA: 46s - loss: 2.0375 - regression_loss: 1.5408 - classification_loss: 0.4967\n",
            "36/86 [===========>..................] - ETA: 46s - loss: 2.0412 - regression_loss: 1.5442 - classification_loss: 0.4970\n",
            "37/86 [===========>..................] - ETA: 44s - loss: 2.0440 - regression_loss: 1.5471 - classification_loss: 0.4968\n",
            "38/86 [============>.................] - ETA: 43s - loss: 2.0375 - regression_loss: 1.5429 - classification_loss: 0.4946\n",
            "39/86 [============>.................] - ETA: 42s - loss: 2.0351 - regression_loss: 1.5413 - classification_loss: 0.4938\n",
            "40/86 [============>.................] - ETA: 42s - loss: 2.0361 - regression_loss: 1.5423 - classification_loss: 0.4938\n",
            "41/86 [=============>................] - ETA: 40s - loss: 2.0456 - regression_loss: 1.5491 - classification_loss: 0.4965\n",
            "42/86 [=============>................] - ETA: 40s - loss: 2.0437 - regression_loss: 1.5481 - classification_loss: 0.4956\n",
            "43/86 [==============>...............] - ETA: 39s - loss: 2.0453 - regression_loss: 1.5498 - classification_loss: 0.4955\n",
            "44/86 [==============>...............] - ETA: 38s - loss: 2.0530 - regression_loss: 1.5540 - classification_loss: 0.4990\n",
            "45/86 [==============>...............] - ETA: 37s - loss: 2.0565 - regression_loss: 1.5568 - classification_loss: 0.4998\n",
            "46/86 [===============>..............] - ETA: 36s - loss: 2.0627 - regression_loss: 1.5612 - classification_loss: 0.5015\n",
            "47/86 [===============>..............] - ETA: 35s - loss: 2.0638 - regression_loss: 1.5609 - classification_loss: 0.5029\n",
            "48/86 [===============>..............] - ETA: 34s - loss: 2.0587 - regression_loss: 1.5563 - classification_loss: 0.5024\n",
            "49/86 [================>.............] - ETA: 33s - loss: 2.0599 - regression_loss: 1.5583 - classification_loss: 0.5017\n",
            "50/86 [================>.............] - ETA: 32s - loss: 2.0598 - regression_loss: 1.5580 - classification_loss: 0.5017\n",
            "51/86 [================>.............] - ETA: 31s - loss: 2.0579 - regression_loss: 1.5561 - classification_loss: 0.5017\n",
            "52/86 [=================>............] - ETA: 30s - loss: 2.0636 - regression_loss: 1.5607 - classification_loss: 0.5029\n",
            "53/86 [=================>............] - ETA: 29s - loss: 2.0677 - regression_loss: 1.5636 - classification_loss: 0.5041\n",
            "54/86 [=================>............] - ETA: 29s - loss: 2.0702 - regression_loss: 1.5664 - classification_loss: 0.5038\n",
            "55/86 [==================>...........] - ETA: 28s - loss: 2.0738 - regression_loss: 1.5692 - classification_loss: 0.5045\n",
            "56/86 [==================>...........] - ETA: 27s - loss: 2.0696 - regression_loss: 1.5664 - classification_loss: 0.5032\n",
            "57/86 [==================>...........] - ETA: 26s - loss: 2.0709 - regression_loss: 1.5676 - classification_loss: 0.5033\n",
            "58/86 [===================>..........] - ETA: 25s - loss: 2.0680 - regression_loss: 1.5651 - classification_loss: 0.5029\n",
            "59/86 [===================>..........] - ETA: 24s - loss: 2.0669 - regression_loss: 1.5657 - classification_loss: 0.5012\n",
            "60/86 [===================>..........] - ETA: 23s - loss: 2.0705 - regression_loss: 1.5678 - classification_loss: 0.5027\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0700 - regression_loss: 1.5681 - classification_loss: 0.5019\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.0711 - regression_loss: 1.5687 - classification_loss: 0.5024\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.0701 - regression_loss: 1.5682 - classification_loss: 0.5019\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.0713 - regression_loss: 1.5685 - classification_loss: 0.5028\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.0761 - regression_loss: 1.5727 - classification_loss: 0.5034\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.0773 - regression_loss: 1.5735 - classification_loss: 0.5038\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.0738 - regression_loss: 1.5705 - classification_loss: 0.5033\n",
            "68/86 [======================>.......] - ETA: 16s - loss: 2.0736 - regression_loss: 1.5703 - classification_loss: 0.5033\n",
            "69/86 [=======================>......] - ETA: 15s - loss: 2.0733 - regression_loss: 1.5669 - classification_loss: 0.5063\n",
            "70/86 [=======================>......] - ETA: 14s - loss: 2.0751 - regression_loss: 1.5677 - classification_loss: 0.5074\n",
            "71/86 [=======================>......] - ETA: 13s - loss: 2.0721 - regression_loss: 1.5653 - classification_loss: 0.5068\n",
            "72/86 [========================>.....] - ETA: 12s - loss: 2.0749 - regression_loss: 1.5677 - classification_loss: 0.5072\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0759 - regression_loss: 1.5682 - classification_loss: 0.5077\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0759 - regression_loss: 1.5690 - classification_loss: 0.5069\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0800 - regression_loss: 1.5720 - classification_loss: 0.5081\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0842 - regression_loss: 1.5749 - classification_loss: 0.5093 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0850 - regression_loss: 1.5757 - classification_loss: 0.5093\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0846 - regression_loss: 1.5761 - classification_loss: 0.5085\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0860 - regression_loss: 1.5774 - classification_loss: 0.5086\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0853 - regression_loss: 1.5766 - classification_loss: 0.5087\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0843 - regression_loss: 1.5760 - classification_loss: 0.5083\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0859 - regression_loss: 1.5765 - classification_loss: 0.5094\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0789 - regression_loss: 1.5713 - classification_loss: 0.5077\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0793 - regression_loss: 1.5721 - classification_loss: 0.5073\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0768 - regression_loss: 1.5701 - classification_loss: 0.5067\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0769 - regression_loss: 1.5696 - classification_loss: 0.5073\n",
            "Epoch 50: saving model to ./snapshots\\resnet50_csv_50.h5\n",
            "\n",
            "Epoch 50: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-23.\n",
            "\n",
            "86/86 [==============================] - 81s 941ms/step - loss: 2.0769 - regression_loss: 1.5696 - classification_loss: 0.5073 - lr: 1.0000e-21\n",
            "Epoch 51/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:57 - loss: 1.9393 - regression_loss: 1.4922 - classification_loss: 0.4472\n",
            " 2/86 [..............................] - ETA: 1:07 - loss: 2.1273 - regression_loss: 1.6164 - classification_loss: 0.5109\n",
            " 3/86 [>.............................] - ETA: 1:09 - loss: 2.1334 - regression_loss: 1.6054 - classification_loss: 0.5280\n",
            " 4/86 [>.............................] - ETA: 1:09 - loss: 2.1921 - regression_loss: 1.6564 - classification_loss: 0.5357\n",
            " 5/86 [>.............................] - ETA: 1:08 - loss: 2.1835 - regression_loss: 1.6369 - classification_loss: 0.5466\n",
            " 6/86 [=>............................] - ETA: 1:09 - loss: 2.1964 - regression_loss: 1.6509 - classification_loss: 0.5455\n",
            " 7/86 [=>............................] - ETA: 1:08 - loss: 2.1679 - regression_loss: 1.6339 - classification_loss: 0.5340\n",
            " 8/86 [=>............................] - ETA: 1:07 - loss: 2.1340 - regression_loss: 1.6093 - classification_loss: 0.5247\n",
            " 9/86 [==>...........................] - ETA: 1:07 - loss: 2.0709 - regression_loss: 1.5639 - classification_loss: 0.5070\n",
            "10/86 [==>...........................] - ETA: 1:09 - loss: 2.0827 - regression_loss: 1.5773 - classification_loss: 0.5053\n",
            "11/86 [==>...........................] - ETA: 1:07 - loss: 2.1049 - regression_loss: 1.5936 - classification_loss: 0.5113\n",
            "12/86 [===>..........................] - ETA: 1:07 - loss: 2.1197 - regression_loss: 1.6075 - classification_loss: 0.5122\n",
            "13/86 [===>..........................] - ETA: 1:08 - loss: 2.1238 - regression_loss: 1.6144 - classification_loss: 0.5094\n",
            "14/86 [===>..........................] - ETA: 1:07 - loss: 2.0970 - regression_loss: 1.5909 - classification_loss: 0.5061\n",
            "15/86 [====>.........................] - ETA: 1:05 - loss: 2.1190 - regression_loss: 1.6057 - classification_loss: 0.5133\n",
            "16/86 [====>.........................] - ETA: 1:06 - loss: 2.1244 - regression_loss: 1.6101 - classification_loss: 0.5142\n",
            "17/86 [====>.........................] - ETA: 1:07 - loss: 2.1195 - regression_loss: 1.6042 - classification_loss: 0.5152\n",
            "18/86 [=====>........................] - ETA: 1:06 - loss: 2.1277 - regression_loss: 1.6121 - classification_loss: 0.5156\n",
            "19/86 [=====>........................] - ETA: 1:05 - loss: 2.1340 - regression_loss: 1.6203 - classification_loss: 0.5137\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.1263 - regression_loss: 1.6125 - classification_loss: 0.5138\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.1271 - regression_loss: 1.6130 - classification_loss: 0.5141\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.1195 - regression_loss: 1.6052 - classification_loss: 0.5144\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.1226 - regression_loss: 1.6071 - classification_loss: 0.5155\n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.1216 - regression_loss: 1.6052 - classification_loss: 0.5164 \n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.1303 - regression_loss: 1.6131 - classification_loss: 0.5172\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.1200 - regression_loss: 1.6056 - classification_loss: 0.5144\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.1036 - regression_loss: 1.5936 - classification_loss: 0.5100\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.1063 - regression_loss: 1.5963 - classification_loss: 0.5100\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.1039 - regression_loss: 1.5929 - classification_loss: 0.5110\n",
            "30/86 [=========>....................] - ETA: 52s - loss: 2.1053 - regression_loss: 1.5953 - classification_loss: 0.5100\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.1056 - regression_loss: 1.5971 - classification_loss: 0.5086\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.1014 - regression_loss: 1.5942 - classification_loss: 0.5073\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.1001 - regression_loss: 1.5916 - classification_loss: 0.5084\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.1021 - regression_loss: 1.5908 - classification_loss: 0.5114\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.0955 - regression_loss: 1.5865 - classification_loss: 0.5090\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.0945 - regression_loss: 1.5852 - classification_loss: 0.5093\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.1053 - regression_loss: 1.5942 - classification_loss: 0.5112\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.1035 - regression_loss: 1.5922 - classification_loss: 0.5113\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.0967 - regression_loss: 1.5872 - classification_loss: 0.5095\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.0969 - regression_loss: 1.5865 - classification_loss: 0.5104\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.1035 - regression_loss: 1.5924 - classification_loss: 0.5111\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.0918 - regression_loss: 1.5839 - classification_loss: 0.5080\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.0902 - regression_loss: 1.5828 - classification_loss: 0.5074\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.0947 - regression_loss: 1.5860 - classification_loss: 0.5087\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.1035 - regression_loss: 1.5922 - classification_loss: 0.5113\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.1082 - regression_loss: 1.5963 - classification_loss: 0.5119\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1093 - regression_loss: 1.5974 - classification_loss: 0.5119\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.1135 - regression_loss: 1.6008 - classification_loss: 0.5126\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1133 - regression_loss: 1.5999 - classification_loss: 0.5134\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1150 - regression_loss: 1.5999 - classification_loss: 0.5151\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1190 - regression_loss: 1.6044 - classification_loss: 0.5146\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1263 - regression_loss: 1.6100 - classification_loss: 0.5164\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1359 - regression_loss: 1.6176 - classification_loss: 0.5183\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1370 - regression_loss: 1.6186 - classification_loss: 0.5184\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1352 - regression_loss: 1.6183 - classification_loss: 0.5169\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1410 - regression_loss: 1.6212 - classification_loss: 0.5198\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1430 - regression_loss: 1.6237 - classification_loss: 0.5193\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1408 - regression_loss: 1.6227 - classification_loss: 0.5181\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1421 - regression_loss: 1.6225 - classification_loss: 0.5197\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1379 - regression_loss: 1.6167 - classification_loss: 0.5211\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1334 - regression_loss: 1.6132 - classification_loss: 0.5201\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1350 - regression_loss: 1.6142 - classification_loss: 0.5208\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1308 - regression_loss: 1.6104 - classification_loss: 0.5204\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1283 - regression_loss: 1.6087 - classification_loss: 0.5196\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1243 - regression_loss: 1.6056 - classification_loss: 0.5187\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.1222 - regression_loss: 1.6038 - classification_loss: 0.5184\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1199 - regression_loss: 1.6035 - classification_loss: 0.5164\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1263 - regression_loss: 1.6085 - classification_loss: 0.5178\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1267 - regression_loss: 1.6088 - classification_loss: 0.5179\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1265 - regression_loss: 1.6088 - classification_loss: 0.5177\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1250 - regression_loss: 1.6078 - classification_loss: 0.5172\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1241 - regression_loss: 1.6068 - classification_loss: 0.5173\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1222 - regression_loss: 1.6055 - classification_loss: 0.5166\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1197 - regression_loss: 1.6040 - classification_loss: 0.5157\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1222 - regression_loss: 1.6062 - classification_loss: 0.5160\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1262 - regression_loss: 1.6102 - classification_loss: 0.5160 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1246 - regression_loss: 1.6092 - classification_loss: 0.5154\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1264 - regression_loss: 1.6108 - classification_loss: 0.5156\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1249 - regression_loss: 1.6098 - classification_loss: 0.5152\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1250 - regression_loss: 1.6101 - classification_loss: 0.5149\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1248 - regression_loss: 1.6106 - classification_loss: 0.5142\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1211 - regression_loss: 1.6080 - classification_loss: 0.5131\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1209 - regression_loss: 1.6085 - classification_loss: 0.5124\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1194 - regression_loss: 1.6068 - classification_loss: 0.5126\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1201 - regression_loss: 1.6077 - classification_loss: 0.5124\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1198 - regression_loss: 1.6075 - classification_loss: 0.5123\n",
            "Epoch 51: saving model to ./snapshots\\resnet50_csv_51.h5\n",
            "\n",
            "86/86 [==============================] - 84s 971ms/step - loss: 2.1198 - regression_loss: 1.6075 - classification_loss: 0.5123 - lr: 1.0000e-22\n",
            "Epoch 52/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:53 - loss: 2.2778 - regression_loss: 1.7379 - classification_loss: 0.5399\n",
            " 2/86 [..............................] - ETA: 1:44 - loss: 2.1129 - regression_loss: 1.6067 - classification_loss: 0.5062\n",
            " 3/86 [>.............................] - ETA: 1:39 - loss: 2.0979 - regression_loss: 1.5826 - classification_loss: 0.5153\n",
            " 4/86 [>.............................] - ETA: 1:29 - loss: 2.0819 - regression_loss: 1.5662 - classification_loss: 0.5157\n",
            " 5/86 [>.............................] - ETA: 1:24 - loss: 2.0361 - regression_loss: 1.5270 - classification_loss: 0.5091\n",
            " 6/86 [=>............................] - ETA: 1:22 - loss: 2.0566 - regression_loss: 1.5513 - classification_loss: 0.5053\n",
            " 7/86 [=>............................] - ETA: 1:19 - loss: 2.0516 - regression_loss: 1.5399 - classification_loss: 0.5116\n",
            " 8/86 [=>............................] - ETA: 1:17 - loss: 2.0868 - regression_loss: 1.5657 - classification_loss: 0.5211\n",
            " 9/86 [==>...........................] - ETA: 1:14 - loss: 2.0689 - regression_loss: 1.5555 - classification_loss: 0.5134\n",
            "10/86 [==>...........................] - ETA: 1:14 - loss: 2.0626 - regression_loss: 1.5553 - classification_loss: 0.5073\n",
            "11/86 [==>...........................] - ETA: 1:15 - loss: 2.0719 - regression_loss: 1.5643 - classification_loss: 0.5075\n",
            "12/86 [===>..........................] - ETA: 1:13 - loss: 2.0645 - regression_loss: 1.5581 - classification_loss: 0.5064\n",
            "13/86 [===>..........................] - ETA: 1:11 - loss: 2.0166 - regression_loss: 1.5235 - classification_loss: 0.4930\n",
            "14/86 [===>..........................] - ETA: 1:09 - loss: 2.0157 - regression_loss: 1.5254 - classification_loss: 0.4903\n",
            "15/86 [====>.........................] - ETA: 1:10 - loss: 2.0551 - regression_loss: 1.5580 - classification_loss: 0.4971\n",
            "16/86 [====>.........................] - ETA: 1:07 - loss: 2.0527 - regression_loss: 1.5555 - classification_loss: 0.4972\n",
            "17/86 [====>.........................] - ETA: 1:08 - loss: 2.0667 - regression_loss: 1.5647 - classification_loss: 0.5020\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.0673 - regression_loss: 1.5674 - classification_loss: 0.4999\n",
            "19/86 [=====>........................] - ETA: 1:05 - loss: 2.0828 - regression_loss: 1.5761 - classification_loss: 0.5067\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.0642 - regression_loss: 1.5631 - classification_loss: 0.5011\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.0706 - regression_loss: 1.5668 - classification_loss: 0.5038\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.0676 - regression_loss: 1.5651 - classification_loss: 0.5025\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.0689 - regression_loss: 1.5682 - classification_loss: 0.5007\n",
            "24/86 [=======>......................] - ETA: 1:00 - loss: 2.0748 - regression_loss: 1.5737 - classification_loss: 0.5011\n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.0715 - regression_loss: 1.5718 - classification_loss: 0.4997 \n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.0675 - regression_loss: 1.5703 - classification_loss: 0.4972\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.0626 - regression_loss: 1.5653 - classification_loss: 0.4972\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.0681 - regression_loss: 1.5675 - classification_loss: 0.5006\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.0645 - regression_loss: 1.5650 - classification_loss: 0.4995\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.0665 - regression_loss: 1.5663 - classification_loss: 0.5002\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.0586 - regression_loss: 1.5599 - classification_loss: 0.4986\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.0565 - regression_loss: 1.5586 - classification_loss: 0.4979\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.0590 - regression_loss: 1.5607 - classification_loss: 0.4983\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.0585 - regression_loss: 1.5625 - classification_loss: 0.4960\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.0676 - regression_loss: 1.5697 - classification_loss: 0.4979\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.0591 - regression_loss: 1.5643 - classification_loss: 0.4947\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.0647 - regression_loss: 1.5688 - classification_loss: 0.4958\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.0589 - regression_loss: 1.5642 - classification_loss: 0.4947\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.0584 - regression_loss: 1.5643 - classification_loss: 0.4941\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.0660 - regression_loss: 1.5701 - classification_loss: 0.4959\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.0677 - regression_loss: 1.5732 - classification_loss: 0.4945\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.0683 - regression_loss: 1.5737 - classification_loss: 0.4946\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.0744 - regression_loss: 1.5785 - classification_loss: 0.4959\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.0730 - regression_loss: 1.5765 - classification_loss: 0.4965\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.0757 - regression_loss: 1.5771 - classification_loss: 0.4986\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.0695 - regression_loss: 1.5733 - classification_loss: 0.4962\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.0708 - regression_loss: 1.5753 - classification_loss: 0.4955\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.0658 - regression_loss: 1.5721 - classification_loss: 0.4937\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.0705 - regression_loss: 1.5750 - classification_loss: 0.4955\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.0758 - regression_loss: 1.5788 - classification_loss: 0.4970\n",
            "51/86 [================>.............] - ETA: 32s - loss: 2.0792 - regression_loss: 1.5821 - classification_loss: 0.4970\n",
            "52/86 [=================>............] - ETA: 31s - loss: 2.0789 - regression_loss: 1.5815 - classification_loss: 0.4975\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0773 - regression_loss: 1.5802 - classification_loss: 0.4971\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0782 - regression_loss: 1.5805 - classification_loss: 0.4977\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0842 - regression_loss: 1.5865 - classification_loss: 0.4976\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0851 - regression_loss: 1.5873 - classification_loss: 0.4978\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0863 - regression_loss: 1.5877 - classification_loss: 0.4985\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0930 - regression_loss: 1.5926 - classification_loss: 0.5004\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0950 - regression_loss: 1.5940 - classification_loss: 0.5010\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0938 - regression_loss: 1.5933 - classification_loss: 0.5004\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0882 - regression_loss: 1.5885 - classification_loss: 0.4997\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.0914 - regression_loss: 1.5919 - classification_loss: 0.4995\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.0932 - regression_loss: 1.5920 - classification_loss: 0.5012\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.0962 - regression_loss: 1.5925 - classification_loss: 0.5038\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1006 - regression_loss: 1.5958 - classification_loss: 0.5048\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.1030 - regression_loss: 1.5982 - classification_loss: 0.5048\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1039 - regression_loss: 1.5984 - classification_loss: 0.5055\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1068 - regression_loss: 1.5997 - classification_loss: 0.5072\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1068 - regression_loss: 1.5995 - classification_loss: 0.5073\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1076 - regression_loss: 1.5997 - classification_loss: 0.5079\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1046 - regression_loss: 1.5970 - classification_loss: 0.5076\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1030 - regression_loss: 1.5948 - classification_loss: 0.5082\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1027 - regression_loss: 1.5944 - classification_loss: 0.5083\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1024 - regression_loss: 1.5948 - classification_loss: 0.5076\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0975 - regression_loss: 1.5912 - classification_loss: 0.5063\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0931 - regression_loss: 1.5874 - classification_loss: 0.5057 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0922 - regression_loss: 1.5866 - classification_loss: 0.5056\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0887 - regression_loss: 1.5835 - classification_loss: 0.5052\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0884 - regression_loss: 1.5834 - classification_loss: 0.5050\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0872 - regression_loss: 1.5822 - classification_loss: 0.5050\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0913 - regression_loss: 1.5855 - classification_loss: 0.5058\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0868 - regression_loss: 1.5821 - classification_loss: 0.5047\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0872 - regression_loss: 1.5827 - classification_loss: 0.5045\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0895 - regression_loss: 1.5849 - classification_loss: 0.5046\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0903 - regression_loss: 1.5853 - classification_loss: 0.5050\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0928 - regression_loss: 1.5882 - classification_loss: 0.5046\n",
            "Epoch 52: saving model to ./snapshots\\resnet50_csv_52.h5\n",
            "\n",
            "Epoch 52: ReduceLROnPlateau reducing learning rate to 9.999999682655227e-24.\n",
            "\n",
            "86/86 [==============================] - 82s 954ms/step - loss: 2.0928 - regression_loss: 1.5882 - classification_loss: 0.5046 - lr: 1.0000e-22\n",
            "Epoch 53/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:41 - loss: 2.0672 - regression_loss: 1.5607 - classification_loss: 0.5065\n",
            " 2/86 [..............................] - ETA: 1:45 - loss: 2.0857 - regression_loss: 1.5586 - classification_loss: 0.5270\n",
            " 3/86 [>.............................] - ETA: 1:27 - loss: 2.0430 - regression_loss: 1.5332 - classification_loss: 0.5098\n",
            " 4/86 [>.............................] - ETA: 1:18 - loss: 2.1057 - regression_loss: 1.5947 - classification_loss: 0.5109\n",
            " 5/86 [>.............................] - ETA: 1:22 - loss: 2.1481 - regression_loss: 1.6453 - classification_loss: 0.5029\n",
            " 6/86 [=>............................] - ETA: 1:17 - loss: 2.1188 - regression_loss: 1.6244 - classification_loss: 0.4943\n",
            " 7/86 [=>............................] - ETA: 1:20 - loss: 2.1529 - regression_loss: 1.6449 - classification_loss: 0.5081\n",
            " 8/86 [=>............................] - ETA: 1:16 - loss: 2.1450 - regression_loss: 1.6271 - classification_loss: 0.5178\n",
            " 9/86 [==>...........................] - ETA: 1:15 - loss: 2.1782 - regression_loss: 1.6524 - classification_loss: 0.5258\n",
            "10/86 [==>...........................] - ETA: 1:14 - loss: 2.1623 - regression_loss: 1.6474 - classification_loss: 0.5149\n",
            "11/86 [==>...........................] - ETA: 1:13 - loss: 2.1562 - regression_loss: 1.6464 - classification_loss: 0.5098\n",
            "12/86 [===>..........................] - ETA: 1:12 - loss: 2.1563 - regression_loss: 1.6480 - classification_loss: 0.5083\n",
            "13/86 [===>..........................] - ETA: 1:10 - loss: 2.1524 - regression_loss: 1.6432 - classification_loss: 0.5091\n",
            "14/86 [===>..........................] - ETA: 1:10 - loss: 2.1473 - regression_loss: 1.6352 - classification_loss: 0.5121\n",
            "15/86 [====>.........................] - ETA: 1:08 - loss: 2.1596 - regression_loss: 1.6455 - classification_loss: 0.5140\n",
            "16/86 [====>.........................] - ETA: 1:09 - loss: 2.1609 - regression_loss: 1.6443 - classification_loss: 0.5166\n",
            "17/86 [====>.........................] - ETA: 1:08 - loss: 2.1572 - regression_loss: 1.6411 - classification_loss: 0.5161\n",
            "18/86 [=====>........................] - ETA: 1:07 - loss: 2.1425 - regression_loss: 1.6323 - classification_loss: 0.5102\n",
            "19/86 [=====>........................] - ETA: 1:05 - loss: 2.1399 - regression_loss: 1.6283 - classification_loss: 0.5117\n",
            "20/86 [=====>........................] - ETA: 1:05 - loss: 2.1303 - regression_loss: 1.6149 - classification_loss: 0.5153\n",
            "21/86 [======>.......................] - ETA: 1:03 - loss: 2.1455 - regression_loss: 1.6267 - classification_loss: 0.5188\n",
            "22/86 [======>.......................] - ETA: 1:02 - loss: 2.1303 - regression_loss: 1.6147 - classification_loss: 0.5157\n",
            "23/86 [=======>......................] - ETA: 1:01 - loss: 2.1324 - regression_loss: 1.6155 - classification_loss: 0.5169\n",
            "24/86 [=======>......................] - ETA: 1:01 - loss: 2.1243 - regression_loss: 1.6103 - classification_loss: 0.5141\n",
            "25/86 [=======>......................] - ETA: 59s - loss: 2.1257 - regression_loss: 1.6119 - classification_loss: 0.5137 \n",
            "26/86 [========>.....................] - ETA: 59s - loss: 2.1204 - regression_loss: 1.6074 - classification_loss: 0.5130\n",
            "27/86 [========>.....................] - ETA: 58s - loss: 2.1170 - regression_loss: 1.6037 - classification_loss: 0.5133\n",
            "28/86 [========>.....................] - ETA: 57s - loss: 2.1180 - regression_loss: 1.6030 - classification_loss: 0.5150\n",
            "29/86 [=========>....................] - ETA: 56s - loss: 2.1113 - regression_loss: 1.6007 - classification_loss: 0.5106\n",
            "30/86 [=========>....................] - ETA: 55s - loss: 2.1070 - regression_loss: 1.5972 - classification_loss: 0.5098\n",
            "31/86 [=========>....................] - ETA: 54s - loss: 2.1153 - regression_loss: 1.6063 - classification_loss: 0.5090\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.1141 - regression_loss: 1.6046 - classification_loss: 0.5094\n",
            "33/86 [==========>...................] - ETA: 52s - loss: 2.1103 - regression_loss: 1.6022 - classification_loss: 0.5081\n",
            "34/86 [==========>...................] - ETA: 51s - loss: 2.1149 - regression_loss: 1.6063 - classification_loss: 0.5085\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.1084 - regression_loss: 1.6003 - classification_loss: 0.5081\n",
            "36/86 [===========>..................] - ETA: 49s - loss: 2.1099 - regression_loss: 1.6027 - classification_loss: 0.5073\n",
            "37/86 [===========>..................] - ETA: 48s - loss: 2.0994 - regression_loss: 1.5955 - classification_loss: 0.5039\n",
            "38/86 [============>.................] - ETA: 47s - loss: 2.0962 - regression_loss: 1.5928 - classification_loss: 0.5034\n",
            "39/86 [============>.................] - ETA: 46s - loss: 2.0918 - regression_loss: 1.5896 - classification_loss: 0.5022\n",
            "40/86 [============>.................] - ETA: 45s - loss: 2.0889 - regression_loss: 1.5864 - classification_loss: 0.5025\n",
            "41/86 [=============>................] - ETA: 44s - loss: 2.0812 - regression_loss: 1.5809 - classification_loss: 0.5003\n",
            "42/86 [=============>................] - ETA: 43s - loss: 2.0819 - regression_loss: 1.5809 - classification_loss: 0.5009\n",
            "43/86 [==============>...............] - ETA: 42s - loss: 2.0879 - regression_loss: 1.5864 - classification_loss: 0.5015\n",
            "44/86 [==============>...............] - ETA: 41s - loss: 2.0829 - regression_loss: 1.5825 - classification_loss: 0.5004\n",
            "45/86 [==============>...............] - ETA: 40s - loss: 2.0687 - regression_loss: 1.5717 - classification_loss: 0.4969\n",
            "46/86 [===============>..............] - ETA: 39s - loss: 2.0744 - regression_loss: 1.5756 - classification_loss: 0.4988\n",
            "47/86 [===============>..............] - ETA: 38s - loss: 2.0669 - regression_loss: 1.5702 - classification_loss: 0.4967\n",
            "48/86 [===============>..............] - ETA: 37s - loss: 2.0693 - regression_loss: 1.5732 - classification_loss: 0.4961\n",
            "49/86 [================>.............] - ETA: 36s - loss: 2.0668 - regression_loss: 1.5718 - classification_loss: 0.4950\n",
            "50/86 [================>.............] - ETA: 35s - loss: 2.0723 - regression_loss: 1.5741 - classification_loss: 0.4982\n",
            "51/86 [================>.............] - ETA: 34s - loss: 2.0738 - regression_loss: 1.5751 - classification_loss: 0.4987\n",
            "52/86 [=================>............] - ETA: 33s - loss: 2.0750 - regression_loss: 1.5766 - classification_loss: 0.4985\n",
            "53/86 [=================>............] - ETA: 32s - loss: 2.0654 - regression_loss: 1.5691 - classification_loss: 0.4963\n",
            "54/86 [=================>............] - ETA: 31s - loss: 2.0650 - regression_loss: 1.5686 - classification_loss: 0.4964\n",
            "55/86 [==================>...........] - ETA: 30s - loss: 2.0625 - regression_loss: 1.5673 - classification_loss: 0.4953\n",
            "56/86 [==================>...........] - ETA: 29s - loss: 2.0608 - regression_loss: 1.5658 - classification_loss: 0.4950\n",
            "57/86 [==================>...........] - ETA: 28s - loss: 2.0598 - regression_loss: 1.5643 - classification_loss: 0.4955\n",
            "58/86 [===================>..........] - ETA: 27s - loss: 2.0661 - regression_loss: 1.5701 - classification_loss: 0.4960\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.0696 - regression_loss: 1.5726 - classification_loss: 0.4970\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.0690 - regression_loss: 1.5720 - classification_loss: 0.4970\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.0682 - regression_loss: 1.5718 - classification_loss: 0.4964\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.0695 - regression_loss: 1.5741 - classification_loss: 0.4954\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.0674 - regression_loss: 1.5715 - classification_loss: 0.4959\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.0672 - regression_loss: 1.5714 - classification_loss: 0.4959\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.0659 - regression_loss: 1.5698 - classification_loss: 0.4961\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0644 - regression_loss: 1.5683 - classification_loss: 0.4961\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0672 - regression_loss: 1.5705 - classification_loss: 0.4966\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0630 - regression_loss: 1.5670 - classification_loss: 0.4960\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0602 - regression_loss: 1.5646 - classification_loss: 0.4956\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0612 - regression_loss: 1.5657 - classification_loss: 0.4956\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0650 - regression_loss: 1.5690 - classification_loss: 0.4960\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0618 - regression_loss: 1.5662 - classification_loss: 0.4956\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0634 - regression_loss: 1.5672 - classification_loss: 0.4961\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0654 - regression_loss: 1.5684 - classification_loss: 0.4971\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0662 - regression_loss: 1.5691 - classification_loss: 0.4971\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0666 - regression_loss: 1.5693 - classification_loss: 0.4974 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0714 - regression_loss: 1.5733 - classification_loss: 0.4981\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0714 - regression_loss: 1.5730 - classification_loss: 0.4983\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0733 - regression_loss: 1.5741 - classification_loss: 0.4992\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0787 - regression_loss: 1.5782 - classification_loss: 0.5004\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0802 - regression_loss: 1.5787 - classification_loss: 0.5015\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0826 - regression_loss: 1.5807 - classification_loss: 0.5019\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0815 - regression_loss: 1.5799 - classification_loss: 0.5016\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0816 - regression_loss: 1.5800 - classification_loss: 0.5016\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0818 - regression_loss: 1.5795 - classification_loss: 0.5022\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0810 - regression_loss: 1.5795 - classification_loss: 0.5015\n",
            "Epoch 53: saving model to ./snapshots\\resnet50_csv_53.h5\n",
            "\n",
            "86/86 [==============================] - 85s 982ms/step - loss: 2.0810 - regression_loss: 1.5795 - classification_loss: 0.5015 - lr: 1.0000e-23\n",
            "Epoch 54/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:19 - loss: 2.1465 - regression_loss: 1.6388 - classification_loss: 0.5077\n",
            " 2/86 [..............................] - ETA: 1:05 - loss: 1.9796 - regression_loss: 1.5290 - classification_loss: 0.4505\n",
            " 3/86 [>.............................] - ETA: 1:10 - loss: 2.0472 - regression_loss: 1.5663 - classification_loss: 0.4809\n",
            " 4/86 [>.............................] - ETA: 1:10 - loss: 2.0684 - regression_loss: 1.5614 - classification_loss: 0.5070\n",
            " 5/86 [>.............................] - ETA: 1:10 - loss: 2.0996 - regression_loss: 1.5809 - classification_loss: 0.5187\n",
            " 6/86 [=>............................] - ETA: 1:08 - loss: 2.0989 - regression_loss: 1.5814 - classification_loss: 0.5176\n",
            " 7/86 [=>............................] - ETA: 1:10 - loss: 2.1315 - regression_loss: 1.5984 - classification_loss: 0.5332\n",
            " 8/86 [=>............................] - ETA: 1:08 - loss: 2.1662 - regression_loss: 1.6293 - classification_loss: 0.5369\n",
            " 9/86 [==>...........................] - ETA: 1:10 - loss: 2.1405 - regression_loss: 1.6141 - classification_loss: 0.5265\n",
            "10/86 [==>...........................] - ETA: 1:09 - loss: 2.1388 - regression_loss: 1.6113 - classification_loss: 0.5275\n",
            "11/86 [==>...........................] - ETA: 1:11 - loss: 2.1280 - regression_loss: 1.6060 - classification_loss: 0.5221\n",
            "12/86 [===>..........................] - ETA: 1:12 - loss: 2.1259 - regression_loss: 1.6021 - classification_loss: 0.5238\n",
            "13/86 [===>..........................] - ETA: 1:11 - loss: 2.1095 - regression_loss: 1.5905 - classification_loss: 0.5191\n",
            "14/86 [===>..........................] - ETA: 1:09 - loss: 2.1264 - regression_loss: 1.6053 - classification_loss: 0.5211\n",
            "15/86 [====>.........................] - ETA: 1:08 - loss: 2.1403 - regression_loss: 1.6161 - classification_loss: 0.5242\n",
            "16/86 [====>.........................] - ETA: 1:08 - loss: 2.1308 - regression_loss: 1.6073 - classification_loss: 0.5236\n",
            "17/86 [====>.........................] - ETA: 1:07 - loss: 2.1412 - regression_loss: 1.6100 - classification_loss: 0.5312\n",
            "18/86 [=====>........................] - ETA: 1:06 - loss: 2.1427 - regression_loss: 1.6108 - classification_loss: 0.5319\n",
            "19/86 [=====>........................] - ETA: 1:06 - loss: 2.1430 - regression_loss: 1.6130 - classification_loss: 0.5301\n",
            "20/86 [=====>........................] - ETA: 1:05 - loss: 2.1679 - regression_loss: 1.6347 - classification_loss: 0.5332\n",
            "21/86 [======>.......................] - ETA: 1:03 - loss: 2.1531 - regression_loss: 1.6213 - classification_loss: 0.5319\n",
            "22/86 [======>.......................] - ETA: 1:02 - loss: 2.1511 - regression_loss: 1.6185 - classification_loss: 0.5326\n",
            "23/86 [=======>......................] - ETA: 1:02 - loss: 2.1365 - regression_loss: 1.6066 - classification_loss: 0.5299\n",
            "24/86 [=======>......................] - ETA: 1:02 - loss: 2.1386 - regression_loss: 1.6102 - classification_loss: 0.5284\n",
            "25/86 [=======>......................] - ETA: 1:01 - loss: 2.1454 - regression_loss: 1.6148 - classification_loss: 0.5306\n",
            "26/86 [========>.....................] - ETA: 1:00 - loss: 2.1457 - regression_loss: 1.6114 - classification_loss: 0.5343\n",
            "27/86 [========>.....................] - ETA: 59s - loss: 2.1322 - regression_loss: 1.6005 - classification_loss: 0.5318 \n",
            "28/86 [========>.....................] - ETA: 58s - loss: 2.1465 - regression_loss: 1.6115 - classification_loss: 0.5350\n",
            "29/86 [=========>....................] - ETA: 56s - loss: 2.1456 - regression_loss: 1.6111 - classification_loss: 0.5345\n",
            "30/86 [=========>....................] - ETA: 55s - loss: 2.1387 - regression_loss: 1.6055 - classification_loss: 0.5332\n",
            "31/86 [=========>....................] - ETA: 55s - loss: 2.1357 - regression_loss: 1.6026 - classification_loss: 0.5331\n",
            "32/86 [==========>...................] - ETA: 54s - loss: 2.1330 - regression_loss: 1.6018 - classification_loss: 0.5312\n",
            "33/86 [==========>...................] - ETA: 53s - loss: 2.1316 - regression_loss: 1.6000 - classification_loss: 0.5315\n",
            "34/86 [==========>...................] - ETA: 51s - loss: 2.1322 - regression_loss: 1.6010 - classification_loss: 0.5312\n",
            "35/86 [===========>..................] - ETA: 50s - loss: 2.1245 - regression_loss: 1.5962 - classification_loss: 0.5283\n",
            "36/86 [===========>..................] - ETA: 49s - loss: 2.1399 - regression_loss: 1.6090 - classification_loss: 0.5309\n",
            "37/86 [===========>..................] - ETA: 48s - loss: 2.1407 - regression_loss: 1.6106 - classification_loss: 0.5301\n",
            "38/86 [============>.................] - ETA: 47s - loss: 2.1399 - regression_loss: 1.6105 - classification_loss: 0.5295\n",
            "39/86 [============>.................] - ETA: 46s - loss: 2.1424 - regression_loss: 1.6134 - classification_loss: 0.5290\n",
            "40/86 [============>.................] - ETA: 45s - loss: 2.1333 - regression_loss: 1.6080 - classification_loss: 0.5253\n",
            "41/86 [=============>................] - ETA: 44s - loss: 2.1355 - regression_loss: 1.6094 - classification_loss: 0.5261\n",
            "42/86 [=============>................] - ETA: 43s - loss: 2.1341 - regression_loss: 1.6082 - classification_loss: 0.5259\n",
            "43/86 [==============>...............] - ETA: 42s - loss: 2.1313 - regression_loss: 1.6061 - classification_loss: 0.5252\n",
            "44/86 [==============>...............] - ETA: 41s - loss: 2.1312 - regression_loss: 1.6080 - classification_loss: 0.5231\n",
            "45/86 [==============>...............] - ETA: 40s - loss: 2.1319 - regression_loss: 1.6088 - classification_loss: 0.5231\n",
            "46/86 [===============>..............] - ETA: 39s - loss: 2.1347 - regression_loss: 1.6105 - classification_loss: 0.5242\n",
            "47/86 [===============>..............] - ETA: 38s - loss: 2.1314 - regression_loss: 1.6083 - classification_loss: 0.5231\n",
            "48/86 [===============>..............] - ETA: 37s - loss: 2.1355 - regression_loss: 1.6112 - classification_loss: 0.5243\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1329 - regression_loss: 1.6088 - classification_loss: 0.5240\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1342 - regression_loss: 1.6091 - classification_loss: 0.5251\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1361 - regression_loss: 1.6105 - classification_loss: 0.5256\n",
            "52/86 [=================>............] - ETA: 33s - loss: 2.1325 - regression_loss: 1.6082 - classification_loss: 0.5243\n",
            "53/86 [=================>............] - ETA: 32s - loss: 2.1313 - regression_loss: 1.6075 - classification_loss: 0.5238\n",
            "54/86 [=================>............] - ETA: 31s - loss: 2.1391 - regression_loss: 1.6125 - classification_loss: 0.5267\n",
            "55/86 [==================>...........] - ETA: 30s - loss: 2.1370 - regression_loss: 1.6107 - classification_loss: 0.5263\n",
            "56/86 [==================>...........] - ETA: 29s - loss: 2.1341 - regression_loss: 1.6082 - classification_loss: 0.5259\n",
            "57/86 [==================>...........] - ETA: 28s - loss: 2.1387 - regression_loss: 1.6117 - classification_loss: 0.5269\n",
            "58/86 [===================>..........] - ETA: 27s - loss: 2.1412 - regression_loss: 1.6147 - classification_loss: 0.5264\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.1315 - regression_loss: 1.6073 - classification_loss: 0.5242\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.1313 - regression_loss: 1.6081 - classification_loss: 0.5232\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.1306 - regression_loss: 1.6087 - classification_loss: 0.5219\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.1302 - regression_loss: 1.6086 - classification_loss: 0.5216\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.1316 - regression_loss: 1.6087 - classification_loss: 0.5229\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.1339 - regression_loss: 1.6114 - classification_loss: 0.5225\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1301 - regression_loss: 1.6082 - classification_loss: 0.5219\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1288 - regression_loss: 1.6072 - classification_loss: 0.5216\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1244 - regression_loss: 1.6042 - classification_loss: 0.5202\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1200 - regression_loss: 1.6009 - classification_loss: 0.5191\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1184 - regression_loss: 1.5996 - classification_loss: 0.5188\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1180 - regression_loss: 1.5988 - classification_loss: 0.5192\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1159 - regression_loss: 1.5980 - classification_loss: 0.5178\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1160 - regression_loss: 1.5980 - classification_loss: 0.5179\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1161 - regression_loss: 1.5982 - classification_loss: 0.5179\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1140 - regression_loss: 1.5970 - classification_loss: 0.5171\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1116 - regression_loss: 1.5952 - classification_loss: 0.5164\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1100 - regression_loss: 1.5940 - classification_loss: 0.5160 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1079 - regression_loss: 1.5929 - classification_loss: 0.5150\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1096 - regression_loss: 1.5934 - classification_loss: 0.5162\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1116 - regression_loss: 1.5946 - classification_loss: 0.5170\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1121 - regression_loss: 1.5948 - classification_loss: 0.5172\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1075 - regression_loss: 1.5909 - classification_loss: 0.5166\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1094 - regression_loss: 1.5920 - classification_loss: 0.5173\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1115 - regression_loss: 1.5939 - classification_loss: 0.5176\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1128 - regression_loss: 1.5952 - classification_loss: 0.5176\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1133 - regression_loss: 1.5956 - classification_loss: 0.5177\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1126 - regression_loss: 1.5949 - classification_loss: 0.5177\n",
            "Epoch 54: saving model to ./snapshots\\resnet50_csv_54.h5\n",
            "\n",
            "Epoch 54: ReduceLROnPlateau reducing learning rate to 9.999999998199588e-25.\n",
            "\n",
            "86/86 [==============================] - 84s 967ms/step - loss: 2.1126 - regression_loss: 1.5949 - classification_loss: 0.5177 - lr: 1.0000e-23\n",
            "Epoch 55/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:04 - loss: 2.2606 - regression_loss: 1.6888 - classification_loss: 0.5718\n",
            " 2/86 [..............................] - ETA: 1:04 - loss: 2.0747 - regression_loss: 1.5727 - classification_loss: 0.5020\n",
            " 3/86 [>.............................] - ETA: 1:18 - loss: 2.1513 - regression_loss: 1.6107 - classification_loss: 0.5406\n",
            " 4/86 [>.............................] - ETA: 1:21 - loss: 2.1254 - regression_loss: 1.6070 - classification_loss: 0.5184\n",
            " 5/86 [>.............................] - ETA: 1:14 - loss: 2.0750 - regression_loss: 1.5625 - classification_loss: 0.5125\n",
            " 6/86 [=>............................] - ETA: 1:15 - loss: 2.1645 - regression_loss: 1.6334 - classification_loss: 0.5311\n",
            " 7/86 [=>............................] - ETA: 1:13 - loss: 2.1836 - regression_loss: 1.6527 - classification_loss: 0.5309\n",
            " 8/86 [=>............................] - ETA: 1:12 - loss: 2.1732 - regression_loss: 1.6434 - classification_loss: 0.5297\n",
            " 9/86 [==>...........................] - ETA: 1:12 - loss: 2.2047 - regression_loss: 1.6693 - classification_loss: 0.5354\n",
            "10/86 [==>...........................] - ETA: 1:09 - loss: 2.1918 - regression_loss: 1.6606 - classification_loss: 0.5312\n",
            "11/86 [==>...........................] - ETA: 1:08 - loss: 2.1631 - regression_loss: 1.6367 - classification_loss: 0.5264\n",
            "12/86 [===>..........................] - ETA: 1:08 - loss: 2.1646 - regression_loss: 1.6420 - classification_loss: 0.5226\n",
            "13/86 [===>..........................] - ETA: 1:07 - loss: 2.1596 - regression_loss: 1.6370 - classification_loss: 0.5226\n",
            "14/86 [===>..........................] - ETA: 1:05 - loss: 2.1547 - regression_loss: 1.6334 - classification_loss: 0.5213\n",
            "15/86 [====>.........................] - ETA: 1:05 - loss: 2.1419 - regression_loss: 1.6250 - classification_loss: 0.5168\n",
            "16/86 [====>.........................] - ETA: 1:05 - loss: 2.1422 - regression_loss: 1.6247 - classification_loss: 0.5175\n",
            "17/86 [====>.........................] - ETA: 1:04 - loss: 2.1316 - regression_loss: 1.6146 - classification_loss: 0.5171\n",
            "18/86 [=====>........................] - ETA: 1:04 - loss: 2.1352 - regression_loss: 1.6134 - classification_loss: 0.5218\n",
            "19/86 [=====>........................] - ETA: 1:02 - loss: 2.1442 - regression_loss: 1.6216 - classification_loss: 0.5226\n",
            "20/86 [=====>........................] - ETA: 1:01 - loss: 2.1362 - regression_loss: 1.6161 - classification_loss: 0.5200\n",
            "21/86 [======>.......................] - ETA: 1:01 - loss: 2.1437 - regression_loss: 1.6208 - classification_loss: 0.5229\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.1352 - regression_loss: 1.6149 - classification_loss: 0.5203\n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.1238 - regression_loss: 1.6054 - classification_loss: 0.5184 \n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.1196 - regression_loss: 1.6043 - classification_loss: 0.5153\n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.1237 - regression_loss: 1.6071 - classification_loss: 0.5165\n",
            "26/86 [========>.....................] - ETA: 56s - loss: 2.1375 - regression_loss: 1.6182 - classification_loss: 0.5193\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.1298 - regression_loss: 1.6144 - classification_loss: 0.5154\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.1256 - regression_loss: 1.6111 - classification_loss: 0.5144\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.1248 - regression_loss: 1.6115 - classification_loss: 0.5133\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.1224 - regression_loss: 1.6095 - classification_loss: 0.5129\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.1196 - regression_loss: 1.6062 - classification_loss: 0.5134\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.1283 - regression_loss: 1.6147 - classification_loss: 0.5136\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.1304 - regression_loss: 1.6171 - classification_loss: 0.5134\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.1381 - regression_loss: 1.6223 - classification_loss: 0.5158\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.1346 - regression_loss: 1.6181 - classification_loss: 0.5165\n",
            "36/86 [===========>..................] - ETA: 48s - loss: 2.1304 - regression_loss: 1.6147 - classification_loss: 0.5157\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.1355 - regression_loss: 1.6193 - classification_loss: 0.5162\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.1330 - regression_loss: 1.6174 - classification_loss: 0.5156\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.1328 - regression_loss: 1.6178 - classification_loss: 0.5149\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.1250 - regression_loss: 1.6118 - classification_loss: 0.5131\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.1267 - regression_loss: 1.6125 - classification_loss: 0.5141\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1329 - regression_loss: 1.6171 - classification_loss: 0.5158\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1385 - regression_loss: 1.6211 - classification_loss: 0.5174\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1356 - regression_loss: 1.6181 - classification_loss: 0.5175\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.1341 - regression_loss: 1.6168 - classification_loss: 0.5173\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1302 - regression_loss: 1.6147 - classification_loss: 0.5155\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1325 - regression_loss: 1.6149 - classification_loss: 0.5177\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1310 - regression_loss: 1.6136 - classification_loss: 0.5174\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1314 - regression_loss: 1.6140 - classification_loss: 0.5173\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1389 - regression_loss: 1.6177 - classification_loss: 0.5213\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1416 - regression_loss: 1.6204 - classification_loss: 0.5213\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1338 - regression_loss: 1.6149 - classification_loss: 0.5189\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1326 - regression_loss: 1.6140 - classification_loss: 0.5186\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1304 - regression_loss: 1.6116 - classification_loss: 0.5187\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1346 - regression_loss: 1.6153 - classification_loss: 0.5193\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1252 - regression_loss: 1.6086 - classification_loss: 0.5166\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1215 - regression_loss: 1.6050 - classification_loss: 0.5165\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1192 - regression_loss: 1.6024 - classification_loss: 0.5168\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1129 - regression_loss: 1.5970 - classification_loss: 0.5159\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1123 - regression_loss: 1.5976 - classification_loss: 0.5147\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1164 - regression_loss: 1.6011 - classification_loss: 0.5153\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1203 - regression_loss: 1.6040 - classification_loss: 0.5163\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1105 - regression_loss: 1.5965 - classification_loss: 0.5141\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.1099 - regression_loss: 1.5954 - classification_loss: 0.5145\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1092 - regression_loss: 1.5947 - classification_loss: 0.5145\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1131 - regression_loss: 1.5993 - classification_loss: 0.5139\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1127 - regression_loss: 1.5987 - classification_loss: 0.5140\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1115 - regression_loss: 1.5971 - classification_loss: 0.5144\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1152 - regression_loss: 1.5999 - classification_loss: 0.5153\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1182 - regression_loss: 1.6024 - classification_loss: 0.5158\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1175 - regression_loss: 1.6016 - classification_loss: 0.5158\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1241 - regression_loss: 1.6075 - classification_loss: 0.5166\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1233 - regression_loss: 1.6066 - classification_loss: 0.5167\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1266 - regression_loss: 1.6079 - classification_loss: 0.5187\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1286 - regression_loss: 1.6092 - classification_loss: 0.5194\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1317 - regression_loss: 1.6114 - classification_loss: 0.5203 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1344 - regression_loss: 1.6132 - classification_loss: 0.5213\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1297 - regression_loss: 1.6098 - classification_loss: 0.5200\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1357 - regression_loss: 1.6147 - classification_loss: 0.5210\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1314 - regression_loss: 1.6117 - classification_loss: 0.5197\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1304 - regression_loss: 1.6113 - classification_loss: 0.5191\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1320 - regression_loss: 1.6132 - classification_loss: 0.5188\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1303 - regression_loss: 1.6122 - classification_loss: 0.5181\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1308 - regression_loss: 1.6127 - classification_loss: 0.5181\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1316 - regression_loss: 1.6137 - classification_loss: 0.5179\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1334 - regression_loss: 1.6157 - classification_loss: 0.5177\n",
            "Epoch 55: saving model to ./snapshots\\resnet50_csv_55.h5\n",
            "\n",
            "86/86 [==============================] - 83s 964ms/step - loss: 2.1334 - regression_loss: 1.6157 - classification_loss: 0.5177 - lr: 1.0000e-24\n",
            "Epoch 56/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:50 - loss: 2.1305 - regression_loss: 1.6057 - classification_loss: 0.5248\n",
            " 2/86 [..............................] - ETA: 1:17 - loss: 2.3768 - regression_loss: 1.7601 - classification_loss: 0.6167\n",
            " 3/86 [>.............................] - ETA: 1:28 - loss: 2.4597 - regression_loss: 1.8511 - classification_loss: 0.6087\n",
            " 4/86 [>.............................] - ETA: 1:20 - loss: 2.3854 - regression_loss: 1.7862 - classification_loss: 0.5992\n",
            " 5/86 [>.............................] - ETA: 1:18 - loss: 2.3024 - regression_loss: 1.7298 - classification_loss: 0.5726\n",
            " 6/86 [=>............................] - ETA: 1:19 - loss: 2.2884 - regression_loss: 1.7318 - classification_loss: 0.5567\n",
            " 7/86 [=>............................] - ETA: 1:17 - loss: 2.3026 - regression_loss: 1.7470 - classification_loss: 0.5556\n",
            " 8/86 [=>............................] - ETA: 1:14 - loss: 2.3179 - regression_loss: 1.7536 - classification_loss: 0.5643\n",
            " 9/86 [==>...........................] - ETA: 1:16 - loss: 2.2852 - regression_loss: 1.7300 - classification_loss: 0.5552\n",
            "10/86 [==>...........................] - ETA: 1:13 - loss: 2.2599 - regression_loss: 1.7090 - classification_loss: 0.5508\n",
            "11/86 [==>...........................] - ETA: 1:12 - loss: 2.2265 - regression_loss: 1.6789 - classification_loss: 0.5477\n",
            "12/86 [===>..........................] - ETA: 1:11 - loss: 2.2298 - regression_loss: 1.6838 - classification_loss: 0.5460\n",
            "13/86 [===>..........................] - ETA: 1:09 - loss: 2.2310 - regression_loss: 1.6814 - classification_loss: 0.5496\n",
            "14/86 [===>..........................] - ETA: 1:08 - loss: 2.2286 - regression_loss: 1.6803 - classification_loss: 0.5483\n",
            "15/86 [====>.........................] - ETA: 1:07 - loss: 2.2252 - regression_loss: 1.6785 - classification_loss: 0.5468\n",
            "16/86 [====>.........................] - ETA: 1:06 - loss: 2.2324 - regression_loss: 1.6832 - classification_loss: 0.5492\n",
            "17/86 [====>.........................] - ETA: 1:05 - loss: 2.2309 - regression_loss: 1.6807 - classification_loss: 0.5502\n",
            "18/86 [=====>........................] - ETA: 1:03 - loss: 2.2270 - regression_loss: 1.6778 - classification_loss: 0.5492\n",
            "19/86 [=====>........................] - ETA: 1:03 - loss: 2.2354 - regression_loss: 1.6877 - classification_loss: 0.5477\n",
            "20/86 [=====>........................] - ETA: 1:02 - loss: 2.2370 - regression_loss: 1.6902 - classification_loss: 0.5468\n",
            "21/86 [======>.......................] - ETA: 1:00 - loss: 2.2521 - regression_loss: 1.7010 - classification_loss: 0.5512\n",
            "22/86 [======>.......................] - ETA: 59s - loss: 2.2209 - regression_loss: 1.6787 - classification_loss: 0.5423 \n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.2197 - regression_loss: 1.6764 - classification_loss: 0.5433\n",
            "24/86 [=======>......................] - ETA: 57s - loss: 2.2064 - regression_loss: 1.6640 - classification_loss: 0.5424\n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.2026 - regression_loss: 1.6608 - classification_loss: 0.5418\n",
            "26/86 [========>.....................] - ETA: 56s - loss: 2.2001 - regression_loss: 1.6608 - classification_loss: 0.5393\n",
            "27/86 [========>.....................] - ETA: 55s - loss: 2.1875 - regression_loss: 1.6506 - classification_loss: 0.5370\n",
            "28/86 [========>.....................] - ETA: 54s - loss: 2.1734 - regression_loss: 1.6400 - classification_loss: 0.5334\n",
            "29/86 [=========>....................] - ETA: 53s - loss: 2.1815 - regression_loss: 1.6457 - classification_loss: 0.5358\n",
            "30/86 [=========>....................] - ETA: 52s - loss: 2.1730 - regression_loss: 1.6418 - classification_loss: 0.5312\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.1711 - regression_loss: 1.6388 - classification_loss: 0.5323\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.1703 - regression_loss: 1.6363 - classification_loss: 0.5339\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.1665 - regression_loss: 1.6324 - classification_loss: 0.5341\n",
            "34/86 [==========>...................] - ETA: 48s - loss: 2.1646 - regression_loss: 1.6295 - classification_loss: 0.5351\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.1626 - regression_loss: 1.6281 - classification_loss: 0.5345\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.1617 - regression_loss: 1.6274 - classification_loss: 0.5342\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.1536 - regression_loss: 1.6209 - classification_loss: 0.5327\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.1566 - regression_loss: 1.6233 - classification_loss: 0.5332\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.1456 - regression_loss: 1.6154 - classification_loss: 0.5301\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.1419 - regression_loss: 1.6134 - classification_loss: 0.5286\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.1487 - regression_loss: 1.6189 - classification_loss: 0.5298\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.1437 - regression_loss: 1.6157 - classification_loss: 0.5279\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.1413 - regression_loss: 1.6134 - classification_loss: 0.5279\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1407 - regression_loss: 1.6136 - classification_loss: 0.5270\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.1482 - regression_loss: 1.6195 - classification_loss: 0.5287\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1461 - regression_loss: 1.6178 - classification_loss: 0.5283\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.1436 - regression_loss: 1.6167 - classification_loss: 0.5269\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1410 - regression_loss: 1.6148 - classification_loss: 0.5262\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1361 - regression_loss: 1.6114 - classification_loss: 0.5247\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1389 - regression_loss: 1.6152 - classification_loss: 0.5237\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1406 - regression_loss: 1.6164 - classification_loss: 0.5243\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1353 - regression_loss: 1.6118 - classification_loss: 0.5235\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1391 - regression_loss: 1.6146 - classification_loss: 0.5245\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1364 - regression_loss: 1.6124 - classification_loss: 0.5239\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1341 - regression_loss: 1.6108 - classification_loss: 0.5233\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1325 - regression_loss: 1.6094 - classification_loss: 0.5231\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1332 - regression_loss: 1.6107 - classification_loss: 0.5225\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1280 - regression_loss: 1.6071 - classification_loss: 0.5209\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1280 - regression_loss: 1.6079 - classification_loss: 0.5201\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1254 - regression_loss: 1.6061 - classification_loss: 0.5193\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1236 - regression_loss: 1.6029 - classification_loss: 0.5207\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1285 - regression_loss: 1.6068 - classification_loss: 0.5217\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1294 - regression_loss: 1.6077 - classification_loss: 0.5217\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1281 - regression_loss: 1.6077 - classification_loss: 0.5204\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1291 - regression_loss: 1.6084 - classification_loss: 0.5207\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.1240 - regression_loss: 1.6041 - classification_loss: 0.5199\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.1209 - regression_loss: 1.6018 - classification_loss: 0.5191\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1220 - regression_loss: 1.6036 - classification_loss: 0.5184\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1253 - regression_loss: 1.6067 - classification_loss: 0.5186\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1227 - regression_loss: 1.6049 - classification_loss: 0.5177\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1198 - regression_loss: 1.6021 - classification_loss: 0.5177\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1173 - regression_loss: 1.6007 - classification_loss: 0.5167\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1190 - regression_loss: 1.6024 - classification_loss: 0.5166\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1212 - regression_loss: 1.6040 - classification_loss: 0.5172\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1216 - regression_loss: 1.6047 - classification_loss: 0.5169\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1253 - regression_loss: 1.6071 - classification_loss: 0.5181 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1215 - regression_loss: 1.6041 - classification_loss: 0.5174\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1205 - regression_loss: 1.6035 - classification_loss: 0.5170\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1205 - regression_loss: 1.6024 - classification_loss: 0.5180\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1197 - regression_loss: 1.6024 - classification_loss: 0.5174\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1205 - regression_loss: 1.6028 - classification_loss: 0.5177\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1181 - regression_loss: 1.6005 - classification_loss: 0.5176\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1169 - regression_loss: 1.6000 - classification_loss: 0.5169\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1141 - regression_loss: 1.5982 - classification_loss: 0.5159\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1155 - regression_loss: 1.5996 - classification_loss: 0.5159\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1176 - regression_loss: 1.6014 - classification_loss: 0.5162\n",
            "Epoch 56: saving model to ./snapshots\\resnet50_csv_56.h5\n",
            "\n",
            "Epoch 56: ReduceLROnPlateau reducing learning rate to 1.0000000195414814e-25.\n",
            "\n",
            "86/86 [==============================] - 82s 952ms/step - loss: 2.1176 - regression_loss: 1.6014 - classification_loss: 0.5162 - lr: 1.0000e-24\n",
            "Epoch 57/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:49 - loss: 2.2024 - regression_loss: 1.6516 - classification_loss: 0.5508\n",
            " 2/86 [..............................] - ETA: 1:54 - loss: 1.9834 - regression_loss: 1.5022 - classification_loss: 0.4812\n",
            " 3/86 [>.............................] - ETA: 1:28 - loss: 1.8479 - regression_loss: 1.4019 - classification_loss: 0.4460\n",
            " 4/86 [>.............................] - ETA: 1:31 - loss: 1.9406 - regression_loss: 1.4830 - classification_loss: 0.4576\n",
            " 5/86 [>.............................] - ETA: 1:24 - loss: 1.9551 - regression_loss: 1.5011 - classification_loss: 0.4541\n",
            " 6/86 [=>............................] - ETA: 1:20 - loss: 1.9956 - regression_loss: 1.5231 - classification_loss: 0.4725\n",
            " 7/86 [=>............................] - ETA: 1:18 - loss: 2.0182 - regression_loss: 1.5373 - classification_loss: 0.4809\n",
            " 8/86 [=>............................] - ETA: 1:16 - loss: 2.0639 - regression_loss: 1.5602 - classification_loss: 0.5036\n",
            " 9/86 [==>...........................] - ETA: 1:14 - loss: 2.1070 - regression_loss: 1.5893 - classification_loss: 0.5178\n",
            "10/86 [==>...........................] - ETA: 1:15 - loss: 2.0913 - regression_loss: 1.5825 - classification_loss: 0.5088\n",
            "11/86 [==>...........................] - ETA: 1:12 - loss: 2.0901 - regression_loss: 1.5799 - classification_loss: 0.5102\n",
            "12/86 [===>..........................] - ETA: 1:10 - loss: 2.0719 - regression_loss: 1.5700 - classification_loss: 0.5019\n",
            "13/86 [===>..........................] - ETA: 1:09 - loss: 2.0808 - regression_loss: 1.5786 - classification_loss: 0.5022\n",
            "14/86 [===>..........................] - ETA: 1:10 - loss: 2.0783 - regression_loss: 1.5805 - classification_loss: 0.4978\n",
            "15/86 [====>.........................] - ETA: 1:10 - loss: 2.0891 - regression_loss: 1.5900 - classification_loss: 0.4991\n",
            "16/86 [====>.........................] - ETA: 1:08 - loss: 2.0807 - regression_loss: 1.5839 - classification_loss: 0.4968\n",
            "17/86 [====>.........................] - ETA: 1:08 - loss: 2.0747 - regression_loss: 1.5784 - classification_loss: 0.4964\n",
            "18/86 [=====>........................] - ETA: 1:08 - loss: 2.0776 - regression_loss: 1.5782 - classification_loss: 0.4994\n",
            "19/86 [=====>........................] - ETA: 1:06 - loss: 2.0776 - regression_loss: 1.5771 - classification_loss: 0.5005\n",
            "20/86 [=====>........................] - ETA: 1:05 - loss: 2.0779 - regression_loss: 1.5739 - classification_loss: 0.5040\n",
            "21/86 [======>.......................] - ETA: 1:04 - loss: 2.0829 - regression_loss: 1.5817 - classification_loss: 0.5013\n",
            "22/86 [======>.......................] - ETA: 1:02 - loss: 2.0594 - regression_loss: 1.5645 - classification_loss: 0.4950\n",
            "23/86 [=======>......................] - ETA: 1:01 - loss: 2.0550 - regression_loss: 1.5599 - classification_loss: 0.4951\n",
            "24/86 [=======>......................] - ETA: 1:00 - loss: 2.0341 - regression_loss: 1.5435 - classification_loss: 0.4906\n",
            "25/86 [=======>......................] - ETA: 59s - loss: 2.0365 - regression_loss: 1.5445 - classification_loss: 0.4920 \n",
            "26/86 [========>.....................] - ETA: 58s - loss: 2.0278 - regression_loss: 1.5390 - classification_loss: 0.4888\n",
            "27/86 [========>.....................] - ETA: 58s - loss: 2.0226 - regression_loss: 1.5340 - classification_loss: 0.4886\n",
            "28/86 [========>.....................] - ETA: 56s - loss: 2.0218 - regression_loss: 1.5346 - classification_loss: 0.4872\n",
            "29/86 [=========>....................] - ETA: 56s - loss: 2.0247 - regression_loss: 1.5362 - classification_loss: 0.4886\n",
            "30/86 [=========>....................] - ETA: 55s - loss: 2.0303 - regression_loss: 1.5396 - classification_loss: 0.4907\n",
            "31/86 [=========>....................] - ETA: 54s - loss: 2.0264 - regression_loss: 1.5373 - classification_loss: 0.4891\n",
            "32/86 [==========>...................] - ETA: 53s - loss: 2.0215 - regression_loss: 1.5330 - classification_loss: 0.4885\n",
            "33/86 [==========>...................] - ETA: 52s - loss: 2.0209 - regression_loss: 1.5327 - classification_loss: 0.4882\n",
            "34/86 [==========>...................] - ETA: 51s - loss: 2.0227 - regression_loss: 1.5348 - classification_loss: 0.4879\n",
            "35/86 [===========>..................] - ETA: 50s - loss: 2.0132 - regression_loss: 1.5277 - classification_loss: 0.4855\n",
            "36/86 [===========>..................] - ETA: 49s - loss: 2.0196 - regression_loss: 1.5339 - classification_loss: 0.4858\n",
            "37/86 [===========>..................] - ETA: 48s - loss: 2.0293 - regression_loss: 1.5415 - classification_loss: 0.4878\n",
            "38/86 [============>.................] - ETA: 47s - loss: 2.0322 - regression_loss: 1.5447 - classification_loss: 0.4875\n",
            "39/86 [============>.................] - ETA: 46s - loss: 2.0376 - regression_loss: 1.5493 - classification_loss: 0.4883\n",
            "40/86 [============>.................] - ETA: 45s - loss: 2.0464 - regression_loss: 1.5556 - classification_loss: 0.4908\n",
            "41/86 [=============>................] - ETA: 44s - loss: 2.0485 - regression_loss: 1.5560 - classification_loss: 0.4925\n",
            "42/86 [=============>................] - ETA: 43s - loss: 2.0493 - regression_loss: 1.5561 - classification_loss: 0.4933\n",
            "43/86 [==============>...............] - ETA: 42s - loss: 2.0512 - regression_loss: 1.5555 - classification_loss: 0.4957\n",
            "44/86 [==============>...............] - ETA: 41s - loss: 2.0552 - regression_loss: 1.5569 - classification_loss: 0.4982\n",
            "45/86 [==============>...............] - ETA: 41s - loss: 2.0629 - regression_loss: 1.5643 - classification_loss: 0.4986\n",
            "46/86 [===============>..............] - ETA: 39s - loss: 2.0694 - regression_loss: 1.5695 - classification_loss: 0.4998\n",
            "47/86 [===============>..............] - ETA: 39s - loss: 2.0737 - regression_loss: 1.5738 - classification_loss: 0.4999\n",
            "48/86 [===============>..............] - ETA: 38s - loss: 2.0765 - regression_loss: 1.5765 - classification_loss: 0.5000\n",
            "49/86 [================>.............] - ETA: 37s - loss: 2.0746 - regression_loss: 1.5759 - classification_loss: 0.4987\n",
            "50/86 [================>.............] - ETA: 36s - loss: 2.0775 - regression_loss: 1.5796 - classification_loss: 0.4980\n",
            "51/86 [================>.............] - ETA: 35s - loss: 2.0728 - regression_loss: 1.5758 - classification_loss: 0.4970\n",
            "52/86 [=================>............] - ETA: 34s - loss: 2.0716 - regression_loss: 1.5741 - classification_loss: 0.4976\n",
            "53/86 [=================>............] - ETA: 33s - loss: 2.0742 - regression_loss: 1.5765 - classification_loss: 0.4977\n",
            "54/86 [=================>............] - ETA: 32s - loss: 2.0759 - regression_loss: 1.5780 - classification_loss: 0.4980\n",
            "55/86 [==================>...........] - ETA: 31s - loss: 2.0810 - regression_loss: 1.5816 - classification_loss: 0.4994\n",
            "56/86 [==================>...........] - ETA: 30s - loss: 2.0781 - regression_loss: 1.5799 - classification_loss: 0.4983\n",
            "57/86 [==================>...........] - ETA: 29s - loss: 2.0759 - regression_loss: 1.5781 - classification_loss: 0.4978\n",
            "58/86 [===================>..........] - ETA: 28s - loss: 2.0688 - regression_loss: 1.5730 - classification_loss: 0.4958\n",
            "59/86 [===================>..........] - ETA: 27s - loss: 2.0619 - regression_loss: 1.5684 - classification_loss: 0.4936\n",
            "60/86 [===================>..........] - ETA: 26s - loss: 2.0616 - regression_loss: 1.5680 - classification_loss: 0.4936\n",
            "61/86 [====================>.........] - ETA: 25s - loss: 2.0614 - regression_loss: 1.5668 - classification_loss: 0.4945\n",
            "62/86 [====================>.........] - ETA: 24s - loss: 2.0612 - regression_loss: 1.5659 - classification_loss: 0.4953\n",
            "63/86 [====================>.........] - ETA: 23s - loss: 2.0585 - regression_loss: 1.5643 - classification_loss: 0.4942\n",
            "64/86 [=====================>........] - ETA: 22s - loss: 2.0608 - regression_loss: 1.5662 - classification_loss: 0.4947\n",
            "65/86 [=====================>........] - ETA: 21s - loss: 2.0644 - regression_loss: 1.5681 - classification_loss: 0.4963\n",
            "66/86 [======================>.......] - ETA: 20s - loss: 2.0716 - regression_loss: 1.5732 - classification_loss: 0.4983\n",
            "67/86 [======================>.......] - ETA: 19s - loss: 2.0706 - regression_loss: 1.5717 - classification_loss: 0.4989\n",
            "68/86 [======================>.......] - ETA: 18s - loss: 2.0704 - regression_loss: 1.5711 - classification_loss: 0.4993\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0706 - regression_loss: 1.5712 - classification_loss: 0.4993\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0716 - regression_loss: 1.5719 - classification_loss: 0.4998\n",
            "71/86 [=======================>......] - ETA: 15s - loss: 2.0719 - regression_loss: 1.5715 - classification_loss: 0.5005\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0742 - regression_loss: 1.5741 - classification_loss: 0.5000\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0735 - regression_loss: 1.5740 - classification_loss: 0.4995\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0734 - regression_loss: 1.5739 - classification_loss: 0.4995\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0737 - regression_loss: 1.5741 - classification_loss: 0.4996\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0719 - regression_loss: 1.5721 - classification_loss: 0.4998 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0743 - regression_loss: 1.5734 - classification_loss: 0.5009\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0754 - regression_loss: 1.5740 - classification_loss: 0.5014\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0746 - regression_loss: 1.5731 - classification_loss: 0.5015\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0752 - regression_loss: 1.5736 - classification_loss: 0.5016\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0744 - regression_loss: 1.5725 - classification_loss: 0.5019\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0734 - regression_loss: 1.5718 - classification_loss: 0.5015\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0721 - regression_loss: 1.5710 - classification_loss: 0.5011\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0712 - regression_loss: 1.5696 - classification_loss: 0.5016\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0714 - regression_loss: 1.5696 - classification_loss: 0.5019\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0709 - regression_loss: 1.5694 - classification_loss: 0.5014\n",
            "Epoch 57: saving model to ./snapshots\\resnet50_csv_57.h5\n",
            "\n",
            "86/86 [==============================] - 86s 994ms/step - loss: 2.0709 - regression_loss: 1.5694 - classification_loss: 0.5014 - lr: 1.0000e-25\n",
            "Epoch 58/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:56 - loss: 2.0865 - regression_loss: 1.5872 - classification_loss: 0.4993\n",
            " 2/86 [..............................] - ETA: 1:23 - loss: 2.1391 - regression_loss: 1.6413 - classification_loss: 0.4978\n",
            " 3/86 [>.............................] - ETA: 1:18 - loss: 2.1308 - regression_loss: 1.6191 - classification_loss: 0.5117\n",
            " 4/86 [>.............................] - ETA: 1:16 - loss: 2.0446 - regression_loss: 1.5463 - classification_loss: 0.4983\n",
            " 5/86 [>.............................] - ETA: 1:16 - loss: 2.0661 - regression_loss: 1.5545 - classification_loss: 0.5116\n",
            " 6/86 [=>............................] - ETA: 1:18 - loss: 2.0888 - regression_loss: 1.5557 - classification_loss: 0.5331\n",
            " 7/86 [=>............................] - ETA: 1:16 - loss: 2.0736 - regression_loss: 1.5512 - classification_loss: 0.5224\n",
            " 8/86 [=>............................] - ETA: 1:12 - loss: 2.0397 - regression_loss: 1.5253 - classification_loss: 0.5144\n",
            " 9/86 [==>...........................] - ETA: 1:11 - loss: 1.9914 - regression_loss: 1.4927 - classification_loss: 0.4987\n",
            "10/86 [==>...........................] - ETA: 1:09 - loss: 1.9978 - regression_loss: 1.4939 - classification_loss: 0.5038\n",
            "11/86 [==>...........................] - ETA: 1:08 - loss: 1.9992 - regression_loss: 1.4953 - classification_loss: 0.5039\n",
            "12/86 [===>..........................] - ETA: 1:06 - loss: 2.0119 - regression_loss: 1.5046 - classification_loss: 0.5073\n",
            "13/86 [===>..........................] - ETA: 1:06 - loss: 2.0261 - regression_loss: 1.5174 - classification_loss: 0.5087\n",
            "14/86 [===>..........................] - ETA: 1:07 - loss: 2.0230 - regression_loss: 1.5125 - classification_loss: 0.5105\n",
            "15/86 [====>.........................] - ETA: 1:05 - loss: 2.0333 - regression_loss: 1.5225 - classification_loss: 0.5107\n",
            "16/86 [====>.........................] - ETA: 1:04 - loss: 2.0192 - regression_loss: 1.5119 - classification_loss: 0.5074\n",
            "17/86 [====>.........................] - ETA: 1:03 - loss: 2.0385 - regression_loss: 1.5310 - classification_loss: 0.5074\n",
            "18/86 [=====>........................] - ETA: 1:02 - loss: 2.0432 - regression_loss: 1.5345 - classification_loss: 0.5087\n",
            "19/86 [=====>........................] - ETA: 1:01 - loss: 2.0417 - regression_loss: 1.5317 - classification_loss: 0.5100\n",
            "20/86 [=====>........................] - ETA: 1:00 - loss: 2.0581 - regression_loss: 1.5447 - classification_loss: 0.5133\n",
            "21/86 [======>.......................] - ETA: 1:00 - loss: 2.0542 - regression_loss: 1.5405 - classification_loss: 0.5137\n",
            "22/86 [======>.......................] - ETA: 59s - loss: 2.0711 - regression_loss: 1.5505 - classification_loss: 0.5206 \n",
            "23/86 [=======>......................] - ETA: 58s - loss: 2.0549 - regression_loss: 1.5389 - classification_loss: 0.5160\n",
            "24/86 [=======>......................] - ETA: 57s - loss: 2.0579 - regression_loss: 1.5422 - classification_loss: 0.5157\n",
            "25/86 [=======>......................] - ETA: 56s - loss: 2.0489 - regression_loss: 1.5360 - classification_loss: 0.5129\n",
            "26/86 [========>.....................] - ETA: 55s - loss: 2.0570 - regression_loss: 1.5418 - classification_loss: 0.5152\n",
            "27/86 [========>.....................] - ETA: 54s - loss: 2.0617 - regression_loss: 1.5472 - classification_loss: 0.5145\n",
            "28/86 [========>.....................] - ETA: 54s - loss: 2.0689 - regression_loss: 1.5555 - classification_loss: 0.5134\n",
            "29/86 [=========>....................] - ETA: 52s - loss: 2.0617 - regression_loss: 1.5501 - classification_loss: 0.5117\n",
            "30/86 [=========>....................] - ETA: 52s - loss: 2.0599 - regression_loss: 1.5470 - classification_loss: 0.5128\n",
            "31/86 [=========>....................] - ETA: 51s - loss: 2.0681 - regression_loss: 1.5555 - classification_loss: 0.5126\n",
            "32/86 [==========>...................] - ETA: 50s - loss: 2.0676 - regression_loss: 1.5552 - classification_loss: 0.5124\n",
            "33/86 [==========>...................] - ETA: 49s - loss: 2.0698 - regression_loss: 1.5563 - classification_loss: 0.5135\n",
            "34/86 [==========>...................] - ETA: 48s - loss: 2.0704 - regression_loss: 1.5566 - classification_loss: 0.5138\n",
            "35/86 [===========>..................] - ETA: 47s - loss: 2.0692 - regression_loss: 1.5549 - classification_loss: 0.5143\n",
            "36/86 [===========>..................] - ETA: 46s - loss: 2.0743 - regression_loss: 1.5606 - classification_loss: 0.5137\n",
            "37/86 [===========>..................] - ETA: 45s - loss: 2.0772 - regression_loss: 1.5630 - classification_loss: 0.5142\n",
            "38/86 [============>.................] - ETA: 44s - loss: 2.0793 - regression_loss: 1.5651 - classification_loss: 0.5143\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.0782 - regression_loss: 1.5639 - classification_loss: 0.5143\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.0744 - regression_loss: 1.5606 - classification_loss: 0.5138\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.0696 - regression_loss: 1.5568 - classification_loss: 0.5129\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.0709 - regression_loss: 1.5577 - classification_loss: 0.5132\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.0687 - regression_loss: 1.5558 - classification_loss: 0.5129\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.0743 - regression_loss: 1.5612 - classification_loss: 0.5132\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.0748 - regression_loss: 1.5606 - classification_loss: 0.5143\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.0730 - regression_loss: 1.5592 - classification_loss: 0.5138\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.0724 - regression_loss: 1.5594 - classification_loss: 0.5130\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.0735 - regression_loss: 1.5602 - classification_loss: 0.5133\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.0785 - regression_loss: 1.5631 - classification_loss: 0.5154\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.0837 - regression_loss: 1.5676 - classification_loss: 0.5161\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.0859 - regression_loss: 1.5683 - classification_loss: 0.5176\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.0923 - regression_loss: 1.5739 - classification_loss: 0.5184\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0908 - regression_loss: 1.5737 - classification_loss: 0.5171\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0854 - regression_loss: 1.5696 - classification_loss: 0.5158\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0855 - regression_loss: 1.5702 - classification_loss: 0.5154\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0885 - regression_loss: 1.5721 - classification_loss: 0.5164\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0829 - regression_loss: 1.5687 - classification_loss: 0.5142\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0844 - regression_loss: 1.5701 - classification_loss: 0.5144\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0880 - regression_loss: 1.5730 - classification_loss: 0.5150\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0879 - regression_loss: 1.5722 - classification_loss: 0.5157\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0894 - regression_loss: 1.5733 - classification_loss: 0.5161\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.0896 - regression_loss: 1.5734 - classification_loss: 0.5161\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.0905 - regression_loss: 1.5748 - classification_loss: 0.5157\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.0820 - regression_loss: 1.5687 - classification_loss: 0.5133\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.0809 - regression_loss: 1.5685 - classification_loss: 0.5125\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.0816 - regression_loss: 1.5690 - classification_loss: 0.5126\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.0794 - regression_loss: 1.5677 - classification_loss: 0.5117\n",
            "68/86 [======================>.......] - ETA: 16s - loss: 2.0794 - regression_loss: 1.5684 - classification_loss: 0.5110\n",
            "69/86 [=======================>......] - ETA: 15s - loss: 2.0820 - regression_loss: 1.5707 - classification_loss: 0.5113\n",
            "70/86 [=======================>......] - ETA: 14s - loss: 2.0829 - regression_loss: 1.5708 - classification_loss: 0.5121\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0826 - regression_loss: 1.5710 - classification_loss: 0.5116\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0808 - regression_loss: 1.5699 - classification_loss: 0.5108\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0798 - regression_loss: 1.5695 - classification_loss: 0.5102\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0811 - regression_loss: 1.5709 - classification_loss: 0.5102\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0842 - regression_loss: 1.5732 - classification_loss: 0.5109\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0821 - regression_loss: 1.5712 - classification_loss: 0.5109 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0868 - regression_loss: 1.5740 - classification_loss: 0.5128\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0846 - regression_loss: 1.5708 - classification_loss: 0.5138\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0848 - regression_loss: 1.5712 - classification_loss: 0.5136\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0898 - regression_loss: 1.5749 - classification_loss: 0.5149\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0884 - regression_loss: 1.5741 - classification_loss: 0.5143\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0867 - regression_loss: 1.5727 - classification_loss: 0.5140\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0856 - regression_loss: 1.5719 - classification_loss: 0.5137\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0850 - regression_loss: 1.5712 - classification_loss: 0.5138\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0836 - regression_loss: 1.5705 - classification_loss: 0.5131\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0846 - regression_loss: 1.5710 - classification_loss: 0.5136\n",
            "Epoch 58: saving model to ./snapshots\\resnet50_csv_58.h5\n",
            "\n",
            "Epoch 58: ReduceLROnPlateau reducing learning rate to 1.0000000195414814e-26.\n",
            "\n",
            "86/86 [==============================] - 82s 950ms/step - loss: 2.0846 - regression_loss: 1.5710 - classification_loss: 0.5136 - lr: 1.0000e-25\n",
            "Epoch 59/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:52 - loss: 1.9172 - regression_loss: 1.4134 - classification_loss: 0.5038\n",
            " 2/86 [..............................] - ETA: 1:11 - loss: 2.1035 - regression_loss: 1.5811 - classification_loss: 0.5224\n",
            " 3/86 [>.............................] - ETA: 1:11 - loss: 2.2012 - regression_loss: 1.6562 - classification_loss: 0.5450\n",
            " 4/86 [>.............................] - ETA: 1:13 - loss: 2.1201 - regression_loss: 1.6023 - classification_loss: 0.5178\n",
            " 5/86 [>.............................] - ETA: 1:17 - loss: 2.0575 - regression_loss: 1.5567 - classification_loss: 0.5008\n",
            " 6/86 [=>............................] - ETA: 1:19 - loss: 2.0680 - regression_loss: 1.5643 - classification_loss: 0.5037\n",
            " 7/86 [=>............................] - ETA: 1:14 - loss: 2.0074 - regression_loss: 1.5206 - classification_loss: 0.4868\n",
            " 8/86 [=>............................] - ETA: 1:13 - loss: 1.9977 - regression_loss: 1.5136 - classification_loss: 0.4841\n",
            " 9/86 [==>...........................] - ETA: 1:13 - loss: 2.0646 - regression_loss: 1.5654 - classification_loss: 0.4992\n",
            "10/86 [==>...........................] - ETA: 1:11 - loss: 2.0390 - regression_loss: 1.5492 - classification_loss: 0.4897\n",
            "11/86 [==>...........................] - ETA: 1:11 - loss: 2.0581 - regression_loss: 1.5603 - classification_loss: 0.4978\n",
            "12/86 [===>..........................] - ETA: 1:09 - loss: 2.0665 - regression_loss: 1.5697 - classification_loss: 0.4968\n",
            "13/86 [===>..........................] - ETA: 1:08 - loss: 2.0748 - regression_loss: 1.5774 - classification_loss: 0.4973\n",
            "14/86 [===>..........................] - ETA: 1:07 - loss: 2.0676 - regression_loss: 1.5742 - classification_loss: 0.4934\n",
            "15/86 [====>.........................] - ETA: 1:06 - loss: 2.0754 - regression_loss: 1.5779 - classification_loss: 0.4975\n",
            "16/86 [====>.........................] - ETA: 1:06 - loss: 2.0817 - regression_loss: 1.5839 - classification_loss: 0.4978\n",
            "17/86 [====>.........................] - ETA: 1:04 - loss: 2.0663 - regression_loss: 1.5669 - classification_loss: 0.4994\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.0732 - regression_loss: 1.5718 - classification_loss: 0.5013\n",
            "19/86 [=====>........................] - ETA: 1:04 - loss: 2.0711 - regression_loss: 1.5683 - classification_loss: 0.5027\n",
            "20/86 [=====>........................] - ETA: 1:04 - loss: 2.0764 - regression_loss: 1.5711 - classification_loss: 0.5052\n",
            "21/86 [======>.......................] - ETA: 1:03 - loss: 2.0908 - regression_loss: 1.5786 - classification_loss: 0.5123\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.0978 - regression_loss: 1.5834 - classification_loss: 0.5144\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.1229 - regression_loss: 1.6002 - classification_loss: 0.5227\n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.1305 - regression_loss: 1.6064 - classification_loss: 0.5241 \n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.1314 - regression_loss: 1.6089 - classification_loss: 0.5224\n",
            "26/86 [========>.....................] - ETA: 58s - loss: 2.1286 - regression_loss: 1.6072 - classification_loss: 0.5214\n",
            "27/86 [========>.....................] - ETA: 57s - loss: 2.1293 - regression_loss: 1.6082 - classification_loss: 0.5211\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.1300 - regression_loss: 1.6085 - classification_loss: 0.5215\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.1298 - regression_loss: 1.6104 - classification_loss: 0.5194\n",
            "30/86 [=========>....................] - ETA: 54s - loss: 2.1324 - regression_loss: 1.6132 - classification_loss: 0.5192\n",
            "31/86 [=========>....................] - ETA: 53s - loss: 2.1302 - regression_loss: 1.6101 - classification_loss: 0.5201\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.1297 - regression_loss: 1.6102 - classification_loss: 0.5195\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.1364 - regression_loss: 1.6135 - classification_loss: 0.5228\n",
            "34/86 [==========>...................] - ETA: 51s - loss: 2.1315 - regression_loss: 1.6087 - classification_loss: 0.5228\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.1279 - regression_loss: 1.6058 - classification_loss: 0.5221\n",
            "36/86 [===========>..................] - ETA: 48s - loss: 2.1253 - regression_loss: 1.6047 - classification_loss: 0.5206\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.1252 - regression_loss: 1.6034 - classification_loss: 0.5219\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.1279 - regression_loss: 1.6058 - classification_loss: 0.5221\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.1291 - regression_loss: 1.6075 - classification_loss: 0.5216\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.1345 - regression_loss: 1.6128 - classification_loss: 0.5217\n",
            "41/86 [=============>................] - ETA: 44s - loss: 2.1236 - regression_loss: 1.6045 - classification_loss: 0.5191\n",
            "42/86 [=============>................] - ETA: 43s - loss: 2.1202 - regression_loss: 1.6023 - classification_loss: 0.5180\n",
            "43/86 [==============>...............] - ETA: 42s - loss: 2.1070 - regression_loss: 1.5921 - classification_loss: 0.5150\n",
            "44/86 [==============>...............] - ETA: 41s - loss: 2.1041 - regression_loss: 1.5897 - classification_loss: 0.5144\n",
            "45/86 [==============>...............] - ETA: 40s - loss: 2.1086 - regression_loss: 1.5937 - classification_loss: 0.5148\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1113 - regression_loss: 1.5969 - classification_loss: 0.5144\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1083 - regression_loss: 1.5947 - classification_loss: 0.5136\n",
            "48/86 [===============>..............] - ETA: 37s - loss: 2.1015 - regression_loss: 1.5905 - classification_loss: 0.5109\n",
            "49/86 [================>.............] - ETA: 36s - loss: 2.0989 - regression_loss: 1.5887 - classification_loss: 0.5103\n",
            "50/86 [================>.............] - ETA: 35s - loss: 2.0906 - regression_loss: 1.5818 - classification_loss: 0.5088\n",
            "51/86 [================>.............] - ETA: 34s - loss: 2.0906 - regression_loss: 1.5818 - classification_loss: 0.5088\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.0925 - regression_loss: 1.5825 - classification_loss: 0.5100\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0948 - regression_loss: 1.5843 - classification_loss: 0.5105\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0994 - regression_loss: 1.5887 - classification_loss: 0.5107\n",
            "55/86 [==================>...........] - ETA: 30s - loss: 2.0997 - regression_loss: 1.5896 - classification_loss: 0.5101\n",
            "56/86 [==================>...........] - ETA: 29s - loss: 2.1058 - regression_loss: 1.5943 - classification_loss: 0.5114\n",
            "57/86 [==================>...........] - ETA: 28s - loss: 2.1071 - regression_loss: 1.5956 - classification_loss: 0.5114\n",
            "58/86 [===================>..........] - ETA: 27s - loss: 2.1070 - regression_loss: 1.5956 - classification_loss: 0.5114\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.1053 - regression_loss: 1.5959 - classification_loss: 0.5094\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.1046 - regression_loss: 1.5956 - classification_loss: 0.5090\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.1057 - regression_loss: 1.5966 - classification_loss: 0.5090\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.0985 - regression_loss: 1.5915 - classification_loss: 0.5070\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.0967 - regression_loss: 1.5882 - classification_loss: 0.5086\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.1010 - regression_loss: 1.5916 - classification_loss: 0.5095\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.0946 - regression_loss: 1.5869 - classification_loss: 0.5077\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0929 - regression_loss: 1.5851 - classification_loss: 0.5079\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0926 - regression_loss: 1.5851 - classification_loss: 0.5075\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0993 - regression_loss: 1.5898 - classification_loss: 0.5095\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0995 - regression_loss: 1.5894 - classification_loss: 0.5101\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0975 - regression_loss: 1.5886 - classification_loss: 0.5089\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0981 - regression_loss: 1.5890 - classification_loss: 0.5091\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1012 - regression_loss: 1.5914 - classification_loss: 0.5098\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1021 - regression_loss: 1.5930 - classification_loss: 0.5091\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1007 - regression_loss: 1.5925 - classification_loss: 0.5083\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1022 - regression_loss: 1.5927 - classification_loss: 0.5095\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1037 - regression_loss: 1.5936 - classification_loss: 0.5101 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1033 - regression_loss: 1.5934 - classification_loss: 0.5099\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1041 - regression_loss: 1.5936 - classification_loss: 0.5105\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1045 - regression_loss: 1.5939 - classification_loss: 0.5106\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1073 - regression_loss: 1.5960 - classification_loss: 0.5114\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1044 - regression_loss: 1.5942 - classification_loss: 0.5102\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1045 - regression_loss: 1.5942 - classification_loss: 0.5103\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1046 - regression_loss: 1.5933 - classification_loss: 0.5114\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1068 - regression_loss: 1.5949 - classification_loss: 0.5119\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1094 - regression_loss: 1.5965 - classification_loss: 0.5129\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1100 - regression_loss: 1.5968 - classification_loss: 0.5132\n",
            "Epoch 59: saving model to ./snapshots\\resnet50_csv_59.h5\n",
            "\n",
            "86/86 [==============================] - 83s 966ms/step - loss: 2.1100 - regression_loss: 1.5968 - classification_loss: 0.5132 - lr: 1.0000e-26\n",
            "Epoch 60/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:42 - loss: 2.2360 - regression_loss: 1.6826 - classification_loss: 0.5534\n",
            " 2/86 [..............................] - ETA: 1:17 - loss: 2.1446 - regression_loss: 1.6055 - classification_loss: 0.5391\n",
            " 3/86 [>.............................] - ETA: 1:30 - loss: 2.1907 - regression_loss: 1.6454 - classification_loss: 0.5453\n",
            " 4/86 [>.............................] - ETA: 1:22 - loss: 2.2922 - regression_loss: 1.7049 - classification_loss: 0.5873\n",
            " 5/86 [>.............................] - ETA: 1:17 - loss: 2.2009 - regression_loss: 1.6329 - classification_loss: 0.5680\n",
            " 6/86 [=>............................] - ETA: 1:16 - loss: 2.1949 - regression_loss: 1.6422 - classification_loss: 0.5527\n",
            " 7/86 [=>............................] - ETA: 1:14 - loss: 2.2226 - regression_loss: 1.6645 - classification_loss: 0.5581\n",
            " 8/86 [=>............................] - ETA: 1:15 - loss: 2.1993 - regression_loss: 1.6459 - classification_loss: 0.5533\n",
            " 9/86 [==>...........................] - ETA: 1:14 - loss: 2.2319 - regression_loss: 1.6660 - classification_loss: 0.5659\n",
            "10/86 [==>...........................] - ETA: 1:13 - loss: 2.2217 - regression_loss: 1.6673 - classification_loss: 0.5544\n",
            "11/86 [==>...........................] - ETA: 1:10 - loss: 2.1832 - regression_loss: 1.6434 - classification_loss: 0.5398\n",
            "12/86 [===>..........................] - ETA: 1:12 - loss: 2.1684 - regression_loss: 1.6323 - classification_loss: 0.5361\n",
            "13/86 [===>..........................] - ETA: 1:10 - loss: 2.1664 - regression_loss: 1.6310 - classification_loss: 0.5353\n",
            "14/86 [===>..........................] - ETA: 1:09 - loss: 2.1984 - regression_loss: 1.6576 - classification_loss: 0.5408\n",
            "15/86 [====>.........................] - ETA: 1:09 - loss: 2.2062 - regression_loss: 1.6627 - classification_loss: 0.5435\n",
            "16/86 [====>.........................] - ETA: 1:08 - loss: 2.1984 - regression_loss: 1.6575 - classification_loss: 0.5409\n",
            "17/86 [====>.........................] - ETA: 1:07 - loss: 2.2053 - regression_loss: 1.6613 - classification_loss: 0.5440\n",
            "18/86 [=====>........................] - ETA: 1:06 - loss: 2.2180 - regression_loss: 1.6720 - classification_loss: 0.5460\n",
            "19/86 [=====>........................] - ETA: 1:04 - loss: 2.1926 - regression_loss: 1.6508 - classification_loss: 0.5418\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.1728 - regression_loss: 1.6374 - classification_loss: 0.5354\n",
            "21/86 [======>.......................] - ETA: 1:01 - loss: 2.1628 - regression_loss: 1.6303 - classification_loss: 0.5325\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.1580 - regression_loss: 1.6268 - classification_loss: 0.5311\n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.1342 - regression_loss: 1.6089 - classification_loss: 0.5253 \n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.1346 - regression_loss: 1.6073 - classification_loss: 0.5273\n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.1334 - regression_loss: 1.6054 - classification_loss: 0.5280\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.1276 - regression_loss: 1.6025 - classification_loss: 0.5251\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.1262 - regression_loss: 1.6022 - classification_loss: 0.5240\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.1155 - regression_loss: 1.5939 - classification_loss: 0.5215\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.1101 - regression_loss: 1.5900 - classification_loss: 0.5201\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.1136 - regression_loss: 1.5916 - classification_loss: 0.5220\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.1059 - regression_loss: 1.5863 - classification_loss: 0.5196\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.0980 - regression_loss: 1.5813 - classification_loss: 0.5167\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.1052 - regression_loss: 1.5870 - classification_loss: 0.5182\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.1072 - regression_loss: 1.5882 - classification_loss: 0.5190\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.1131 - regression_loss: 1.5933 - classification_loss: 0.5198\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.1094 - regression_loss: 1.5912 - classification_loss: 0.5183\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.1067 - regression_loss: 1.5916 - classification_loss: 0.5150\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.1087 - regression_loss: 1.5931 - classification_loss: 0.5155\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.1089 - regression_loss: 1.5943 - classification_loss: 0.5146\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.1061 - regression_loss: 1.5924 - classification_loss: 0.5137\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.1095 - regression_loss: 1.5946 - classification_loss: 0.5149\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.1046 - regression_loss: 1.5891 - classification_loss: 0.5155\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.1031 - regression_loss: 1.5882 - classification_loss: 0.5150\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.1001 - regression_loss: 1.5856 - classification_loss: 0.5145\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.0962 - regression_loss: 1.5829 - classification_loss: 0.5132\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.0918 - regression_loss: 1.5797 - classification_loss: 0.5121\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.0910 - regression_loss: 1.5788 - classification_loss: 0.5122\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.0848 - regression_loss: 1.5743 - classification_loss: 0.5105\n",
            "49/86 [================>.............] - ETA: 34s - loss: 2.0855 - regression_loss: 1.5745 - classification_loss: 0.5109\n",
            "50/86 [================>.............] - ETA: 33s - loss: 2.0887 - regression_loss: 1.5774 - classification_loss: 0.5113\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.0853 - regression_loss: 1.5741 - classification_loss: 0.5112\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.0879 - regression_loss: 1.5768 - classification_loss: 0.5112\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0863 - regression_loss: 1.5763 - classification_loss: 0.5100\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0916 - regression_loss: 1.5806 - classification_loss: 0.5109\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0898 - regression_loss: 1.5794 - classification_loss: 0.5103\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0933 - regression_loss: 1.5828 - classification_loss: 0.5105\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0965 - regression_loss: 1.5844 - classification_loss: 0.5121\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0924 - regression_loss: 1.5813 - classification_loss: 0.5111\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0934 - regression_loss: 1.5821 - classification_loss: 0.5113\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0988 - regression_loss: 1.5873 - classification_loss: 0.5115\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1036 - regression_loss: 1.5916 - classification_loss: 0.5120\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1042 - regression_loss: 1.5910 - classification_loss: 0.5132\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1031 - regression_loss: 1.5894 - classification_loss: 0.5137\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.0973 - regression_loss: 1.5850 - classification_loss: 0.5123\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.0931 - regression_loss: 1.5825 - classification_loss: 0.5106\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.0946 - regression_loss: 1.5826 - classification_loss: 0.5120\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.0890 - regression_loss: 1.5787 - classification_loss: 0.5103\n",
            "68/86 [======================>.......] - ETA: 16s - loss: 2.0919 - regression_loss: 1.5813 - classification_loss: 0.5106\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0906 - regression_loss: 1.5799 - classification_loss: 0.5107\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0938 - regression_loss: 1.5812 - classification_loss: 0.5126\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0942 - regression_loss: 1.5815 - classification_loss: 0.5127\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0939 - regression_loss: 1.5821 - classification_loss: 0.5118\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0912 - regression_loss: 1.5798 - classification_loss: 0.5113\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0949 - regression_loss: 1.5826 - classification_loss: 0.5123\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0973 - regression_loss: 1.5838 - classification_loss: 0.5135\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0964 - regression_loss: 1.5834 - classification_loss: 0.5130 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0962 - regression_loss: 1.5829 - classification_loss: 0.5133\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0985 - regression_loss: 1.5852 - classification_loss: 0.5133\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0954 - regression_loss: 1.5825 - classification_loss: 0.5130\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0960 - regression_loss: 1.5830 - classification_loss: 0.5130\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0942 - regression_loss: 1.5810 - classification_loss: 0.5132\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0932 - regression_loss: 1.5805 - classification_loss: 0.5127\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0933 - regression_loss: 1.5805 - classification_loss: 0.5128\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0934 - regression_loss: 1.5806 - classification_loss: 0.5128\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0919 - regression_loss: 1.5796 - classification_loss: 0.5122\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0918 - regression_loss: 1.5800 - classification_loss: 0.5118\n",
            "Epoch 60: saving model to ./snapshots\\resnet50_csv_60.h5\n",
            "\n",
            "Epoch 60: ReduceLROnPlateau reducing learning rate to 9.999999887266024e-28.\n",
            "\n",
            "86/86 [==============================] - 83s 959ms/step - loss: 2.0918 - regression_loss: 1.5800 - classification_loss: 0.5118 - lr: 1.0000e-26\n",
            "Epoch 61/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:45 - loss: 1.8861 - regression_loss: 1.4494 - classification_loss: 0.4367\n",
            " 2/86 [..............................] - ETA: 1:30 - loss: 2.1041 - regression_loss: 1.6260 - classification_loss: 0.4782\n",
            " 3/86 [>.............................] - ETA: 1:18 - loss: 2.1384 - regression_loss: 1.6603 - classification_loss: 0.4781\n",
            " 4/86 [>.............................] - ETA: 1:22 - loss: 2.1834 - regression_loss: 1.6632 - classification_loss: 0.5202\n",
            " 5/86 [>.............................] - ETA: 1:15 - loss: 2.2059 - regression_loss: 1.6832 - classification_loss: 0.5227\n",
            " 6/86 [=>............................] - ETA: 1:19 - loss: 2.1637 - regression_loss: 1.6616 - classification_loss: 0.5021\n",
            " 7/86 [=>............................] - ETA: 1:17 - loss: 2.1349 - regression_loss: 1.6285 - classification_loss: 0.5064\n",
            " 8/86 [=>............................] - ETA: 1:14 - loss: 2.1472 - regression_loss: 1.6362 - classification_loss: 0.5110\n",
            " 9/86 [==>...........................] - ETA: 1:10 - loss: 2.1430 - regression_loss: 1.6252 - classification_loss: 0.5178\n",
            "10/86 [==>...........................] - ETA: 1:13 - loss: 2.1342 - regression_loss: 1.6231 - classification_loss: 0.5111\n",
            "11/86 [==>...........................] - ETA: 1:11 - loss: 2.1342 - regression_loss: 1.6217 - classification_loss: 0.5125\n",
            "12/86 [===>..........................] - ETA: 1:10 - loss: 2.1191 - regression_loss: 1.6082 - classification_loss: 0.5109\n",
            "13/86 [===>..........................] - ETA: 1:08 - loss: 2.0959 - regression_loss: 1.5908 - classification_loss: 0.5052\n",
            "14/86 [===>..........................] - ETA: 1:08 - loss: 2.0719 - regression_loss: 1.5677 - classification_loss: 0.5043\n",
            "15/86 [====>.........................] - ETA: 1:07 - loss: 2.0783 - regression_loss: 1.5736 - classification_loss: 0.5047\n",
            "16/86 [====>.........................] - ETA: 1:05 - loss: 2.0793 - regression_loss: 1.5752 - classification_loss: 0.5042\n",
            "17/86 [====>.........................] - ETA: 1:05 - loss: 2.0662 - regression_loss: 1.5669 - classification_loss: 0.4992\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.0713 - regression_loss: 1.5711 - classification_loss: 0.5001\n",
            "19/86 [=====>........................] - ETA: 1:03 - loss: 2.0654 - regression_loss: 1.5665 - classification_loss: 0.4989\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.0765 - regression_loss: 1.5775 - classification_loss: 0.4989\n",
            "21/86 [======>.......................] - ETA: 1:01 - loss: 2.0724 - regression_loss: 1.5730 - classification_loss: 0.4994\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.0598 - regression_loss: 1.5631 - classification_loss: 0.4967\n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.0817 - regression_loss: 1.5773 - classification_loss: 0.5044 \n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.0767 - regression_loss: 1.5719 - classification_loss: 0.5049\n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.0793 - regression_loss: 1.5751 - classification_loss: 0.5041\n",
            "26/86 [========>.....................] - ETA: 56s - loss: 2.0670 - regression_loss: 1.5651 - classification_loss: 0.5019\n",
            "27/86 [========>.....................] - ETA: 55s - loss: 2.0811 - regression_loss: 1.5745 - classification_loss: 0.5066\n",
            "28/86 [========>.....................] - ETA: 54s - loss: 2.0765 - regression_loss: 1.5729 - classification_loss: 0.5036\n",
            "29/86 [=========>....................] - ETA: 53s - loss: 2.0878 - regression_loss: 1.5823 - classification_loss: 0.5055\n",
            "30/86 [=========>....................] - ETA: 52s - loss: 2.0894 - regression_loss: 1.5837 - classification_loss: 0.5056\n",
            "31/86 [=========>....................] - ETA: 51s - loss: 2.0837 - regression_loss: 1.5794 - classification_loss: 0.5043\n",
            "32/86 [==========>...................] - ETA: 50s - loss: 2.0842 - regression_loss: 1.5803 - classification_loss: 0.5039\n",
            "33/86 [==========>...................] - ETA: 49s - loss: 2.0865 - regression_loss: 1.5834 - classification_loss: 0.5031\n",
            "34/86 [==========>...................] - ETA: 48s - loss: 2.0871 - regression_loss: 1.5827 - classification_loss: 0.5044\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.0839 - regression_loss: 1.5802 - classification_loss: 0.5037\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.0796 - regression_loss: 1.5770 - classification_loss: 0.5026\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.0791 - regression_loss: 1.5779 - classification_loss: 0.5013\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.0771 - regression_loss: 1.5770 - classification_loss: 0.5002\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.0866 - regression_loss: 1.5848 - classification_loss: 0.5017\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.0842 - regression_loss: 1.5819 - classification_loss: 0.5024\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.0857 - regression_loss: 1.5827 - classification_loss: 0.5030\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.0870 - regression_loss: 1.5832 - classification_loss: 0.5038\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.0821 - regression_loss: 1.5793 - classification_loss: 0.5028\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.0819 - regression_loss: 1.5788 - classification_loss: 0.5031\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.0938 - regression_loss: 1.5886 - classification_loss: 0.5052\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.0999 - regression_loss: 1.5935 - classification_loss: 0.5064\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1008 - regression_loss: 1.5935 - classification_loss: 0.5073\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1016 - regression_loss: 1.5935 - classification_loss: 0.5081\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1020 - regression_loss: 1.5939 - classification_loss: 0.5081\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1065 - regression_loss: 1.5972 - classification_loss: 0.5093\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1092 - regression_loss: 1.5987 - classification_loss: 0.5105\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1158 - regression_loss: 1.6016 - classification_loss: 0.5142\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1163 - regression_loss: 1.6021 - classification_loss: 0.5142\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1176 - regression_loss: 1.6015 - classification_loss: 0.5162\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1101 - regression_loss: 1.5957 - classification_loss: 0.5144\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1115 - regression_loss: 1.5970 - classification_loss: 0.5145\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1094 - regression_loss: 1.5950 - classification_loss: 0.5144\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1036 - regression_loss: 1.5900 - classification_loss: 0.5136\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1072 - regression_loss: 1.5928 - classification_loss: 0.5143\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1045 - regression_loss: 1.5913 - classification_loss: 0.5132\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1109 - regression_loss: 1.5952 - classification_loss: 0.5157\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1082 - regression_loss: 1.5930 - classification_loss: 0.5152\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1110 - regression_loss: 1.5956 - classification_loss: 0.5154\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1178 - regression_loss: 1.5997 - classification_loss: 0.5181\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1178 - regression_loss: 1.5994 - classification_loss: 0.5183\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1165 - regression_loss: 1.5985 - classification_loss: 0.5180\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1166 - regression_loss: 1.5988 - classification_loss: 0.5177\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1176 - regression_loss: 1.5985 - classification_loss: 0.5191\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1206 - regression_loss: 1.6007 - classification_loss: 0.5199\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1167 - regression_loss: 1.5981 - classification_loss: 0.5185\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1159 - regression_loss: 1.5976 - classification_loss: 0.5182\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1138 - regression_loss: 1.5963 - classification_loss: 0.5175\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1151 - regression_loss: 1.5974 - classification_loss: 0.5178\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1111 - regression_loss: 1.5946 - classification_loss: 0.5165\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1130 - regression_loss: 1.5969 - classification_loss: 0.5161\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1152 - regression_loss: 1.5986 - classification_loss: 0.5165 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1118 - regression_loss: 1.5964 - classification_loss: 0.5154\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1120 - regression_loss: 1.5960 - classification_loss: 0.5160\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1108 - regression_loss: 1.5951 - classification_loss: 0.5157\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1119 - regression_loss: 1.5963 - classification_loss: 0.5155\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1083 - regression_loss: 1.5926 - classification_loss: 0.5157\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1079 - regression_loss: 1.5922 - classification_loss: 0.5158\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1082 - regression_loss: 1.5927 - classification_loss: 0.5155\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1075 - regression_loss: 1.5921 - classification_loss: 0.5154\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1029 - regression_loss: 1.5891 - classification_loss: 0.5138\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1002 - regression_loss: 1.5874 - classification_loss: 0.5128\n",
            "Epoch 61: saving model to ./snapshots\\resnet50_csv_61.h5\n",
            "\n",
            "86/86 [==============================] - 83s 963ms/step - loss: 2.1002 - regression_loss: 1.5874 - classification_loss: 0.5128 - lr: 1.0000e-27\n",
            "Epoch 62/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:27 - loss: 1.9854 - regression_loss: 1.5124 - classification_loss: 0.4730\n",
            " 2/86 [..............................] - ETA: 53s - loss: 2.0108 - regression_loss: 1.5445 - classification_loss: 0.4662 \n",
            " 3/86 [>.............................] - ETA: 1:00 - loss: 1.9884 - regression_loss: 1.5200 - classification_loss: 0.4683\n",
            " 4/86 [>.............................] - ETA: 1:05 - loss: 2.0320 - regression_loss: 1.5523 - classification_loss: 0.4796\n",
            " 5/86 [>.............................] - ETA: 1:14 - loss: 2.0491 - regression_loss: 1.5558 - classification_loss: 0.4933\n",
            " 6/86 [=>............................] - ETA: 1:12 - loss: 2.0386 - regression_loss: 1.5479 - classification_loss: 0.4907\n",
            " 7/86 [=>............................] - ETA: 1:13 - loss: 2.0489 - regression_loss: 1.5537 - classification_loss: 0.4952\n",
            " 8/86 [=>............................] - ETA: 1:13 - loss: 2.0564 - regression_loss: 1.5663 - classification_loss: 0.4901\n",
            " 9/86 [==>...........................] - ETA: 1:12 - loss: 2.0849 - regression_loss: 1.5861 - classification_loss: 0.4988\n",
            "10/86 [==>...........................] - ETA: 1:12 - loss: 2.1203 - regression_loss: 1.6012 - classification_loss: 0.5191\n",
            "11/86 [==>...........................] - ETA: 1:10 - loss: 2.1595 - regression_loss: 1.6308 - classification_loss: 0.5287\n",
            "12/86 [===>..........................] - ETA: 1:09 - loss: 2.1555 - regression_loss: 1.6269 - classification_loss: 0.5285\n",
            "13/86 [===>..........................] - ETA: 1:08 - loss: 2.1328 - regression_loss: 1.6088 - classification_loss: 0.5240\n",
            "14/86 [===>..........................] - ETA: 1:09 - loss: 2.1212 - regression_loss: 1.6009 - classification_loss: 0.5203\n",
            "15/86 [====>.........................] - ETA: 1:08 - loss: 2.1161 - regression_loss: 1.5997 - classification_loss: 0.5165\n",
            "16/86 [====>.........................] - ETA: 1:07 - loss: 2.1264 - regression_loss: 1.6096 - classification_loss: 0.5169\n",
            "17/86 [====>.........................] - ETA: 1:06 - loss: 2.1326 - regression_loss: 1.6130 - classification_loss: 0.5196\n",
            "18/86 [=====>........................] - ETA: 1:04 - loss: 2.1067 - regression_loss: 1.5946 - classification_loss: 0.5121\n",
            "19/86 [=====>........................] - ETA: 1:05 - loss: 2.1101 - regression_loss: 1.5997 - classification_loss: 0.5104\n",
            "20/86 [=====>........................] - ETA: 1:05 - loss: 2.1113 - regression_loss: 1.6009 - classification_loss: 0.5105\n",
            "21/86 [======>.......................] - ETA: 1:04 - loss: 2.1014 - regression_loss: 1.5944 - classification_loss: 0.5070\n",
            "22/86 [======>.......................] - ETA: 1:03 - loss: 2.1018 - regression_loss: 1.5917 - classification_loss: 0.5101\n",
            "23/86 [=======>......................] - ETA: 1:02 - loss: 2.1061 - regression_loss: 1.5933 - classification_loss: 0.5128\n",
            "24/86 [=======>......................] - ETA: 1:00 - loss: 2.1081 - regression_loss: 1.5951 - classification_loss: 0.5131\n",
            "25/86 [=======>......................] - ETA: 59s - loss: 2.1062 - regression_loss: 1.5936 - classification_loss: 0.5126 \n",
            "26/86 [========>.....................] - ETA: 58s - loss: 2.1096 - regression_loss: 1.5945 - classification_loss: 0.5151\n",
            "27/86 [========>.....................] - ETA: 57s - loss: 2.1123 - regression_loss: 1.5974 - classification_loss: 0.5149\n",
            "28/86 [========>.....................] - ETA: 56s - loss: 2.1248 - regression_loss: 1.6048 - classification_loss: 0.5200\n",
            "29/86 [=========>....................] - ETA: 55s - loss: 2.1340 - regression_loss: 1.6119 - classification_loss: 0.5220\n",
            "30/86 [=========>....................] - ETA: 54s - loss: 2.1415 - regression_loss: 1.6208 - classification_loss: 0.5207\n",
            "31/86 [=========>....................] - ETA: 53s - loss: 2.1334 - regression_loss: 1.6155 - classification_loss: 0.5179\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.1278 - regression_loss: 1.6111 - classification_loss: 0.5167\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.1177 - regression_loss: 1.6028 - classification_loss: 0.5149\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.1154 - regression_loss: 1.6003 - classification_loss: 0.5151\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.1130 - regression_loss: 1.5975 - classification_loss: 0.5155\n",
            "36/86 [===========>..................] - ETA: 49s - loss: 2.1109 - regression_loss: 1.5965 - classification_loss: 0.5143\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.1057 - regression_loss: 1.5934 - classification_loss: 0.5123\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.1047 - regression_loss: 1.5922 - classification_loss: 0.5126\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.0982 - regression_loss: 1.5882 - classification_loss: 0.5100\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.1095 - regression_loss: 1.5977 - classification_loss: 0.5118\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.1110 - regression_loss: 1.5996 - classification_loss: 0.5115\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1097 - regression_loss: 1.5991 - classification_loss: 0.5106\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1124 - regression_loss: 1.6007 - classification_loss: 0.5117\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1133 - regression_loss: 1.6025 - classification_loss: 0.5109\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.1151 - regression_loss: 1.6041 - classification_loss: 0.5109\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1152 - regression_loss: 1.6040 - classification_loss: 0.5111\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1146 - regression_loss: 1.6029 - classification_loss: 0.5118\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1111 - regression_loss: 1.5993 - classification_loss: 0.5119\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1076 - regression_loss: 1.5974 - classification_loss: 0.5102\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1071 - regression_loss: 1.5965 - classification_loss: 0.5106\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1070 - regression_loss: 1.5978 - classification_loss: 0.5092\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1125 - regression_loss: 1.6017 - classification_loss: 0.5108\n",
            "53/86 [=================>............] - ETA: 32s - loss: 2.1106 - regression_loss: 1.6013 - classification_loss: 0.5093\n",
            "54/86 [=================>............] - ETA: 31s - loss: 2.1090 - regression_loss: 1.6003 - classification_loss: 0.5087\n",
            "55/86 [==================>...........] - ETA: 30s - loss: 2.1073 - regression_loss: 1.5965 - classification_loss: 0.5109\n",
            "56/86 [==================>...........] - ETA: 29s - loss: 2.1084 - regression_loss: 1.5970 - classification_loss: 0.5114\n",
            "57/86 [==================>...........] - ETA: 28s - loss: 2.1086 - regression_loss: 1.5972 - classification_loss: 0.5114\n",
            "58/86 [===================>..........] - ETA: 27s - loss: 2.1065 - regression_loss: 1.5945 - classification_loss: 0.5120\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.1106 - regression_loss: 1.5984 - classification_loss: 0.5122\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.1096 - regression_loss: 1.5969 - classification_loss: 0.5127\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.1143 - regression_loss: 1.5993 - classification_loss: 0.5150\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.1137 - regression_loss: 1.5987 - classification_loss: 0.5150\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.1153 - regression_loss: 1.6000 - classification_loss: 0.5152\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.1124 - regression_loss: 1.5980 - classification_loss: 0.5144\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1091 - regression_loss: 1.5958 - classification_loss: 0.5133\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1064 - regression_loss: 1.5942 - classification_loss: 0.5122\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1015 - regression_loss: 1.5906 - classification_loss: 0.5108\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1004 - regression_loss: 1.5898 - classification_loss: 0.5106\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1043 - regression_loss: 1.5928 - classification_loss: 0.5114\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1037 - regression_loss: 1.5915 - classification_loss: 0.5122\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1062 - regression_loss: 1.5942 - classification_loss: 0.5119\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1013 - regression_loss: 1.5908 - classification_loss: 0.5105\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1004 - regression_loss: 1.5901 - classification_loss: 0.5103\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1040 - regression_loss: 1.5932 - classification_loss: 0.5108\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1023 - regression_loss: 1.5931 - classification_loss: 0.5092\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1000 - regression_loss: 1.5914 - classification_loss: 0.5086 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1006 - regression_loss: 1.5915 - classification_loss: 0.5091\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1038 - regression_loss: 1.5950 - classification_loss: 0.5088\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1082 - regression_loss: 1.5978 - classification_loss: 0.5104\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1108 - regression_loss: 1.5998 - classification_loss: 0.5111\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1103 - regression_loss: 1.5995 - classification_loss: 0.5108\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1113 - regression_loss: 1.6000 - classification_loss: 0.5113\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1154 - regression_loss: 1.6031 - classification_loss: 0.5124\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1159 - regression_loss: 1.6034 - classification_loss: 0.5125\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1170 - regression_loss: 1.6043 - classification_loss: 0.5127\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1183 - regression_loss: 1.6057 - classification_loss: 0.5126\n",
            "Epoch 62: saving model to ./snapshots\\resnet50_csv_62.h5\n",
            "\n",
            "Epoch 62: ReduceLROnPlateau reducing learning rate to 1.0000000272452012e-28.\n",
            "\n",
            "86/86 [==============================] - 86s 987ms/step - loss: 2.1183 - regression_loss: 1.6057 - classification_loss: 0.5126 - lr: 1.0000e-27\n",
            "Epoch 63/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:18 - loss: 2.2599 - regression_loss: 1.6830 - classification_loss: 0.5769\n",
            " 2/86 [..............................] - ETA: 1:06 - loss: 2.1834 - regression_loss: 1.6434 - classification_loss: 0.5400\n",
            " 3/86 [>.............................] - ETA: 1:25 - loss: 2.1716 - regression_loss: 1.6328 - classification_loss: 0.5388\n",
            " 4/86 [>.............................] - ETA: 1:31 - loss: 2.1565 - regression_loss: 1.6120 - classification_loss: 0.5445\n",
            " 5/86 [>.............................] - ETA: 1:28 - loss: 2.1643 - regression_loss: 1.6163 - classification_loss: 0.5481\n",
            " 6/86 [=>............................] - ETA: 1:31 - loss: 2.1611 - regression_loss: 1.6249 - classification_loss: 0.5362\n",
            " 7/86 [=>............................] - ETA: 1:29 - loss: 2.1753 - regression_loss: 1.6383 - classification_loss: 0.5370\n",
            " 8/86 [=>............................] - ETA: 1:24 - loss: 2.1611 - regression_loss: 1.6297 - classification_loss: 0.5314\n",
            " 9/86 [==>...........................] - ETA: 1:23 - loss: 2.1348 - regression_loss: 1.6061 - classification_loss: 0.5287\n",
            "10/86 [==>...........................] - ETA: 1:21 - loss: 2.1271 - regression_loss: 1.5976 - classification_loss: 0.5295\n",
            "11/86 [==>...........................] - ETA: 1:19 - loss: 2.1710 - regression_loss: 1.6345 - classification_loss: 0.5365\n",
            "12/86 [===>..........................] - ETA: 1:17 - loss: 2.1731 - regression_loss: 1.6373 - classification_loss: 0.5358\n",
            "13/86 [===>..........................] - ETA: 1:14 - loss: 2.1636 - regression_loss: 1.6300 - classification_loss: 0.5336\n",
            "14/86 [===>..........................] - ETA: 1:13 - loss: 2.1622 - regression_loss: 1.6296 - classification_loss: 0.5326\n",
            "15/86 [====>.........................] - ETA: 1:13 - loss: 2.1713 - regression_loss: 1.6352 - classification_loss: 0.5361\n",
            "16/86 [====>.........................] - ETA: 1:12 - loss: 2.1707 - regression_loss: 1.6365 - classification_loss: 0.5342\n",
            "17/86 [====>.........................] - ETA: 1:10 - loss: 2.1809 - regression_loss: 1.6437 - classification_loss: 0.5372\n",
            "18/86 [=====>........................] - ETA: 1:08 - loss: 2.1821 - regression_loss: 1.6454 - classification_loss: 0.5367\n",
            "19/86 [=====>........................] - ETA: 1:07 - loss: 2.1943 - regression_loss: 1.6553 - classification_loss: 0.5391\n",
            "20/86 [=====>........................] - ETA: 1:06 - loss: 2.1971 - regression_loss: 1.6580 - classification_loss: 0.5391\n",
            "21/86 [======>.......................] - ETA: 1:05 - loss: 2.1893 - regression_loss: 1.6548 - classification_loss: 0.5344\n",
            "22/86 [======>.......................] - ETA: 1:03 - loss: 2.1757 - regression_loss: 1.6440 - classification_loss: 0.5317\n",
            "23/86 [=======>......................] - ETA: 1:03 - loss: 2.1660 - regression_loss: 1.6357 - classification_loss: 0.5304\n",
            "24/86 [=======>......................] - ETA: 1:01 - loss: 2.1567 - regression_loss: 1.6280 - classification_loss: 0.5288\n",
            "25/86 [=======>......................] - ETA: 1:00 - loss: 2.1446 - regression_loss: 1.6174 - classification_loss: 0.5272\n",
            "26/86 [========>.....................] - ETA: 59s - loss: 2.1316 - regression_loss: 1.6075 - classification_loss: 0.5241 \n",
            "27/86 [========>.....................] - ETA: 57s - loss: 2.1358 - regression_loss: 1.6114 - classification_loss: 0.5244\n",
            "28/86 [========>.....................] - ETA: 56s - loss: 2.1423 - regression_loss: 1.6176 - classification_loss: 0.5247\n",
            "29/86 [=========>....................] - ETA: 55s - loss: 2.1377 - regression_loss: 1.6129 - classification_loss: 0.5249\n",
            "30/86 [=========>....................] - ETA: 54s - loss: 2.1378 - regression_loss: 1.6144 - classification_loss: 0.5234\n",
            "31/86 [=========>....................] - ETA: 53s - loss: 2.1324 - regression_loss: 1.6108 - classification_loss: 0.5216\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.1363 - regression_loss: 1.6109 - classification_loss: 0.5254\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.1386 - regression_loss: 1.6132 - classification_loss: 0.5253\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.1493 - regression_loss: 1.6219 - classification_loss: 0.5275\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.1390 - regression_loss: 1.6144 - classification_loss: 0.5246\n",
            "36/86 [===========>..................] - ETA: 48s - loss: 2.1253 - regression_loss: 1.6042 - classification_loss: 0.5211\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.1255 - regression_loss: 1.6051 - classification_loss: 0.5204\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.1241 - regression_loss: 1.6041 - classification_loss: 0.5200\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.1159 - regression_loss: 1.5976 - classification_loss: 0.5183\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.1215 - regression_loss: 1.6032 - classification_loss: 0.5182\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.1192 - regression_loss: 1.6028 - classification_loss: 0.5163\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1238 - regression_loss: 1.6066 - classification_loss: 0.5172\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1206 - regression_loss: 1.6046 - classification_loss: 0.5160\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1167 - regression_loss: 1.6021 - classification_loss: 0.5146\n",
            "45/86 [==============>...............] - ETA: 40s - loss: 2.1186 - regression_loss: 1.6028 - classification_loss: 0.5158\n",
            "46/86 [===============>..............] - ETA: 39s - loss: 2.1138 - regression_loss: 1.6005 - classification_loss: 0.5133\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1158 - regression_loss: 1.6006 - classification_loss: 0.5152\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1145 - regression_loss: 1.6002 - classification_loss: 0.5143\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1085 - regression_loss: 1.5955 - classification_loss: 0.5130\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.0985 - regression_loss: 1.5882 - classification_loss: 0.5103\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.0982 - regression_loss: 1.5881 - classification_loss: 0.5101\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.0980 - regression_loss: 1.5876 - classification_loss: 0.5104\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1023 - regression_loss: 1.5908 - classification_loss: 0.5115\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1022 - regression_loss: 1.5903 - classification_loss: 0.5119\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0999 - regression_loss: 1.5889 - classification_loss: 0.5111\n",
            "56/86 [==================>...........] - ETA: 29s - loss: 2.1057 - regression_loss: 1.5945 - classification_loss: 0.5112\n",
            "57/86 [==================>...........] - ETA: 28s - loss: 2.1074 - regression_loss: 1.5948 - classification_loss: 0.5126\n",
            "58/86 [===================>..........] - ETA: 27s - loss: 2.1056 - regression_loss: 1.5926 - classification_loss: 0.5130\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.1037 - regression_loss: 1.5906 - classification_loss: 0.5131\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.1048 - regression_loss: 1.5917 - classification_loss: 0.5131\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.1029 - regression_loss: 1.5900 - classification_loss: 0.5129\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.1077 - regression_loss: 1.5928 - classification_loss: 0.5149\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.1047 - regression_loss: 1.5903 - classification_loss: 0.5144\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.1031 - regression_loss: 1.5885 - classification_loss: 0.5146\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1056 - regression_loss: 1.5917 - classification_loss: 0.5139\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1004 - regression_loss: 1.5877 - classification_loss: 0.5127\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0972 - regression_loss: 1.5853 - classification_loss: 0.5120\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0950 - regression_loss: 1.5834 - classification_loss: 0.5116\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0959 - regression_loss: 1.5839 - classification_loss: 0.5120\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1006 - regression_loss: 1.5878 - classification_loss: 0.5128\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1064 - regression_loss: 1.5925 - classification_loss: 0.5139\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1091 - regression_loss: 1.5946 - classification_loss: 0.5145\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1050 - regression_loss: 1.5910 - classification_loss: 0.5140\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0997 - regression_loss: 1.5874 - classification_loss: 0.5123\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1004 - regression_loss: 1.5879 - classification_loss: 0.5124\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1020 - regression_loss: 1.5893 - classification_loss: 0.5127 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1018 - regression_loss: 1.5889 - classification_loss: 0.5128\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1004 - regression_loss: 1.5882 - classification_loss: 0.5122\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0984 - regression_loss: 1.5875 - classification_loss: 0.5109\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0994 - regression_loss: 1.5879 - classification_loss: 0.5114\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1020 - regression_loss: 1.5904 - classification_loss: 0.5116\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1019 - regression_loss: 1.5908 - classification_loss: 0.5112\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1036 - regression_loss: 1.5911 - classification_loss: 0.5125\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1044 - regression_loss: 1.5924 - classification_loss: 0.5120\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1053 - regression_loss: 1.5930 - classification_loss: 0.5122\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1059 - regression_loss: 1.5938 - classification_loss: 0.5120\n",
            "Epoch 63: saving model to ./snapshots\\resnet50_csv_63.h5\n",
            "\n",
            "86/86 [==============================] - 85s 977ms/step - loss: 2.1059 - regression_loss: 1.5938 - classification_loss: 0.5120 - lr: 1.0000e-28\n",
            "Epoch 64/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:44 - loss: 1.8056 - regression_loss: 1.3718 - classification_loss: 0.4338\n",
            " 2/86 [..............................] - ETA: 1:16 - loss: 1.9713 - regression_loss: 1.4808 - classification_loss: 0.4905\n",
            " 3/86 [>.............................] - ETA: 1:30 - loss: 1.9894 - regression_loss: 1.4904 - classification_loss: 0.4991\n",
            " 4/86 [>.............................] - ETA: 1:23 - loss: 1.9821 - regression_loss: 1.4755 - classification_loss: 0.5066\n",
            " 5/86 [>.............................] - ETA: 1:27 - loss: 1.9863 - regression_loss: 1.4822 - classification_loss: 0.5042\n",
            " 6/86 [=>............................] - ETA: 1:21 - loss: 2.0064 - regression_loss: 1.4913 - classification_loss: 0.5151\n",
            " 7/86 [=>............................] - ETA: 1:17 - loss: 2.0409 - regression_loss: 1.5079 - classification_loss: 0.5330\n",
            " 8/86 [=>............................] - ETA: 1:17 - loss: 2.0373 - regression_loss: 1.5122 - classification_loss: 0.5251\n",
            " 9/86 [==>...........................] - ETA: 1:15 - loss: 2.0423 - regression_loss: 1.5177 - classification_loss: 0.5246\n",
            "10/86 [==>...........................] - ETA: 1:14 - loss: 2.0656 - regression_loss: 1.5392 - classification_loss: 0.5264\n",
            "11/86 [==>...........................] - ETA: 1:11 - loss: 2.0566 - regression_loss: 1.5324 - classification_loss: 0.5242\n",
            "12/86 [===>..........................] - ETA: 1:11 - loss: 2.0949 - regression_loss: 1.5649 - classification_loss: 0.5300\n",
            "13/86 [===>..........................] - ETA: 1:12 - loss: 2.0905 - regression_loss: 1.5615 - classification_loss: 0.5291\n",
            "14/86 [===>..........................] - ETA: 1:10 - loss: 2.0744 - regression_loss: 1.5495 - classification_loss: 0.5249\n",
            "15/86 [====>.........................] - ETA: 1:09 - loss: 2.0878 - regression_loss: 1.5599 - classification_loss: 0.5279\n",
            "16/86 [====>.........................] - ETA: 1:08 - loss: 2.0826 - regression_loss: 1.5550 - classification_loss: 0.5276\n",
            "17/86 [====>.........................] - ETA: 1:06 - loss: 2.0690 - regression_loss: 1.5455 - classification_loss: 0.5235\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.0660 - regression_loss: 1.5437 - classification_loss: 0.5223\n",
            "19/86 [=====>........................] - ETA: 1:04 - loss: 2.0746 - regression_loss: 1.5509 - classification_loss: 0.5237\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.0710 - regression_loss: 1.5488 - classification_loss: 0.5222\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.0608 - regression_loss: 1.5424 - classification_loss: 0.5184\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.0762 - regression_loss: 1.5559 - classification_loss: 0.5203\n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.0974 - regression_loss: 1.5735 - classification_loss: 0.5240 \n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.1090 - regression_loss: 1.5826 - classification_loss: 0.5264\n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.1132 - regression_loss: 1.5876 - classification_loss: 0.5256\n",
            "26/86 [========>.....................] - ETA: 56s - loss: 2.1204 - regression_loss: 1.5944 - classification_loss: 0.5260\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.1164 - regression_loss: 1.5919 - classification_loss: 0.5245\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.1116 - regression_loss: 1.5883 - classification_loss: 0.5233\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.1169 - regression_loss: 1.5951 - classification_loss: 0.5217\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.1152 - regression_loss: 1.5931 - classification_loss: 0.5221\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.1132 - regression_loss: 1.5906 - classification_loss: 0.5226\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.1178 - regression_loss: 1.5955 - classification_loss: 0.5223\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.1161 - regression_loss: 1.5950 - classification_loss: 0.5210\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.1177 - regression_loss: 1.5966 - classification_loss: 0.5211\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.1123 - regression_loss: 1.5936 - classification_loss: 0.5187\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.1061 - regression_loss: 1.5875 - classification_loss: 0.5186\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.1160 - regression_loss: 1.5952 - classification_loss: 0.5208\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.1147 - regression_loss: 1.5954 - classification_loss: 0.5193\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.1224 - regression_loss: 1.6013 - classification_loss: 0.5211\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.1216 - regression_loss: 1.6016 - classification_loss: 0.5200\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.1307 - regression_loss: 1.6090 - classification_loss: 0.5216\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1287 - regression_loss: 1.6088 - classification_loss: 0.5199\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1230 - regression_loss: 1.6052 - classification_loss: 0.5178\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1172 - regression_loss: 1.6004 - classification_loss: 0.5168\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.1125 - regression_loss: 1.5983 - classification_loss: 0.5142\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1113 - regression_loss: 1.5978 - classification_loss: 0.5135\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1148 - regression_loss: 1.5998 - classification_loss: 0.5150\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1177 - regression_loss: 1.6019 - classification_loss: 0.5158\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1100 - regression_loss: 1.5961 - classification_loss: 0.5139\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1130 - regression_loss: 1.5967 - classification_loss: 0.5163\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1058 - regression_loss: 1.5921 - classification_loss: 0.5137\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1078 - regression_loss: 1.5938 - classification_loss: 0.5141\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1094 - regression_loss: 1.5963 - classification_loss: 0.5130\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1140 - regression_loss: 1.6003 - classification_loss: 0.5138\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1086 - regression_loss: 1.5965 - classification_loss: 0.5122\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1097 - regression_loss: 1.5965 - classification_loss: 0.5132\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1112 - regression_loss: 1.5973 - classification_loss: 0.5139\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1122 - regression_loss: 1.5990 - classification_loss: 0.5132\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1137 - regression_loss: 1.6006 - classification_loss: 0.5131\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1165 - regression_loss: 1.6023 - classification_loss: 0.5142\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.1116 - regression_loss: 1.5984 - classification_loss: 0.5132\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.1036 - regression_loss: 1.5925 - classification_loss: 0.5111\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.1083 - regression_loss: 1.5965 - classification_loss: 0.5119\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.1054 - regression_loss: 1.5945 - classification_loss: 0.5109\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1052 - regression_loss: 1.5950 - classification_loss: 0.5103\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1052 - regression_loss: 1.5947 - classification_loss: 0.5105\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1081 - regression_loss: 1.5971 - classification_loss: 0.5110\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1042 - regression_loss: 1.5943 - classification_loss: 0.5099\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1033 - regression_loss: 1.5943 - classification_loss: 0.5090\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0995 - regression_loss: 1.5912 - classification_loss: 0.5083\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0941 - regression_loss: 1.5868 - classification_loss: 0.5073\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0921 - regression_loss: 1.5852 - classification_loss: 0.5069\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0911 - regression_loss: 1.5842 - classification_loss: 0.5069\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0874 - regression_loss: 1.5811 - classification_loss: 0.5063\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0873 - regression_loss: 1.5804 - classification_loss: 0.5069\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0899 - regression_loss: 1.5820 - classification_loss: 0.5079 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0874 - regression_loss: 1.5803 - classification_loss: 0.5070\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0869 - regression_loss: 1.5795 - classification_loss: 0.5074\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0878 - regression_loss: 1.5800 - classification_loss: 0.5078\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0872 - regression_loss: 1.5801 - classification_loss: 0.5071\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0886 - regression_loss: 1.5815 - classification_loss: 0.5071\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0922 - regression_loss: 1.5840 - classification_loss: 0.5082\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0865 - regression_loss: 1.5799 - classification_loss: 0.5066\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0873 - regression_loss: 1.5805 - classification_loss: 0.5068\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0909 - regression_loss: 1.5832 - classification_loss: 0.5077\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0891 - regression_loss: 1.5822 - classification_loss: 0.5069\n",
            "Epoch 64: saving model to ./snapshots\\resnet50_csv_64.h5\n",
            "\n",
            "Epoch 64: ReduceLROnPlateau reducing learning rate to 1.0000000031710769e-29.\n",
            "\n",
            "86/86 [==============================] - 83s 962ms/step - loss: 2.0891 - regression_loss: 1.5822 - classification_loss: 0.5069 - lr: 1.0000e-28\n",
            "Epoch 65/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:51 - loss: 2.2949 - regression_loss: 1.7446 - classification_loss: 0.5502\n",
            " 2/86 [..............................] - ETA: 1:42 - loss: 2.2300 - regression_loss: 1.7061 - classification_loss: 0.5238\n",
            " 3/86 [>.............................] - ETA: 1:33 - loss: 2.1659 - regression_loss: 1.6465 - classification_loss: 0.5194\n",
            " 4/86 [>.............................] - ETA: 1:21 - loss: 2.0827 - regression_loss: 1.5802 - classification_loss: 0.5025\n",
            " 5/86 [>.............................] - ETA: 1:23 - loss: 2.1345 - regression_loss: 1.6052 - classification_loss: 0.5293\n",
            " 6/86 [=>............................] - ETA: 1:18 - loss: 2.0917 - regression_loss: 1.5630 - classification_loss: 0.5287\n",
            " 7/86 [=>............................] - ETA: 1:16 - loss: 2.0842 - regression_loss: 1.5569 - classification_loss: 0.5273\n",
            " 8/86 [=>............................] - ETA: 1:14 - loss: 2.0934 - regression_loss: 1.5661 - classification_loss: 0.5273\n",
            " 9/86 [==>...........................] - ETA: 1:13 - loss: 2.1109 - regression_loss: 1.5867 - classification_loss: 0.5241\n",
            "10/86 [==>...........................] - ETA: 1:10 - loss: 2.1237 - regression_loss: 1.5953 - classification_loss: 0.5284\n",
            "11/86 [==>...........................] - ETA: 1:10 - loss: 2.1232 - regression_loss: 1.6029 - classification_loss: 0.5203\n",
            "12/86 [===>..........................] - ETA: 1:09 - loss: 2.1603 - regression_loss: 1.6295 - classification_loss: 0.5308\n",
            "13/86 [===>..........................] - ETA: 1:07 - loss: 2.1582 - regression_loss: 1.6264 - classification_loss: 0.5318\n",
            "14/86 [===>..........................] - ETA: 1:05 - loss: 2.1469 - regression_loss: 1.6202 - classification_loss: 0.5267\n",
            "15/86 [====>.........................] - ETA: 1:04 - loss: 2.1317 - regression_loss: 1.6067 - classification_loss: 0.5250\n",
            "16/86 [====>.........................] - ETA: 1:04 - loss: 2.1376 - regression_loss: 1.6140 - classification_loss: 0.5236\n",
            "17/86 [====>.........................] - ETA: 1:03 - loss: 2.1194 - regression_loss: 1.5991 - classification_loss: 0.5202\n",
            "18/86 [=====>........................] - ETA: 1:02 - loss: 2.1003 - regression_loss: 1.5873 - classification_loss: 0.5130\n",
            "19/86 [=====>........................] - ETA: 1:02 - loss: 2.0822 - regression_loss: 1.5759 - classification_loss: 0.5064\n",
            "20/86 [=====>........................] - ETA: 1:00 - loss: 2.0718 - regression_loss: 1.5686 - classification_loss: 0.5031\n",
            "21/86 [======>.......................] - ETA: 1:00 - loss: 2.0701 - regression_loss: 1.5674 - classification_loss: 0.5027\n",
            "22/86 [======>.......................] - ETA: 59s - loss: 2.0803 - regression_loss: 1.5771 - classification_loss: 0.5032 \n",
            "23/86 [=======>......................] - ETA: 58s - loss: 2.0816 - regression_loss: 1.5780 - classification_loss: 0.5035\n",
            "24/86 [=======>......................] - ETA: 57s - loss: 2.0625 - regression_loss: 1.5641 - classification_loss: 0.4984\n",
            "25/86 [=======>......................] - ETA: 56s - loss: 2.0544 - regression_loss: 1.5558 - classification_loss: 0.4986\n",
            "26/86 [========>.....................] - ETA: 56s - loss: 2.0434 - regression_loss: 1.5474 - classification_loss: 0.4960\n",
            "27/86 [========>.....................] - ETA: 54s - loss: 2.0383 - regression_loss: 1.5416 - classification_loss: 0.4967\n",
            "28/86 [========>.....................] - ETA: 53s - loss: 2.0528 - regression_loss: 1.5514 - classification_loss: 0.5014\n",
            "29/86 [=========>....................] - ETA: 53s - loss: 2.0501 - regression_loss: 1.5491 - classification_loss: 0.5011\n",
            "30/86 [=========>....................] - ETA: 52s - loss: 2.0592 - regression_loss: 1.5561 - classification_loss: 0.5031\n",
            "31/86 [=========>....................] - ETA: 51s - loss: 2.0558 - regression_loss: 1.5539 - classification_loss: 0.5020\n",
            "32/86 [==========>...................] - ETA: 50s - loss: 2.0594 - regression_loss: 1.5576 - classification_loss: 0.5018\n",
            "33/86 [==========>...................] - ETA: 49s - loss: 2.0706 - regression_loss: 1.5658 - classification_loss: 0.5048\n",
            "34/86 [==========>...................] - ETA: 48s - loss: 2.0648 - regression_loss: 1.5624 - classification_loss: 0.5024\n",
            "35/86 [===========>..................] - ETA: 47s - loss: 2.0738 - regression_loss: 1.5689 - classification_loss: 0.5049\n",
            "36/86 [===========>..................] - ETA: 46s - loss: 2.0897 - regression_loss: 1.5818 - classification_loss: 0.5079\n",
            "37/86 [===========>..................] - ETA: 45s - loss: 2.0923 - regression_loss: 1.5836 - classification_loss: 0.5087\n",
            "38/86 [============>.................] - ETA: 44s - loss: 2.0945 - regression_loss: 1.5861 - classification_loss: 0.5083\n",
            "39/86 [============>.................] - ETA: 43s - loss: 2.0944 - regression_loss: 1.5861 - classification_loss: 0.5083\n",
            "40/86 [============>.................] - ETA: 42s - loss: 2.0873 - regression_loss: 1.5812 - classification_loss: 0.5061\n",
            "41/86 [=============>................] - ETA: 41s - loss: 2.0863 - regression_loss: 1.5805 - classification_loss: 0.5058\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.0856 - regression_loss: 1.5801 - classification_loss: 0.5055\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.0838 - regression_loss: 1.5784 - classification_loss: 0.5054\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.0866 - regression_loss: 1.5789 - classification_loss: 0.5077\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.0893 - regression_loss: 1.5815 - classification_loss: 0.5078\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.0929 - regression_loss: 1.5836 - classification_loss: 0.5093\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.0915 - regression_loss: 1.5824 - classification_loss: 0.5091\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.0893 - regression_loss: 1.5799 - classification_loss: 0.5094\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.0909 - regression_loss: 1.5806 - classification_loss: 0.5103\n",
            "50/86 [================>.............] - ETA: 33s - loss: 2.0935 - regression_loss: 1.5826 - classification_loss: 0.5109\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.0948 - regression_loss: 1.5838 - classification_loss: 0.5110\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.0955 - regression_loss: 1.5844 - classification_loss: 0.5112\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0967 - regression_loss: 1.5852 - classification_loss: 0.5115\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0973 - regression_loss: 1.5860 - classification_loss: 0.5113\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0990 - regression_loss: 1.5877 - classification_loss: 0.5113\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0932 - regression_loss: 1.5837 - classification_loss: 0.5095\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0937 - regression_loss: 1.5843 - classification_loss: 0.5095\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0938 - regression_loss: 1.5842 - classification_loss: 0.5096\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0977 - regression_loss: 1.5877 - classification_loss: 0.5100\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0964 - regression_loss: 1.5867 - classification_loss: 0.5097\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0974 - regression_loss: 1.5880 - classification_loss: 0.5094\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1006 - regression_loss: 1.5916 - classification_loss: 0.5090\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.0991 - regression_loss: 1.5905 - classification_loss: 0.5087\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.0990 - regression_loss: 1.5901 - classification_loss: 0.5090\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1014 - regression_loss: 1.5920 - classification_loss: 0.5093\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.1022 - regression_loss: 1.5936 - classification_loss: 0.5087\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.1008 - regression_loss: 1.5930 - classification_loss: 0.5078\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0982 - regression_loss: 1.5911 - classification_loss: 0.5072\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0976 - regression_loss: 1.5910 - classification_loss: 0.5066\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0997 - regression_loss: 1.5937 - classification_loss: 0.5061\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1018 - regression_loss: 1.5952 - classification_loss: 0.5066\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1018 - regression_loss: 1.5945 - classification_loss: 0.5073\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1016 - regression_loss: 1.5953 - classification_loss: 0.5063\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1013 - regression_loss: 1.5939 - classification_loss: 0.5074\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1037 - regression_loss: 1.5960 - classification_loss: 0.5077\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1062 - regression_loss: 1.5974 - classification_loss: 0.5089 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1049 - regression_loss: 1.5975 - classification_loss: 0.5074\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1018 - regression_loss: 1.5952 - classification_loss: 0.5067\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1013 - regression_loss: 1.5945 - classification_loss: 0.5068\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1021 - regression_loss: 1.5953 - classification_loss: 0.5068\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0988 - regression_loss: 1.5930 - classification_loss: 0.5058\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0996 - regression_loss: 1.5928 - classification_loss: 0.5069\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0934 - regression_loss: 1.5879 - classification_loss: 0.5054\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0922 - regression_loss: 1.5865 - classification_loss: 0.5058\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0921 - regression_loss: 1.5871 - classification_loss: 0.5050\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0928 - regression_loss: 1.5867 - classification_loss: 0.5061\n",
            "Epoch 65: saving model to ./snapshots\\resnet50_csv_65.h5\n",
            "\n",
            "86/86 [==============================] - 82s 948ms/step - loss: 2.0928 - regression_loss: 1.5867 - classification_loss: 0.5061 - lr: 1.0000e-29\n",
            "Epoch 66/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:55 - loss: 1.9520 - regression_loss: 1.4972 - classification_loss: 0.4548\n",
            " 2/86 [..............................] - ETA: 1:23 - loss: 2.0291 - regression_loss: 1.5379 - classification_loss: 0.4911\n",
            " 3/86 [>.............................] - ETA: 1:18 - loss: 1.9278 - regression_loss: 1.4616 - classification_loss: 0.4662\n",
            " 4/86 [>.............................] - ETA: 1:15 - loss: 1.9791 - regression_loss: 1.4984 - classification_loss: 0.4806\n",
            " 5/86 [>.............................] - ETA: 1:14 - loss: 2.0590 - regression_loss: 1.5424 - classification_loss: 0.5166\n",
            " 6/86 [=>............................] - ETA: 1:13 - loss: 2.0440 - regression_loss: 1.5506 - classification_loss: 0.4935\n",
            " 7/86 [=>............................] - ETA: 1:13 - loss: 2.0525 - regression_loss: 1.5636 - classification_loss: 0.4890\n",
            " 8/86 [=>............................] - ETA: 1:12 - loss: 2.0623 - regression_loss: 1.5664 - classification_loss: 0.4959\n",
            " 9/86 [==>...........................] - ETA: 1:09 - loss: 2.0943 - regression_loss: 1.5892 - classification_loss: 0.5051\n",
            "10/86 [==>...........................] - ETA: 1:09 - loss: 2.0892 - regression_loss: 1.5824 - classification_loss: 0.5068\n",
            "11/86 [==>...........................] - ETA: 1:08 - loss: 2.1113 - regression_loss: 1.5971 - classification_loss: 0.5142\n",
            "12/86 [===>..........................] - ETA: 1:08 - loss: 2.1107 - regression_loss: 1.6005 - classification_loss: 0.5102\n",
            "13/86 [===>..........................] - ETA: 1:08 - loss: 2.1187 - regression_loss: 1.5993 - classification_loss: 0.5194\n",
            "14/86 [===>..........................] - ETA: 1:07 - loss: 2.1284 - regression_loss: 1.6077 - classification_loss: 0.5207\n",
            "15/86 [====>.........................] - ETA: 1:06 - loss: 2.1060 - regression_loss: 1.5929 - classification_loss: 0.5131\n",
            "16/86 [====>.........................] - ETA: 1:06 - loss: 2.1037 - regression_loss: 1.5894 - classification_loss: 0.5143\n",
            "17/86 [====>.........................] - ETA: 1:04 - loss: 2.0917 - regression_loss: 1.5809 - classification_loss: 0.5108\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.0946 - regression_loss: 1.5839 - classification_loss: 0.5107\n",
            "19/86 [=====>........................] - ETA: 1:03 - loss: 2.0956 - regression_loss: 1.5848 - classification_loss: 0.5109\n",
            "20/86 [=====>........................] - ETA: 1:02 - loss: 2.0855 - regression_loss: 1.5767 - classification_loss: 0.5088\n",
            "21/86 [======>.......................] - ETA: 1:01 - loss: 2.0744 - regression_loss: 1.5686 - classification_loss: 0.5058\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.0856 - regression_loss: 1.5781 - classification_loss: 0.5075\n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.0905 - regression_loss: 1.5822 - classification_loss: 0.5083 \n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.0963 - regression_loss: 1.5897 - classification_loss: 0.5066\n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.0965 - regression_loss: 1.5907 - classification_loss: 0.5058\n",
            "26/86 [========>.....................] - ETA: 56s - loss: 2.0998 - regression_loss: 1.5932 - classification_loss: 0.5066\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.0952 - regression_loss: 1.5896 - classification_loss: 0.5055\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.0974 - regression_loss: 1.5923 - classification_loss: 0.5051\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.0974 - regression_loss: 1.5909 - classification_loss: 0.5065\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.0966 - regression_loss: 1.5889 - classification_loss: 0.5077\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.1014 - regression_loss: 1.5942 - classification_loss: 0.5071\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.1102 - regression_loss: 1.5999 - classification_loss: 0.5103\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.1077 - regression_loss: 1.5993 - classification_loss: 0.5085\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.1023 - regression_loss: 1.5946 - classification_loss: 0.5076\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.1059 - regression_loss: 1.5950 - classification_loss: 0.5109\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.1121 - regression_loss: 1.5991 - classification_loss: 0.5129\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.1109 - regression_loss: 1.5979 - classification_loss: 0.5130\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.1191 - regression_loss: 1.6053 - classification_loss: 0.5138\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.1190 - regression_loss: 1.6066 - classification_loss: 0.5124\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.1089 - regression_loss: 1.5992 - classification_loss: 0.5096\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.1088 - regression_loss: 1.5992 - classification_loss: 0.5096\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.1141 - regression_loss: 1.6025 - classification_loss: 0.5115\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.1209 - regression_loss: 1.6076 - classification_loss: 0.5133\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.1259 - regression_loss: 1.6121 - classification_loss: 0.5138\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.1257 - regression_loss: 1.6113 - classification_loss: 0.5144\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.1207 - regression_loss: 1.6077 - classification_loss: 0.5130\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.1250 - regression_loss: 1.6106 - classification_loss: 0.5144\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.1333 - regression_loss: 1.6160 - classification_loss: 0.5174\n",
            "49/86 [================>.............] - ETA: 34s - loss: 2.1333 - regression_loss: 1.6167 - classification_loss: 0.5167\n",
            "50/86 [================>.............] - ETA: 33s - loss: 2.1304 - regression_loss: 1.6141 - classification_loss: 0.5163\n",
            "51/86 [================>.............] - ETA: 32s - loss: 2.1330 - regression_loss: 1.6157 - classification_loss: 0.5173\n",
            "52/86 [=================>............] - ETA: 31s - loss: 2.1307 - regression_loss: 1.6140 - classification_loss: 0.5167\n",
            "53/86 [=================>............] - ETA: 30s - loss: 2.1309 - regression_loss: 1.6143 - classification_loss: 0.5166\n",
            "54/86 [=================>............] - ETA: 29s - loss: 2.1245 - regression_loss: 1.6102 - classification_loss: 0.5143\n",
            "55/86 [==================>...........] - ETA: 28s - loss: 2.1256 - regression_loss: 1.6115 - classification_loss: 0.5141\n",
            "56/86 [==================>...........] - ETA: 27s - loss: 2.1281 - regression_loss: 1.6135 - classification_loss: 0.5147\n",
            "57/86 [==================>...........] - ETA: 26s - loss: 2.1250 - regression_loss: 1.6107 - classification_loss: 0.5143\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1211 - regression_loss: 1.6076 - classification_loss: 0.5135\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1169 - regression_loss: 1.6046 - classification_loss: 0.5123\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1195 - regression_loss: 1.6064 - classification_loss: 0.5132\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1229 - regression_loss: 1.6085 - classification_loss: 0.5144\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1276 - regression_loss: 1.6118 - classification_loss: 0.5158\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1273 - regression_loss: 1.6114 - classification_loss: 0.5159\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1168 - regression_loss: 1.6034 - classification_loss: 0.5134\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1143 - regression_loss: 1.6023 - classification_loss: 0.5120\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.1105 - regression_loss: 1.5991 - classification_loss: 0.5114\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.1106 - regression_loss: 1.5996 - classification_loss: 0.5110\n",
            "68/86 [======================>.......] - ETA: 16s - loss: 2.1095 - regression_loss: 1.5987 - classification_loss: 0.5109\n",
            "69/86 [=======================>......] - ETA: 15s - loss: 2.1056 - regression_loss: 1.5950 - classification_loss: 0.5106\n",
            "70/86 [=======================>......] - ETA: 14s - loss: 2.1063 - regression_loss: 1.5959 - classification_loss: 0.5104\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1062 - regression_loss: 1.5948 - classification_loss: 0.5115\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1072 - regression_loss: 1.5950 - classification_loss: 0.5122\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1145 - regression_loss: 1.6007 - classification_loss: 0.5137\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1158 - regression_loss: 1.6024 - classification_loss: 0.5135\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1147 - regression_loss: 1.6021 - classification_loss: 0.5126\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1131 - regression_loss: 1.6014 - classification_loss: 0.5117 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1116 - regression_loss: 1.6005 - classification_loss: 0.5110\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1140 - regression_loss: 1.6027 - classification_loss: 0.5114\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1133 - regression_loss: 1.6016 - classification_loss: 0.5116\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1118 - regression_loss: 1.6003 - classification_loss: 0.5115\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1148 - regression_loss: 1.6029 - classification_loss: 0.5119\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1140 - regression_loss: 1.6020 - classification_loss: 0.5119\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1139 - regression_loss: 1.6020 - classification_loss: 0.5120\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1132 - regression_loss: 1.6009 - classification_loss: 0.5123\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1116 - regression_loss: 1.5992 - classification_loss: 0.5125\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1100 - regression_loss: 1.5980 - classification_loss: 0.5120\n",
            "Epoch 66: saving model to ./snapshots\\resnet50_csv_66.h5\n",
            "\n",
            "Epoch 66: ReduceLROnPlateau reducing learning rate to 1.0000000031710769e-30.\n",
            "\n",
            "86/86 [==============================] - 82s 949ms/step - loss: 2.1100 - regression_loss: 1.5980 - classification_loss: 0.5120 - lr: 1.0000e-29\n",
            "Epoch 67/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:49 - loss: 2.1426 - regression_loss: 1.6241 - classification_loss: 0.5185\n",
            " 2/86 [..............................] - ETA: 1:40 - loss: 2.1402 - regression_loss: 1.6154 - classification_loss: 0.5248\n",
            " 3/86 [>.............................] - ETA: 1:22 - loss: 2.0800 - regression_loss: 1.5829 - classification_loss: 0.4970\n",
            " 4/86 [>.............................] - ETA: 1:18 - loss: 2.1772 - regression_loss: 1.6478 - classification_loss: 0.5294\n",
            " 5/86 [>.............................] - ETA: 1:15 - loss: 2.2200 - regression_loss: 1.6758 - classification_loss: 0.5442\n",
            " 6/86 [=>............................] - ETA: 1:15 - loss: 2.1531 - regression_loss: 1.6284 - classification_loss: 0.5247\n",
            " 7/86 [=>............................] - ETA: 1:13 - loss: 2.1190 - regression_loss: 1.6043 - classification_loss: 0.5147\n",
            " 8/86 [=>............................] - ETA: 1:12 - loss: 2.1294 - regression_loss: 1.6136 - classification_loss: 0.5157\n",
            " 9/86 [==>...........................] - ETA: 1:11 - loss: 2.1330 - regression_loss: 1.6156 - classification_loss: 0.5174\n",
            "10/86 [==>...........................] - ETA: 1:10 - loss: 2.1076 - regression_loss: 1.5914 - classification_loss: 0.5162\n",
            "11/86 [==>...........................] - ETA: 1:12 - loss: 2.1054 - regression_loss: 1.5902 - classification_loss: 0.5153\n",
            "12/86 [===>..........................] - ETA: 1:10 - loss: 2.0914 - regression_loss: 1.5818 - classification_loss: 0.5097\n",
            "13/86 [===>..........................] - ETA: 1:09 - loss: 2.1039 - regression_loss: 1.5960 - classification_loss: 0.5079\n",
            "14/86 [===>..........................] - ETA: 1:07 - loss: 2.1101 - regression_loss: 1.6027 - classification_loss: 0.5074\n",
            "15/86 [====>.........................] - ETA: 1:06 - loss: 2.1017 - regression_loss: 1.5958 - classification_loss: 0.5060\n",
            "16/86 [====>.........................] - ETA: 1:04 - loss: 2.1213 - regression_loss: 1.6114 - classification_loss: 0.5099\n",
            "17/86 [====>.........................] - ETA: 1:03 - loss: 2.0881 - regression_loss: 1.5871 - classification_loss: 0.5010\n",
            "18/86 [=====>........................] - ETA: 1:02 - loss: 2.0972 - regression_loss: 1.5911 - classification_loss: 0.5061\n",
            "19/86 [=====>........................] - ETA: 1:01 - loss: 2.0961 - regression_loss: 1.5904 - classification_loss: 0.5057\n",
            "20/86 [=====>........................] - ETA: 1:01 - loss: 2.0848 - regression_loss: 1.5828 - classification_loss: 0.5021\n",
            "21/86 [======>.......................] - ETA: 1:00 - loss: 2.0996 - regression_loss: 1.5938 - classification_loss: 0.5058\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.1126 - regression_loss: 1.6065 - classification_loss: 0.5061\n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.1120 - regression_loss: 1.6058 - classification_loss: 0.5062 \n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.1072 - regression_loss: 1.6019 - classification_loss: 0.5052\n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.1153 - regression_loss: 1.6085 - classification_loss: 0.5068\n",
            "26/86 [========>.....................] - ETA: 56s - loss: 2.1035 - regression_loss: 1.5996 - classification_loss: 0.5039\n",
            "27/86 [========>.....................] - ETA: 55s - loss: 2.1032 - regression_loss: 1.6014 - classification_loss: 0.5018\n",
            "28/86 [========>.....................] - ETA: 54s - loss: 2.1037 - regression_loss: 1.6025 - classification_loss: 0.5012\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.1001 - regression_loss: 1.6009 - classification_loss: 0.4992\n",
            "30/86 [=========>....................] - ETA: 54s - loss: 2.0999 - regression_loss: 1.6015 - classification_loss: 0.4984\n",
            "31/86 [=========>....................] - ETA: 53s - loss: 2.0978 - regression_loss: 1.5989 - classification_loss: 0.4989\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.1045 - regression_loss: 1.6051 - classification_loss: 0.4994\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.1064 - regression_loss: 1.6058 - classification_loss: 0.5007\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.1024 - regression_loss: 1.6025 - classification_loss: 0.4999\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.1081 - regression_loss: 1.6082 - classification_loss: 0.4999\n",
            "36/86 [===========>..................] - ETA: 48s - loss: 2.1155 - regression_loss: 1.6130 - classification_loss: 0.5025\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.1165 - regression_loss: 1.6134 - classification_loss: 0.5031\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.1148 - regression_loss: 1.6114 - classification_loss: 0.5034\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.1188 - regression_loss: 1.6147 - classification_loss: 0.5041\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.1261 - regression_loss: 1.6201 - classification_loss: 0.5060\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.1282 - regression_loss: 1.6200 - classification_loss: 0.5082\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1198 - regression_loss: 1.6137 - classification_loss: 0.5061\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1282 - regression_loss: 1.6205 - classification_loss: 0.5077\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1308 - regression_loss: 1.6220 - classification_loss: 0.5087\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.1336 - regression_loss: 1.6234 - classification_loss: 0.5102\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1382 - regression_loss: 1.6272 - classification_loss: 0.5110\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1385 - regression_loss: 1.6271 - classification_loss: 0.5114\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1361 - regression_loss: 1.6252 - classification_loss: 0.5109\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1389 - regression_loss: 1.6277 - classification_loss: 0.5112\n",
            "50/86 [================>.............] - ETA: 35s - loss: 2.1372 - regression_loss: 1.6254 - classification_loss: 0.5118\n",
            "51/86 [================>.............] - ETA: 34s - loss: 2.1382 - regression_loss: 1.6267 - classification_loss: 0.5115\n",
            "52/86 [=================>............] - ETA: 33s - loss: 2.1421 - regression_loss: 1.6297 - classification_loss: 0.5124\n",
            "53/86 [=================>............] - ETA: 32s - loss: 2.1436 - regression_loss: 1.6306 - classification_loss: 0.5130\n",
            "54/86 [=================>............] - ETA: 31s - loss: 2.1456 - regression_loss: 1.6325 - classification_loss: 0.5132\n",
            "55/86 [==================>...........] - ETA: 30s - loss: 2.1419 - regression_loss: 1.6293 - classification_loss: 0.5127\n",
            "56/86 [==================>...........] - ETA: 29s - loss: 2.1463 - regression_loss: 1.6323 - classification_loss: 0.5140\n",
            "57/86 [==================>...........] - ETA: 28s - loss: 2.1435 - regression_loss: 1.6300 - classification_loss: 0.5136\n",
            "58/86 [===================>..........] - ETA: 27s - loss: 2.1420 - regression_loss: 1.6290 - classification_loss: 0.5129\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.1463 - regression_loss: 1.6308 - classification_loss: 0.5155\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.1439 - regression_loss: 1.6287 - classification_loss: 0.5152\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.1475 - regression_loss: 1.6313 - classification_loss: 0.5162\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.1431 - regression_loss: 1.6272 - classification_loss: 0.5158\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.1399 - regression_loss: 1.6242 - classification_loss: 0.5157\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.1315 - regression_loss: 1.6179 - classification_loss: 0.5136\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1320 - regression_loss: 1.6178 - classification_loss: 0.5142\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1350 - regression_loss: 1.6200 - classification_loss: 0.5150\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1355 - regression_loss: 1.6203 - classification_loss: 0.5152\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1419 - regression_loss: 1.6257 - classification_loss: 0.5162\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1383 - regression_loss: 1.6233 - classification_loss: 0.5150\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1289 - regression_loss: 1.6164 - classification_loss: 0.5126\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1279 - regression_loss: 1.6151 - classification_loss: 0.5128\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1262 - regression_loss: 1.6136 - classification_loss: 0.5125\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1211 - regression_loss: 1.6093 - classification_loss: 0.5118\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1192 - regression_loss: 1.6082 - classification_loss: 0.5110\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1152 - regression_loss: 1.6049 - classification_loss: 0.5103\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1132 - regression_loss: 1.6045 - classification_loss: 0.5087 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1154 - regression_loss: 1.6061 - classification_loss: 0.5093\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1151 - regression_loss: 1.6059 - classification_loss: 0.5091\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1159 - regression_loss: 1.6068 - classification_loss: 0.5091\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1135 - regression_loss: 1.6051 - classification_loss: 0.5084\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1129 - regression_loss: 1.6041 - classification_loss: 0.5088\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1140 - regression_loss: 1.6049 - classification_loss: 0.5091\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1170 - regression_loss: 1.6074 - classification_loss: 0.5095\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1192 - regression_loss: 1.6094 - classification_loss: 0.5098\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1217 - regression_loss: 1.6112 - classification_loss: 0.5105\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1178 - regression_loss: 1.6081 - classification_loss: 0.5097\n",
            "Epoch 67: saving model to ./snapshots\\resnet50_csv_67.h5\n",
            "\n",
            "86/86 [==============================] - 83s 959ms/step - loss: 2.1178 - regression_loss: 1.6081 - classification_loss: 0.5097 - lr: 1.0000e-30\n",
            "Epoch 68/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:53 - loss: 2.4036 - regression_loss: 1.8270 - classification_loss: 0.5766\n",
            " 2/86 [..............................] - ETA: 1:29 - loss: 2.3577 - regression_loss: 1.8111 - classification_loss: 0.5466\n",
            " 3/86 [>.............................] - ETA: 1:24 - loss: 2.3800 - regression_loss: 1.8089 - classification_loss: 0.5711\n",
            " 4/86 [>.............................] - ETA: 1:21 - loss: 2.2218 - regression_loss: 1.6788 - classification_loss: 0.5430\n",
            " 5/86 [>.............................] - ETA: 1:20 - loss: 2.1825 - regression_loss: 1.6602 - classification_loss: 0.5223\n",
            " 6/86 [=>............................] - ETA: 1:15 - loss: 2.1609 - regression_loss: 1.6326 - classification_loss: 0.5283\n",
            " 7/86 [=>............................] - ETA: 1:13 - loss: 2.1778 - regression_loss: 1.6441 - classification_loss: 0.5337\n",
            " 8/86 [=>............................] - ETA: 1:15 - loss: 2.1741 - regression_loss: 1.6406 - classification_loss: 0.5335\n",
            " 9/86 [==>...........................] - ETA: 1:14 - loss: 2.1454 - regression_loss: 1.6182 - classification_loss: 0.5272\n",
            "10/86 [==>...........................] - ETA: 1:13 - loss: 2.1478 - regression_loss: 1.6204 - classification_loss: 0.5274\n",
            "11/86 [==>...........................] - ETA: 1:15 - loss: 2.1344 - regression_loss: 1.6108 - classification_loss: 0.5236\n",
            "12/86 [===>..........................] - ETA: 1:12 - loss: 2.1347 - regression_loss: 1.6072 - classification_loss: 0.5275\n",
            "13/86 [===>..........................] - ETA: 1:14 - loss: 2.1192 - regression_loss: 1.5939 - classification_loss: 0.5252\n",
            "14/86 [===>..........................] - ETA: 1:14 - loss: 2.1203 - regression_loss: 1.5972 - classification_loss: 0.5231\n",
            "15/86 [====>.........................] - ETA: 1:11 - loss: 2.0949 - regression_loss: 1.5806 - classification_loss: 0.5143\n",
            "16/86 [====>.........................] - ETA: 1:10 - loss: 2.0890 - regression_loss: 1.5822 - classification_loss: 0.5068\n",
            "17/86 [====>.........................] - ETA: 1:09 - loss: 2.0588 - regression_loss: 1.5591 - classification_loss: 0.4996\n",
            "18/86 [=====>........................] - ETA: 1:08 - loss: 2.0665 - regression_loss: 1.5631 - classification_loss: 0.5034\n",
            "19/86 [=====>........................] - ETA: 1:06 - loss: 2.0651 - regression_loss: 1.5624 - classification_loss: 0.5027\n",
            "20/86 [=====>........................] - ETA: 1:05 - loss: 2.0714 - regression_loss: 1.5673 - classification_loss: 0.5040\n",
            "21/86 [======>.......................] - ETA: 1:04 - loss: 2.0832 - regression_loss: 1.5785 - classification_loss: 0.5047\n",
            "22/86 [======>.......................] - ETA: 1:02 - loss: 2.0752 - regression_loss: 1.5725 - classification_loss: 0.5028\n",
            "23/86 [=======>......................] - ETA: 1:01 - loss: 2.0717 - regression_loss: 1.5698 - classification_loss: 0.5019\n",
            "24/86 [=======>......................] - ETA: 1:00 - loss: 2.0725 - regression_loss: 1.5713 - classification_loss: 0.5012\n",
            "25/86 [=======>......................] - ETA: 59s - loss: 2.0733 - regression_loss: 1.5703 - classification_loss: 0.5030 \n",
            "26/86 [========>.....................] - ETA: 58s - loss: 2.0892 - regression_loss: 1.5830 - classification_loss: 0.5062\n",
            "27/86 [========>.....................] - ETA: 57s - loss: 2.0950 - regression_loss: 1.5859 - classification_loss: 0.5091\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.1012 - regression_loss: 1.5904 - classification_loss: 0.5108\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.1015 - regression_loss: 1.5898 - classification_loss: 0.5117\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.0978 - regression_loss: 1.5859 - classification_loss: 0.5119\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.0918 - regression_loss: 1.5800 - classification_loss: 0.5118\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.0952 - regression_loss: 1.5844 - classification_loss: 0.5107\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.0951 - regression_loss: 1.5848 - classification_loss: 0.5102\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.0878 - regression_loss: 1.5792 - classification_loss: 0.5086\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.0901 - regression_loss: 1.5809 - classification_loss: 0.5092\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.0882 - regression_loss: 1.5807 - classification_loss: 0.5075\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.0945 - regression_loss: 1.5861 - classification_loss: 0.5084\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.0975 - regression_loss: 1.5893 - classification_loss: 0.5082\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.1025 - regression_loss: 1.5921 - classification_loss: 0.5104\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.1000 - regression_loss: 1.5899 - classification_loss: 0.5101\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.1063 - regression_loss: 1.5945 - classification_loss: 0.5118\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.1033 - regression_loss: 1.5925 - classification_loss: 0.5107\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.1039 - regression_loss: 1.5939 - classification_loss: 0.5100\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.1006 - regression_loss: 1.5920 - classification_loss: 0.5085\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.1066 - regression_loss: 1.5960 - classification_loss: 0.5106\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.1089 - regression_loss: 1.5975 - classification_loss: 0.5114\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.1068 - regression_loss: 1.5966 - classification_loss: 0.5102\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1052 - regression_loss: 1.5955 - classification_loss: 0.5096\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1004 - regression_loss: 1.5910 - classification_loss: 0.5093\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1026 - regression_loss: 1.5906 - classification_loss: 0.5120\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1031 - regression_loss: 1.5906 - classification_loss: 0.5125\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1006 - regression_loss: 1.5895 - classification_loss: 0.5111\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0980 - regression_loss: 1.5884 - classification_loss: 0.5096\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1018 - regression_loss: 1.5917 - classification_loss: 0.5100\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0929 - regression_loss: 1.5854 - classification_loss: 0.5075\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0929 - regression_loss: 1.5849 - classification_loss: 0.5080\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0931 - regression_loss: 1.5852 - classification_loss: 0.5079\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0897 - regression_loss: 1.5826 - classification_loss: 0.5071\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0880 - regression_loss: 1.5808 - classification_loss: 0.5073\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0853 - regression_loss: 1.5790 - classification_loss: 0.5063\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0884 - regression_loss: 1.5825 - classification_loss: 0.5059\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.0900 - regression_loss: 1.5842 - classification_loss: 0.5059\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.0947 - regression_loss: 1.5876 - classification_loss: 0.5071\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.0921 - regression_loss: 1.5849 - classification_loss: 0.5072\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.0901 - regression_loss: 1.5830 - classification_loss: 0.5071\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.0867 - regression_loss: 1.5799 - classification_loss: 0.5068\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0805 - regression_loss: 1.5753 - classification_loss: 0.5052\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0788 - regression_loss: 1.5740 - classification_loss: 0.5048\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0786 - regression_loss: 1.5742 - classification_loss: 0.5044\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0766 - regression_loss: 1.5719 - classification_loss: 0.5048\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0758 - regression_loss: 1.5719 - classification_loss: 0.5039\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0779 - regression_loss: 1.5738 - classification_loss: 0.5041\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0766 - regression_loss: 1.5728 - classification_loss: 0.5037\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0748 - regression_loss: 1.5713 - classification_loss: 0.5034\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0765 - regression_loss: 1.5722 - classification_loss: 0.5043\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0771 - regression_loss: 1.5731 - classification_loss: 0.5039 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0764 - regression_loss: 1.5726 - classification_loss: 0.5038\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0788 - regression_loss: 1.5751 - classification_loss: 0.5037\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0798 - regression_loss: 1.5758 - classification_loss: 0.5040\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0794 - regression_loss: 1.5754 - classification_loss: 0.5040\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0799 - regression_loss: 1.5769 - classification_loss: 0.5031\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0803 - regression_loss: 1.5747 - classification_loss: 0.5057\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0773 - regression_loss: 1.5729 - classification_loss: 0.5044\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0793 - regression_loss: 1.5740 - classification_loss: 0.5053\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0763 - regression_loss: 1.5717 - classification_loss: 0.5046\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0730 - regression_loss: 1.5688 - classification_loss: 0.5042\n",
            "Epoch 68: saving model to ./snapshots\\resnet50_csv_68.h5\n",
            "\n",
            "Epoch 68: ReduceLROnPlateau reducing learning rate to 1.000000003171077e-31.\n",
            "\n",
            "86/86 [==============================] - 82s 954ms/step - loss: 2.0730 - regression_loss: 1.5688 - classification_loss: 0.5042 - lr: 1.0000e-30\n",
            "Epoch 69/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:41 - loss: 2.0244 - regression_loss: 1.5654 - classification_loss: 0.4591\n",
            " 2/86 [..............................] - ETA: 1:17 - loss: 2.0860 - regression_loss: 1.5973 - classification_loss: 0.4887\n",
            " 3/86 [>.............................] - ETA: 1:20 - loss: 2.1760 - regression_loss: 1.6479 - classification_loss: 0.5281\n",
            " 4/86 [>.............................] - ETA: 1:30 - loss: 2.1440 - regression_loss: 1.6183 - classification_loss: 0.5257\n",
            " 5/86 [>.............................] - ETA: 1:24 - loss: 2.2358 - regression_loss: 1.6893 - classification_loss: 0.5465\n",
            " 6/86 [=>............................] - ETA: 1:22 - loss: 2.2009 - regression_loss: 1.6577 - classification_loss: 0.5432\n",
            " 7/86 [=>............................] - ETA: 1:19 - loss: 2.1846 - regression_loss: 1.6445 - classification_loss: 0.5400\n",
            " 8/86 [=>............................] - ETA: 1:18 - loss: 2.1641 - regression_loss: 1.6336 - classification_loss: 0.5304\n",
            " 9/86 [==>...........................] - ETA: 1:17 - loss: 2.1346 - regression_loss: 1.6145 - classification_loss: 0.5201\n",
            "10/86 [==>...........................] - ETA: 1:14 - loss: 2.1505 - regression_loss: 1.6171 - classification_loss: 0.5334\n",
            "11/86 [==>...........................] - ETA: 1:12 - loss: 2.1260 - regression_loss: 1.5943 - classification_loss: 0.5317\n",
            "12/86 [===>..........................] - ETA: 1:13 - loss: 2.1165 - regression_loss: 1.5848 - classification_loss: 0.5317\n",
            "13/86 [===>..........................] - ETA: 1:10 - loss: 2.1007 - regression_loss: 1.5744 - classification_loss: 0.5264\n",
            "14/86 [===>..........................] - ETA: 1:09 - loss: 2.1169 - regression_loss: 1.5898 - classification_loss: 0.5272\n",
            "15/86 [====>.........................] - ETA: 1:07 - loss: 2.1166 - regression_loss: 1.5904 - classification_loss: 0.5262\n",
            "16/86 [====>.........................] - ETA: 1:07 - loss: 2.1321 - regression_loss: 1.6000 - classification_loss: 0.5321\n",
            "17/86 [====>.........................] - ETA: 1:06 - loss: 2.1152 - regression_loss: 1.5879 - classification_loss: 0.5273\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.1208 - regression_loss: 1.5894 - classification_loss: 0.5314\n",
            "19/86 [=====>........................] - ETA: 1:03 - loss: 2.1187 - regression_loss: 1.5878 - classification_loss: 0.5309\n",
            "20/86 [=====>........................] - ETA: 1:04 - loss: 2.1126 - regression_loss: 1.5848 - classification_loss: 0.5279\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.1117 - regression_loss: 1.5840 - classification_loss: 0.5277\n",
            "22/86 [======>.......................] - ETA: 1:02 - loss: 2.1127 - regression_loss: 1.5858 - classification_loss: 0.5269\n",
            "23/86 [=======>......................] - ETA: 1:02 - loss: 2.1066 - regression_loss: 1.5808 - classification_loss: 0.5258\n",
            "24/86 [=======>......................] - ETA: 1:01 - loss: 2.1113 - regression_loss: 1.5826 - classification_loss: 0.5287\n",
            "25/86 [=======>......................] - ETA: 59s - loss: 2.1109 - regression_loss: 1.5832 - classification_loss: 0.5277 \n",
            "26/86 [========>.....................] - ETA: 58s - loss: 2.1093 - regression_loss: 1.5831 - classification_loss: 0.5261\n",
            "27/86 [========>.....................] - ETA: 57s - loss: 2.1199 - regression_loss: 1.5923 - classification_loss: 0.5275\n",
            "28/86 [========>.....................] - ETA: 57s - loss: 2.1162 - regression_loss: 1.5883 - classification_loss: 0.5279\n",
            "29/86 [=========>....................] - ETA: 56s - loss: 2.1132 - regression_loss: 1.5843 - classification_loss: 0.5290\n",
            "30/86 [=========>....................] - ETA: 55s - loss: 2.1109 - regression_loss: 1.5807 - classification_loss: 0.5302\n",
            "31/86 [=========>....................] - ETA: 54s - loss: 2.1028 - regression_loss: 1.5747 - classification_loss: 0.5281\n",
            "32/86 [==========>...................] - ETA: 53s - loss: 2.1036 - regression_loss: 1.5772 - classification_loss: 0.5264\n",
            "33/86 [==========>...................] - ETA: 53s - loss: 2.1091 - regression_loss: 1.5816 - classification_loss: 0.5275\n",
            "34/86 [==========>...................] - ETA: 52s - loss: 2.1043 - regression_loss: 1.5782 - classification_loss: 0.5260\n",
            "35/86 [===========>..................] - ETA: 51s - loss: 2.1100 - regression_loss: 1.5836 - classification_loss: 0.5263\n",
            "36/86 [===========>..................] - ETA: 49s - loss: 2.1107 - regression_loss: 1.5853 - classification_loss: 0.5254\n",
            "37/86 [===========>..................] - ETA: 48s - loss: 2.1117 - regression_loss: 1.5873 - classification_loss: 0.5244\n",
            "38/86 [============>.................] - ETA: 47s - loss: 2.1055 - regression_loss: 1.5827 - classification_loss: 0.5228\n",
            "39/86 [============>.................] - ETA: 46s - loss: 2.1063 - regression_loss: 1.5834 - classification_loss: 0.5230\n",
            "40/86 [============>.................] - ETA: 45s - loss: 2.1062 - regression_loss: 1.5784 - classification_loss: 0.5278\n",
            "41/86 [=============>................] - ETA: 44s - loss: 2.1101 - regression_loss: 1.5831 - classification_loss: 0.5270\n",
            "42/86 [=============>................] - ETA: 43s - loss: 2.1053 - regression_loss: 1.5795 - classification_loss: 0.5258\n",
            "43/86 [==============>...............] - ETA: 42s - loss: 2.0996 - regression_loss: 1.5755 - classification_loss: 0.5241\n",
            "44/86 [==============>...............] - ETA: 41s - loss: 2.0989 - regression_loss: 1.5753 - classification_loss: 0.5236\n",
            "45/86 [==============>...............] - ETA: 40s - loss: 2.1038 - regression_loss: 1.5817 - classification_loss: 0.5222\n",
            "46/86 [===============>..............] - ETA: 39s - loss: 2.0951 - regression_loss: 1.5756 - classification_loss: 0.5195\n",
            "47/86 [===============>..............] - ETA: 38s - loss: 2.0999 - regression_loss: 1.5787 - classification_loss: 0.5211\n",
            "48/86 [===============>..............] - ETA: 37s - loss: 2.0981 - regression_loss: 1.5788 - classification_loss: 0.5193\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.0918 - regression_loss: 1.5737 - classification_loss: 0.5182\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.0960 - regression_loss: 1.5777 - classification_loss: 0.5184\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1020 - regression_loss: 1.5825 - classification_loss: 0.5195\n",
            "52/86 [=================>............] - ETA: 33s - loss: 2.1021 - regression_loss: 1.5824 - classification_loss: 0.5197\n",
            "53/86 [=================>............] - ETA: 32s - loss: 2.0959 - regression_loss: 1.5785 - classification_loss: 0.5174\n",
            "54/86 [=================>............] - ETA: 31s - loss: 2.0972 - regression_loss: 1.5797 - classification_loss: 0.5175\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0867 - regression_loss: 1.5722 - classification_loss: 0.5145\n",
            "56/86 [==================>...........] - ETA: 29s - loss: 2.0873 - regression_loss: 1.5722 - classification_loss: 0.5150\n",
            "57/86 [==================>...........] - ETA: 28s - loss: 2.0881 - regression_loss: 1.5725 - classification_loss: 0.5156\n",
            "58/86 [===================>..........] - ETA: 27s - loss: 2.0883 - regression_loss: 1.5727 - classification_loss: 0.5155\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.0876 - regression_loss: 1.5735 - classification_loss: 0.5141\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.0856 - regression_loss: 1.5721 - classification_loss: 0.5135\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.0906 - regression_loss: 1.5748 - classification_loss: 0.5159\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.0934 - regression_loss: 1.5765 - classification_loss: 0.5169\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.0922 - regression_loss: 1.5754 - classification_loss: 0.5167\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.0901 - regression_loss: 1.5741 - classification_loss: 0.5160\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.0877 - regression_loss: 1.5727 - classification_loss: 0.5150\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0876 - regression_loss: 1.5731 - classification_loss: 0.5145\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0865 - regression_loss: 1.5720 - classification_loss: 0.5145\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0853 - regression_loss: 1.5717 - classification_loss: 0.5136\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0848 - regression_loss: 1.5716 - classification_loss: 0.5131\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0841 - regression_loss: 1.5720 - classification_loss: 0.5121\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0848 - regression_loss: 1.5723 - classification_loss: 0.5125\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0878 - regression_loss: 1.5746 - classification_loss: 0.5132\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0921 - regression_loss: 1.5778 - classification_loss: 0.5143\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0907 - regression_loss: 1.5763 - classification_loss: 0.5144\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0907 - regression_loss: 1.5760 - classification_loss: 0.5147\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0880 - regression_loss: 1.5739 - classification_loss: 0.5141 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0849 - regression_loss: 1.5708 - classification_loss: 0.5141\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0868 - regression_loss: 1.5728 - classification_loss: 0.5140\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0896 - regression_loss: 1.5753 - classification_loss: 0.5143\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0862 - regression_loss: 1.5729 - classification_loss: 0.5132\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0884 - regression_loss: 1.5746 - classification_loss: 0.5139\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0875 - regression_loss: 1.5737 - classification_loss: 0.5138\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0842 - regression_loss: 1.5714 - classification_loss: 0.5127\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0858 - regression_loss: 1.5726 - classification_loss: 0.5132\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0897 - regression_loss: 1.5760 - classification_loss: 0.5137\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0951 - regression_loss: 1.5803 - classification_loss: 0.5148\n",
            "Epoch 69: saving model to ./snapshots\\resnet50_csv_69.h5\n",
            "\n",
            "86/86 [==============================] - 83s 964ms/step - loss: 2.0951 - regression_loss: 1.5803 - classification_loss: 0.5148 - lr: 1.0000e-31\n",
            "Epoch 70/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:56 - loss: 1.9062 - regression_loss: 1.4140 - classification_loss: 0.4922\n",
            " 2/86 [..............................] - ETA: 1:10 - loss: 2.2153 - regression_loss: 1.6533 - classification_loss: 0.5620\n",
            " 3/86 [>.............................] - ETA: 1:10 - loss: 2.1553 - regression_loss: 1.6240 - classification_loss: 0.5314\n",
            " 4/86 [>.............................] - ETA: 1:19 - loss: 2.1360 - regression_loss: 1.5997 - classification_loss: 0.5363\n",
            " 5/86 [>.............................] - ETA: 1:19 - loss: 2.1576 - regression_loss: 1.6076 - classification_loss: 0.5500\n",
            " 6/86 [=>............................] - ETA: 1:20 - loss: 2.1841 - regression_loss: 1.6233 - classification_loss: 0.5608\n",
            " 7/86 [=>............................] - ETA: 1:17 - loss: 2.1329 - regression_loss: 1.5914 - classification_loss: 0.5416\n",
            " 8/86 [=>............................] - ETA: 1:16 - loss: 2.1364 - regression_loss: 1.5994 - classification_loss: 0.5370\n",
            " 9/86 [==>...........................] - ETA: 1:15 - loss: 2.1621 - regression_loss: 1.6221 - classification_loss: 0.5400\n",
            "10/86 [==>...........................] - ETA: 1:14 - loss: 2.1498 - regression_loss: 1.6120 - classification_loss: 0.5378\n",
            "11/86 [==>...........................] - ETA: 1:16 - loss: 2.1493 - regression_loss: 1.6106 - classification_loss: 0.5387\n",
            "12/86 [===>..........................] - ETA: 1:14 - loss: 2.1481 - regression_loss: 1.6161 - classification_loss: 0.5321\n",
            "13/86 [===>..........................] - ETA: 1:15 - loss: 2.1477 - regression_loss: 1.6181 - classification_loss: 0.5296\n",
            "14/86 [===>..........................] - ETA: 1:15 - loss: 2.1419 - regression_loss: 1.6184 - classification_loss: 0.5235\n",
            "15/86 [====>.........................] - ETA: 1:13 - loss: 2.1432 - regression_loss: 1.6178 - classification_loss: 0.5254\n",
            "16/86 [====>.........................] - ETA: 1:12 - loss: 2.1469 - regression_loss: 1.6203 - classification_loss: 0.5266\n",
            "17/86 [====>.........................] - ETA: 1:10 - loss: 2.1536 - regression_loss: 1.6264 - classification_loss: 0.5272\n",
            "18/86 [=====>........................] - ETA: 1:08 - loss: 2.1334 - regression_loss: 1.6112 - classification_loss: 0.5222\n",
            "19/86 [=====>........................] - ETA: 1:07 - loss: 2.1251 - regression_loss: 1.6055 - classification_loss: 0.5195\n",
            "20/86 [=====>........................] - ETA: 1:05 - loss: 2.1244 - regression_loss: 1.6031 - classification_loss: 0.5213\n",
            "21/86 [======>.......................] - ETA: 1:03 - loss: 2.0954 - regression_loss: 1.5820 - classification_loss: 0.5134\n",
            "22/86 [======>.......................] - ETA: 1:02 - loss: 2.0882 - regression_loss: 1.5778 - classification_loss: 0.5104\n",
            "23/86 [=======>......................] - ETA: 1:01 - loss: 2.0995 - regression_loss: 1.5874 - classification_loss: 0.5121\n",
            "24/86 [=======>......................] - ETA: 1:00 - loss: 2.1008 - regression_loss: 1.5866 - classification_loss: 0.5142\n",
            "25/86 [=======>......................] - ETA: 59s - loss: 2.0983 - regression_loss: 1.5829 - classification_loss: 0.5154 \n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.0943 - regression_loss: 1.5817 - classification_loss: 0.5126\n",
            "27/86 [========>.....................] - ETA: 57s - loss: 2.1013 - regression_loss: 1.5870 - classification_loss: 0.5144\n",
            "28/86 [========>.....................] - ETA: 56s - loss: 2.1042 - regression_loss: 1.5897 - classification_loss: 0.5145\n",
            "29/86 [=========>....................] - ETA: 55s - loss: 2.1039 - regression_loss: 1.5878 - classification_loss: 0.5161\n",
            "30/86 [=========>....................] - ETA: 54s - loss: 2.1023 - regression_loss: 1.5856 - classification_loss: 0.5167\n",
            "31/86 [=========>....................] - ETA: 53s - loss: 2.1045 - regression_loss: 1.5863 - classification_loss: 0.5181\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.0979 - regression_loss: 1.5800 - classification_loss: 0.5178\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.0952 - regression_loss: 1.5797 - classification_loss: 0.5155\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.1015 - regression_loss: 1.5842 - classification_loss: 0.5173\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.1050 - regression_loss: 1.5855 - classification_loss: 0.5195\n",
            "36/86 [===========>..................] - ETA: 48s - loss: 2.1073 - regression_loss: 1.5867 - classification_loss: 0.5206\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.1008 - regression_loss: 1.5825 - classification_loss: 0.5182\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.0973 - regression_loss: 1.5794 - classification_loss: 0.5179\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.0971 - regression_loss: 1.5798 - classification_loss: 0.5173\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.0948 - regression_loss: 1.5780 - classification_loss: 0.5168\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.0918 - regression_loss: 1.5756 - classification_loss: 0.5162\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.0964 - regression_loss: 1.5791 - classification_loss: 0.5173\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.0994 - regression_loss: 1.5818 - classification_loss: 0.5176\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.0974 - regression_loss: 1.5813 - classification_loss: 0.5161\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.0960 - regression_loss: 1.5795 - classification_loss: 0.5165\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.0967 - regression_loss: 1.5805 - classification_loss: 0.5162\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.0925 - regression_loss: 1.5780 - classification_loss: 0.5145\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.0953 - regression_loss: 1.5799 - classification_loss: 0.5154\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.0924 - regression_loss: 1.5777 - classification_loss: 0.5147\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.0969 - regression_loss: 1.5818 - classification_loss: 0.5151\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.0997 - regression_loss: 1.5833 - classification_loss: 0.5164\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1023 - regression_loss: 1.5860 - classification_loss: 0.5162\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1038 - regression_loss: 1.5879 - classification_loss: 0.5159\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1097 - regression_loss: 1.5917 - classification_loss: 0.5180\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1139 - regression_loss: 1.5950 - classification_loss: 0.5189\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1120 - regression_loss: 1.5944 - classification_loss: 0.5176\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1055 - regression_loss: 1.5901 - classification_loss: 0.5154\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1044 - regression_loss: 1.5898 - classification_loss: 0.5146\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1040 - regression_loss: 1.5908 - classification_loss: 0.5131\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.1057 - regression_loss: 1.5931 - classification_loss: 0.5126\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1031 - regression_loss: 1.5910 - classification_loss: 0.5121\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1014 - regression_loss: 1.5912 - classification_loss: 0.5101\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.0986 - regression_loss: 1.5890 - classification_loss: 0.5095\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.0982 - regression_loss: 1.5884 - classification_loss: 0.5098\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.0993 - regression_loss: 1.5892 - classification_loss: 0.5101\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0999 - regression_loss: 1.5896 - classification_loss: 0.5102\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1047 - regression_loss: 1.5923 - classification_loss: 0.5125\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1083 - regression_loss: 1.5956 - classification_loss: 0.5128\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1060 - regression_loss: 1.5939 - classification_loss: 0.5121\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1085 - regression_loss: 1.5953 - classification_loss: 0.5132\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1086 - regression_loss: 1.5955 - classification_loss: 0.5131\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1016 - regression_loss: 1.5904 - classification_loss: 0.5112\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1047 - regression_loss: 1.5916 - classification_loss: 0.5131\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1078 - regression_loss: 1.5939 - classification_loss: 0.5139\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1083 - regression_loss: 1.5942 - classification_loss: 0.5141\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1072 - regression_loss: 1.5938 - classification_loss: 0.5135 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1088 - regression_loss: 1.5948 - classification_loss: 0.5139\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1060 - regression_loss: 1.5926 - classification_loss: 0.5134\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1088 - regression_loss: 1.5949 - classification_loss: 0.5139\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1077 - regression_loss: 1.5939 - classification_loss: 0.5138\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1067 - regression_loss: 1.5931 - classification_loss: 0.5136\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1059 - regression_loss: 1.5924 - classification_loss: 0.5134\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1058 - regression_loss: 1.5927 - classification_loss: 0.5130\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1082 - regression_loss: 1.5945 - classification_loss: 0.5137\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1061 - regression_loss: 1.5937 - classification_loss: 0.5124\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1061 - regression_loss: 1.5938 - classification_loss: 0.5123\n",
            "Epoch 70: saving model to ./snapshots\\resnet50_csv_70.h5\n",
            "\n",
            "Epoch 70: ReduceLROnPlateau reducing learning rate to 9.999999796611899e-33.\n",
            "\n",
            "86/86 [==============================] - 83s 955ms/step - loss: 2.1061 - regression_loss: 1.5938 - classification_loss: 0.5123 - lr: 1.0000e-31\n",
            "Epoch 71/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:42 - loss: 2.0411 - regression_loss: 1.5821 - classification_loss: 0.4590\n",
            " 2/86 [..............................] - ETA: 1:20 - loss: 2.1256 - regression_loss: 1.6336 - classification_loss: 0.4920\n",
            " 3/86 [>.............................] - ETA: 1:29 - loss: 2.1086 - regression_loss: 1.6033 - classification_loss: 0.5053\n",
            " 4/86 [>.............................] - ETA: 1:24 - loss: 2.1648 - regression_loss: 1.6490 - classification_loss: 0.5158\n",
            " 5/86 [>.............................] - ETA: 1:20 - loss: 2.1584 - regression_loss: 1.6261 - classification_loss: 0.5323\n",
            " 6/86 [=>............................] - ETA: 1:19 - loss: 2.1776 - regression_loss: 1.6513 - classification_loss: 0.5263\n",
            " 7/86 [=>............................] - ETA: 1:15 - loss: 2.0955 - regression_loss: 1.5911 - classification_loss: 0.5044\n",
            " 8/86 [=>............................] - ETA: 1:13 - loss: 2.1269 - regression_loss: 1.6183 - classification_loss: 0.5086\n",
            " 9/86 [==>...........................] - ETA: 1:12 - loss: 2.0934 - regression_loss: 1.5882 - classification_loss: 0.5052\n",
            "10/86 [==>...........................] - ETA: 1:09 - loss: 2.1053 - regression_loss: 1.5933 - classification_loss: 0.5120\n",
            "11/86 [==>...........................] - ETA: 1:08 - loss: 2.0811 - regression_loss: 1.5692 - classification_loss: 0.5119\n",
            "12/86 [===>..........................] - ETA: 1:10 - loss: 2.0738 - regression_loss: 1.5646 - classification_loss: 0.5092\n",
            "13/86 [===>..........................] - ETA: 1:09 - loss: 2.1060 - regression_loss: 1.5904 - classification_loss: 0.5156\n",
            "14/86 [===>..........................] - ETA: 1:08 - loss: 2.1026 - regression_loss: 1.5900 - classification_loss: 0.5125\n",
            "15/86 [====>.........................] - ETA: 1:06 - loss: 2.0989 - regression_loss: 1.5879 - classification_loss: 0.5110\n",
            "16/86 [====>.........................] - ETA: 1:05 - loss: 2.0774 - regression_loss: 1.5726 - classification_loss: 0.5048\n",
            "17/86 [====>.........................] - ETA: 1:04 - loss: 2.0747 - regression_loss: 1.5734 - classification_loss: 0.5012\n",
            "18/86 [=====>........................] - ETA: 1:03 - loss: 2.0720 - regression_loss: 1.5723 - classification_loss: 0.4996\n",
            "19/86 [=====>........................] - ETA: 1:02 - loss: 2.0740 - regression_loss: 1.5725 - classification_loss: 0.5015\n",
            "20/86 [=====>........................] - ETA: 1:01 - loss: 2.0891 - regression_loss: 1.5826 - classification_loss: 0.5065\n",
            "21/86 [======>.......................] - ETA: 1:01 - loss: 2.0840 - regression_loss: 1.5764 - classification_loss: 0.5076\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.0838 - regression_loss: 1.5749 - classification_loss: 0.5089\n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.0780 - regression_loss: 1.5689 - classification_loss: 0.5091 \n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.0690 - regression_loss: 1.5612 - classification_loss: 0.5077\n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.0777 - regression_loss: 1.5681 - classification_loss: 0.5096\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.0751 - regression_loss: 1.5677 - classification_loss: 0.5074\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.0774 - regression_loss: 1.5703 - classification_loss: 0.5071\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.0743 - regression_loss: 1.5684 - classification_loss: 0.5060\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.0653 - regression_loss: 1.5631 - classification_loss: 0.5022\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.0682 - regression_loss: 1.5654 - classification_loss: 0.5028\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.0747 - regression_loss: 1.5693 - classification_loss: 0.5053\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.0739 - regression_loss: 1.5692 - classification_loss: 0.5047\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.0650 - regression_loss: 1.5619 - classification_loss: 0.5031\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.0658 - regression_loss: 1.5633 - classification_loss: 0.5025\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.0672 - regression_loss: 1.5636 - classification_loss: 0.5036\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.0752 - regression_loss: 1.5724 - classification_loss: 0.5028\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.0758 - regression_loss: 1.5728 - classification_loss: 0.5030\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.0708 - regression_loss: 1.5688 - classification_loss: 0.5020\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.0656 - regression_loss: 1.5650 - classification_loss: 0.5006\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.0751 - regression_loss: 1.5720 - classification_loss: 0.5030\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.0746 - regression_loss: 1.5718 - classification_loss: 0.5028\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.0809 - regression_loss: 1.5744 - classification_loss: 0.5065\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.0794 - regression_loss: 1.5734 - classification_loss: 0.5061\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.0871 - regression_loss: 1.5791 - classification_loss: 0.5080\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.0884 - regression_loss: 1.5809 - classification_loss: 0.5076\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.0973 - regression_loss: 1.5867 - classification_loss: 0.5106\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.0960 - regression_loss: 1.5855 - classification_loss: 0.5105\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.0968 - regression_loss: 1.5862 - classification_loss: 0.5105\n",
            "49/86 [================>.............] - ETA: 34s - loss: 2.1025 - regression_loss: 1.5914 - classification_loss: 0.5112\n",
            "50/86 [================>.............] - ETA: 33s - loss: 2.0993 - regression_loss: 1.5888 - classification_loss: 0.5104\n",
            "51/86 [================>.............] - ETA: 32s - loss: 2.1046 - regression_loss: 1.5911 - classification_loss: 0.5135\n",
            "52/86 [=================>............] - ETA: 31s - loss: 2.1052 - regression_loss: 1.5926 - classification_loss: 0.5126\n",
            "53/86 [=================>............] - ETA: 30s - loss: 2.0996 - regression_loss: 1.5887 - classification_loss: 0.5109\n",
            "54/86 [=================>............] - ETA: 29s - loss: 2.1011 - regression_loss: 1.5899 - classification_loss: 0.5112\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0996 - regression_loss: 1.5892 - classification_loss: 0.5104\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1033 - regression_loss: 1.5913 - classification_loss: 0.5120\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1069 - regression_loss: 1.5921 - classification_loss: 0.5148\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1048 - regression_loss: 1.5900 - classification_loss: 0.5148\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1081 - regression_loss: 1.5926 - classification_loss: 0.5154\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1098 - regression_loss: 1.5942 - classification_loss: 0.5156\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1125 - regression_loss: 1.5962 - classification_loss: 0.5163\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1168 - regression_loss: 1.5991 - classification_loss: 0.5176\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1184 - regression_loss: 1.5999 - classification_loss: 0.5185\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1191 - regression_loss: 1.5996 - classification_loss: 0.5195\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1172 - regression_loss: 1.5981 - classification_loss: 0.5191\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.1077 - regression_loss: 1.5915 - classification_loss: 0.5162\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.1057 - regression_loss: 1.5894 - classification_loss: 0.5164\n",
            "68/86 [======================>.......] - ETA: 16s - loss: 2.1015 - regression_loss: 1.5870 - classification_loss: 0.5145\n",
            "69/86 [=======================>......] - ETA: 15s - loss: 2.1047 - regression_loss: 1.5900 - classification_loss: 0.5147\n",
            "70/86 [=======================>......] - ETA: 14s - loss: 2.1039 - regression_loss: 1.5864 - classification_loss: 0.5176\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1116 - regression_loss: 1.5927 - classification_loss: 0.5189\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1111 - regression_loss: 1.5929 - classification_loss: 0.5181\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1133 - regression_loss: 1.5944 - classification_loss: 0.5189\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1128 - regression_loss: 1.5940 - classification_loss: 0.5189\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1160 - regression_loss: 1.5963 - classification_loss: 0.5197\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1146 - regression_loss: 1.5950 - classification_loss: 0.5196 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1168 - regression_loss: 1.5958 - classification_loss: 0.5210\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1140 - regression_loss: 1.5941 - classification_loss: 0.5200\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1156 - regression_loss: 1.5951 - classification_loss: 0.5205\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1139 - regression_loss: 1.5938 - classification_loss: 0.5201\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1066 - regression_loss: 1.5883 - classification_loss: 0.5183\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1095 - regression_loss: 1.5904 - classification_loss: 0.5190\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1047 - regression_loss: 1.5868 - classification_loss: 0.5179\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1056 - regression_loss: 1.5878 - classification_loss: 0.5178\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1042 - regression_loss: 1.5867 - classification_loss: 0.5175\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1022 - regression_loss: 1.5852 - classification_loss: 0.5170\n",
            "Epoch 71: saving model to ./snapshots\\resnet50_csv_71.h5\n",
            "\n",
            "86/86 [==============================] - 83s 960ms/step - loss: 2.1022 - regression_loss: 1.5852 - classification_loss: 0.5170 - lr: 1.0000e-32\n",
            "Epoch 72/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:42 - loss: 2.0872 - regression_loss: 1.5162 - classification_loss: 0.5711\n",
            " 2/86 [..............................] - ETA: 1:10 - loss: 2.0678 - regression_loss: 1.5363 - classification_loss: 0.5315\n",
            " 3/86 [>.............................] - ETA: 1:09 - loss: 2.0453 - regression_loss: 1.5263 - classification_loss: 0.5190\n",
            " 4/86 [>.............................] - ETA: 1:10 - loss: 2.0893 - regression_loss: 1.5739 - classification_loss: 0.5153\n",
            " 5/86 [>.............................] - ETA: 1:10 - loss: 2.0986 - regression_loss: 1.5812 - classification_loss: 0.5174\n",
            " 6/86 [=>............................] - ETA: 1:15 - loss: 2.0771 - regression_loss: 1.5798 - classification_loss: 0.4973\n",
            " 7/86 [=>............................] - ETA: 1:12 - loss: 2.0970 - regression_loss: 1.5971 - classification_loss: 0.4999\n",
            " 8/86 [=>............................] - ETA: 1:12 - loss: 2.1078 - regression_loss: 1.6070 - classification_loss: 0.5008\n",
            " 9/86 [==>...........................] - ETA: 1:12 - loss: 2.1209 - regression_loss: 1.6113 - classification_loss: 0.5096\n",
            "10/86 [==>...........................] - ETA: 1:09 - loss: 2.0840 - regression_loss: 1.5869 - classification_loss: 0.4970\n",
            "11/86 [==>...........................] - ETA: 1:08 - loss: 2.0551 - regression_loss: 1.5613 - classification_loss: 0.4937\n",
            "12/86 [===>..........................] - ETA: 1:09 - loss: 2.0972 - regression_loss: 1.5972 - classification_loss: 0.5001\n",
            "13/86 [===>..........................] - ETA: 1:08 - loss: 2.0879 - regression_loss: 1.5878 - classification_loss: 0.5002\n",
            "14/86 [===>..........................] - ETA: 1:08 - loss: 2.0911 - regression_loss: 1.5941 - classification_loss: 0.4970\n",
            "15/86 [====>.........................] - ETA: 1:06 - loss: 2.1065 - regression_loss: 1.6070 - classification_loss: 0.4995\n",
            "16/86 [====>.........................] - ETA: 1:05 - loss: 2.0895 - regression_loss: 1.5935 - classification_loss: 0.4960\n",
            "17/86 [====>.........................] - ETA: 1:04 - loss: 2.0900 - regression_loss: 1.5921 - classification_loss: 0.4979\n",
            "18/86 [=====>........................] - ETA: 1:03 - loss: 2.1041 - regression_loss: 1.6048 - classification_loss: 0.4992\n",
            "19/86 [=====>........................] - ETA: 1:03 - loss: 2.1128 - regression_loss: 1.6075 - classification_loss: 0.5053\n",
            "20/86 [=====>........................] - ETA: 1:02 - loss: 2.1005 - regression_loss: 1.5987 - classification_loss: 0.5018\n",
            "21/86 [======>.......................] - ETA: 1:01 - loss: 2.1114 - regression_loss: 1.6054 - classification_loss: 0.5060\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.1308 - regression_loss: 1.6210 - classification_loss: 0.5098\n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.1345 - regression_loss: 1.6237 - classification_loss: 0.5108 \n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.1291 - regression_loss: 1.6198 - classification_loss: 0.5093\n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.1242 - regression_loss: 1.6165 - classification_loss: 0.5077\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.1270 - regression_loss: 1.6190 - classification_loss: 0.5080\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.1402 - regression_loss: 1.6292 - classification_loss: 0.5109\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.1307 - regression_loss: 1.6200 - classification_loss: 0.5108\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.1374 - regression_loss: 1.6251 - classification_loss: 0.5123\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.1359 - regression_loss: 1.6234 - classification_loss: 0.5124\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.1505 - regression_loss: 1.6352 - classification_loss: 0.5152\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.1449 - regression_loss: 1.6316 - classification_loss: 0.5133\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.1400 - regression_loss: 1.6273 - classification_loss: 0.5127\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.1469 - regression_loss: 1.6346 - classification_loss: 0.5123\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.1422 - regression_loss: 1.6307 - classification_loss: 0.5115\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.1380 - regression_loss: 1.6287 - classification_loss: 0.5093\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.1386 - regression_loss: 1.6287 - classification_loss: 0.5099\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.1436 - regression_loss: 1.6323 - classification_loss: 0.5113\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.1406 - regression_loss: 1.6301 - classification_loss: 0.5105\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.1394 - regression_loss: 1.6294 - classification_loss: 0.5100\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.1406 - regression_loss: 1.6302 - classification_loss: 0.5104\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1402 - regression_loss: 1.6293 - classification_loss: 0.5108\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1396 - regression_loss: 1.6272 - classification_loss: 0.5124\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1358 - regression_loss: 1.6246 - classification_loss: 0.5112\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.1406 - regression_loss: 1.6288 - classification_loss: 0.5118\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1342 - regression_loss: 1.6243 - classification_loss: 0.5099\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1356 - regression_loss: 1.6246 - classification_loss: 0.5110\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1387 - regression_loss: 1.6271 - classification_loss: 0.5116\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1416 - regression_loss: 1.6296 - classification_loss: 0.5120\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1465 - regression_loss: 1.6332 - classification_loss: 0.5134\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1408 - regression_loss: 1.6290 - classification_loss: 0.5118\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1397 - regression_loss: 1.6281 - classification_loss: 0.5116\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1442 - regression_loss: 1.6298 - classification_loss: 0.5143\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1421 - regression_loss: 1.6280 - classification_loss: 0.5141\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1421 - regression_loss: 1.6281 - classification_loss: 0.5141\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1502 - regression_loss: 1.6328 - classification_loss: 0.5175\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1510 - regression_loss: 1.6331 - classification_loss: 0.5179\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1476 - regression_loss: 1.6308 - classification_loss: 0.5168\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1454 - regression_loss: 1.6288 - classification_loss: 0.5166\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1448 - regression_loss: 1.6281 - classification_loss: 0.5167\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1386 - regression_loss: 1.6229 - classification_loss: 0.5157\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1402 - regression_loss: 1.6238 - classification_loss: 0.5164\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.1391 - regression_loss: 1.6222 - classification_loss: 0.5169\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.1391 - regression_loss: 1.6218 - classification_loss: 0.5173\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1378 - regression_loss: 1.6202 - classification_loss: 0.5176\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1390 - regression_loss: 1.6214 - classification_loss: 0.5176\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1395 - regression_loss: 1.6215 - classification_loss: 0.5180\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1380 - regression_loss: 1.6213 - classification_loss: 0.5168\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1347 - regression_loss: 1.6181 - classification_loss: 0.5167\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1276 - regression_loss: 1.6128 - classification_loss: 0.5147\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1280 - regression_loss: 1.6133 - classification_loss: 0.5147\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1241 - regression_loss: 1.6100 - classification_loss: 0.5142\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1240 - regression_loss: 1.6088 - classification_loss: 0.5152\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1226 - regression_loss: 1.6079 - classification_loss: 0.5147\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1192 - regression_loss: 1.6052 - classification_loss: 0.5140\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1183 - regression_loss: 1.6049 - classification_loss: 0.5133 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1202 - regression_loss: 1.6072 - classification_loss: 0.5130\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1219 - regression_loss: 1.6088 - classification_loss: 0.5131\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1208 - regression_loss: 1.6078 - classification_loss: 0.5130\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1214 - regression_loss: 1.6082 - classification_loss: 0.5132\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1212 - regression_loss: 1.6075 - classification_loss: 0.5137\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1196 - regression_loss: 1.6063 - classification_loss: 0.5133\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1189 - regression_loss: 1.6058 - classification_loss: 0.5131\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1181 - regression_loss: 1.6046 - classification_loss: 0.5135\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1189 - regression_loss: 1.6052 - classification_loss: 0.5137\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1200 - regression_loss: 1.6058 - classification_loss: 0.5141\n",
            "Epoch 72: saving model to ./snapshots\\resnet50_csv_72.h5\n",
            "\n",
            "Epoch 72: ReduceLROnPlateau reducing learning rate to 9.999999502738312e-34.\n",
            "\n",
            "86/86 [==============================] - 83s 958ms/step - loss: 2.1200 - regression_loss: 1.6058 - classification_loss: 0.5141 - lr: 1.0000e-32\n",
            "Epoch 73/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:43 - loss: 2.0878 - regression_loss: 1.5719 - classification_loss: 0.5159\n",
            " 2/86 [..............................] - ETA: 1:10 - loss: 2.1555 - regression_loss: 1.6289 - classification_loss: 0.5266\n",
            " 3/86 [>.............................] - ETA: 1:13 - loss: 2.1195 - regression_loss: 1.6029 - classification_loss: 0.5166\n",
            " 4/86 [>.............................] - ETA: 1:22 - loss: 2.1228 - regression_loss: 1.6100 - classification_loss: 0.5128\n",
            " 5/86 [>.............................] - ETA: 1:17 - loss: 2.1265 - regression_loss: 1.5993 - classification_loss: 0.5272\n",
            " 6/86 [=>............................] - ETA: 1:16 - loss: 2.1502 - regression_loss: 1.6251 - classification_loss: 0.5251\n",
            " 7/86 [=>............................] - ETA: 1:19 - loss: 2.1344 - regression_loss: 1.6146 - classification_loss: 0.5198\n",
            " 8/86 [=>............................] - ETA: 1:19 - loss: 2.1459 - regression_loss: 1.6263 - classification_loss: 0.5196\n",
            " 9/86 [==>...........................] - ETA: 1:15 - loss: 2.1304 - regression_loss: 1.6139 - classification_loss: 0.5165\n",
            "10/86 [==>...........................] - ETA: 1:14 - loss: 2.1341 - regression_loss: 1.6179 - classification_loss: 0.5162\n",
            "11/86 [==>...........................] - ETA: 1:13 - loss: 2.1176 - regression_loss: 1.6088 - classification_loss: 0.5088\n",
            "12/86 [===>..........................] - ETA: 1:14 - loss: 2.1185 - regression_loss: 1.6096 - classification_loss: 0.5089\n",
            "13/86 [===>..........................] - ETA: 1:13 - loss: 2.1295 - regression_loss: 1.6195 - classification_loss: 0.5100\n",
            "14/86 [===>..........................] - ETA: 1:11 - loss: 2.1374 - regression_loss: 1.6253 - classification_loss: 0.5121\n",
            "15/86 [====>.........................] - ETA: 1:10 - loss: 2.1498 - regression_loss: 1.6366 - classification_loss: 0.5131\n",
            "16/86 [====>.........................] - ETA: 1:10 - loss: 2.1353 - regression_loss: 1.6259 - classification_loss: 0.5093\n",
            "17/86 [====>.........................] - ETA: 1:08 - loss: 2.1339 - regression_loss: 1.6217 - classification_loss: 0.5121\n",
            "18/86 [=====>........................] - ETA: 1:07 - loss: 2.1398 - regression_loss: 1.6292 - classification_loss: 0.5106\n",
            "19/86 [=====>........................] - ETA: 1:04 - loss: 2.1546 - regression_loss: 1.6387 - classification_loss: 0.5158\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.1531 - regression_loss: 1.6360 - classification_loss: 0.5171\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.1525 - regression_loss: 1.6346 - classification_loss: 0.5179\n",
            "22/86 [======>.......................] - ETA: 1:02 - loss: 2.1462 - regression_loss: 1.6318 - classification_loss: 0.5144\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.1401 - regression_loss: 1.6261 - classification_loss: 0.5140\n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.1453 - regression_loss: 1.6290 - classification_loss: 0.5163 \n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.1432 - regression_loss: 1.6264 - classification_loss: 0.5168\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.1480 - regression_loss: 1.6303 - classification_loss: 0.5176\n",
            "27/86 [========>.....................] - ETA: 57s - loss: 2.1425 - regression_loss: 1.6263 - classification_loss: 0.5162\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.1360 - regression_loss: 1.6208 - classification_loss: 0.5152\n",
            "29/86 [=========>....................] - ETA: 55s - loss: 2.1422 - regression_loss: 1.6234 - classification_loss: 0.5188\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.1295 - regression_loss: 1.6129 - classification_loss: 0.5165\n",
            "31/86 [=========>....................] - ETA: 53s - loss: 2.1330 - regression_loss: 1.6151 - classification_loss: 0.5179\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.1273 - regression_loss: 1.6126 - classification_loss: 0.5148\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.1374 - regression_loss: 1.6201 - classification_loss: 0.5173\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.1370 - regression_loss: 1.6196 - classification_loss: 0.5173\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.1290 - regression_loss: 1.6120 - classification_loss: 0.5170\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.1241 - regression_loss: 1.6086 - classification_loss: 0.5155\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.1153 - regression_loss: 1.6024 - classification_loss: 0.5129\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.1170 - regression_loss: 1.6034 - classification_loss: 0.5136\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.1205 - regression_loss: 1.6062 - classification_loss: 0.5143\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.1138 - regression_loss: 1.6016 - classification_loss: 0.5122\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.1063 - regression_loss: 1.5954 - classification_loss: 0.5108\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1059 - regression_loss: 1.5953 - classification_loss: 0.5106\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1078 - regression_loss: 1.5976 - classification_loss: 0.5102\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1058 - regression_loss: 1.5981 - classification_loss: 0.5077\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.1049 - regression_loss: 1.5963 - classification_loss: 0.5086\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1059 - regression_loss: 1.5965 - classification_loss: 0.5094\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1001 - regression_loss: 1.5919 - classification_loss: 0.5082\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.0988 - regression_loss: 1.5912 - classification_loss: 0.5075\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.0961 - regression_loss: 1.5892 - classification_loss: 0.5070\n",
            "50/86 [================>.............] - ETA: 35s - loss: 2.0914 - regression_loss: 1.5860 - classification_loss: 0.5054\n",
            "51/86 [================>.............] - ETA: 34s - loss: 2.0878 - regression_loss: 1.5845 - classification_loss: 0.5032\n",
            "52/86 [=================>............] - ETA: 33s - loss: 2.0898 - regression_loss: 1.5859 - classification_loss: 0.5038\n",
            "53/86 [=================>............] - ETA: 32s - loss: 2.0896 - regression_loss: 1.5862 - classification_loss: 0.5035\n",
            "54/86 [=================>............] - ETA: 31s - loss: 2.0941 - regression_loss: 1.5899 - classification_loss: 0.5042\n",
            "55/86 [==================>...........] - ETA: 30s - loss: 2.0934 - regression_loss: 1.5888 - classification_loss: 0.5046\n",
            "56/86 [==================>...........] - ETA: 29s - loss: 2.0965 - regression_loss: 1.5905 - classification_loss: 0.5059\n",
            "57/86 [==================>...........] - ETA: 28s - loss: 2.1000 - regression_loss: 1.5932 - classification_loss: 0.5068\n",
            "58/86 [===================>..........] - ETA: 27s - loss: 2.0977 - regression_loss: 1.5906 - classification_loss: 0.5071\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.0974 - regression_loss: 1.5897 - classification_loss: 0.5077\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.0967 - regression_loss: 1.5903 - classification_loss: 0.5064\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.0975 - regression_loss: 1.5914 - classification_loss: 0.5061\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.0974 - regression_loss: 1.5911 - classification_loss: 0.5063\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.0928 - regression_loss: 1.5872 - classification_loss: 0.5055\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.0915 - regression_loss: 1.5855 - classification_loss: 0.5059\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.0951 - regression_loss: 1.5870 - classification_loss: 0.5081\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0956 - regression_loss: 1.5868 - classification_loss: 0.5088\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0974 - regression_loss: 1.5877 - classification_loss: 0.5098\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1015 - regression_loss: 1.5912 - classification_loss: 0.5103\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0996 - regression_loss: 1.5899 - classification_loss: 0.5097\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0961 - regression_loss: 1.5874 - classification_loss: 0.5087\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0917 - regression_loss: 1.5848 - classification_loss: 0.5069\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0943 - regression_loss: 1.5875 - classification_loss: 0.5068\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0927 - regression_loss: 1.5860 - classification_loss: 0.5067\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0943 - regression_loss: 1.5863 - classification_loss: 0.5080\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0972 - regression_loss: 1.5883 - classification_loss: 0.5089\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0941 - regression_loss: 1.5855 - classification_loss: 0.5086 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0954 - regression_loss: 1.5872 - classification_loss: 0.5082\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0946 - regression_loss: 1.5859 - classification_loss: 0.5087\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0976 - regression_loss: 1.5875 - classification_loss: 0.5100\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0985 - regression_loss: 1.5885 - classification_loss: 0.5100\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0991 - regression_loss: 1.5893 - classification_loss: 0.5099\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1007 - regression_loss: 1.5900 - classification_loss: 0.5106\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0983 - regression_loss: 1.5883 - classification_loss: 0.5101\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1001 - regression_loss: 1.5893 - classification_loss: 0.5108\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1027 - regression_loss: 1.5913 - classification_loss: 0.5114\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1060 - regression_loss: 1.5941 - classification_loss: 0.5119\n",
            "Epoch 73: saving model to ./snapshots\\resnet50_csv_73.h5\n",
            "\n",
            "86/86 [==============================] - 83s 965ms/step - loss: 2.1060 - regression_loss: 1.5941 - classification_loss: 0.5119 - lr: 1.0000e-33\n",
            "Epoch 74/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:48 - loss: 2.2407 - regression_loss: 1.6705 - classification_loss: 0.5702\n",
            " 2/86 [..............................] - ETA: 1:51 - loss: 2.1660 - regression_loss: 1.6082 - classification_loss: 0.5578\n",
            " 3/86 [>.............................] - ETA: 1:34 - loss: 2.1157 - regression_loss: 1.5913 - classification_loss: 0.5244\n",
            " 4/86 [>.............................] - ETA: 1:28 - loss: 2.1300 - regression_loss: 1.6100 - classification_loss: 0.5200\n",
            " 5/86 [>.............................] - ETA: 1:23 - loss: 2.0844 - regression_loss: 1.5810 - classification_loss: 0.5034\n",
            " 6/86 [=>............................] - ETA: 1:22 - loss: 2.0690 - regression_loss: 1.5709 - classification_loss: 0.4981\n",
            " 7/86 [=>............................] - ETA: 1:24 - loss: 2.0759 - regression_loss: 1.5730 - classification_loss: 0.5029\n",
            " 8/86 [=>............................] - ETA: 1:20 - loss: 2.0631 - regression_loss: 1.5655 - classification_loss: 0.4976\n",
            " 9/86 [==>...........................] - ETA: 1:17 - loss: 2.0814 - regression_loss: 1.5786 - classification_loss: 0.5028\n",
            "10/86 [==>...........................] - ETA: 1:15 - loss: 2.0692 - regression_loss: 1.5710 - classification_loss: 0.4982\n",
            "11/86 [==>...........................] - ETA: 1:14 - loss: 2.0676 - regression_loss: 1.5715 - classification_loss: 0.4961\n",
            "12/86 [===>..........................] - ETA: 1:10 - loss: 2.0716 - regression_loss: 1.5700 - classification_loss: 0.5017\n",
            "13/86 [===>..........................] - ETA: 1:09 - loss: 2.1067 - regression_loss: 1.5960 - classification_loss: 0.5107\n",
            "14/86 [===>..........................] - ETA: 1:08 - loss: 2.0988 - regression_loss: 1.5869 - classification_loss: 0.5120\n",
            "15/86 [====>.........................] - ETA: 1:07 - loss: 2.0746 - regression_loss: 1.5696 - classification_loss: 0.5049\n",
            "16/86 [====>.........................] - ETA: 1:05 - loss: 2.0547 - regression_loss: 1.5539 - classification_loss: 0.5008\n",
            "17/86 [====>.........................] - ETA: 1:04 - loss: 2.0515 - regression_loss: 1.5512 - classification_loss: 0.5003\n",
            "18/86 [=====>........................] - ETA: 1:03 - loss: 2.0546 - regression_loss: 1.5550 - classification_loss: 0.4997\n",
            "19/86 [=====>........................] - ETA: 1:02 - loss: 2.0757 - regression_loss: 1.5707 - classification_loss: 0.5050\n",
            "20/86 [=====>........................] - ETA: 1:02 - loss: 2.0830 - regression_loss: 1.5782 - classification_loss: 0.5048\n",
            "21/86 [======>.......................] - ETA: 1:01 - loss: 2.0688 - regression_loss: 1.5694 - classification_loss: 0.4994\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.0730 - regression_loss: 1.5733 - classification_loss: 0.4997\n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.0724 - regression_loss: 1.5632 - classification_loss: 0.5092 \n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.0817 - regression_loss: 1.5697 - classification_loss: 0.5120\n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.0742 - regression_loss: 1.5650 - classification_loss: 0.5092\n",
            "26/86 [========>.....................] - ETA: 56s - loss: 2.0758 - regression_loss: 1.5677 - classification_loss: 0.5082\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.0782 - regression_loss: 1.5699 - classification_loss: 0.5083\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.0718 - regression_loss: 1.5656 - classification_loss: 0.5063\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.0721 - regression_loss: 1.5652 - classification_loss: 0.5069\n",
            "30/86 [=========>....................] - ETA: 52s - loss: 2.0643 - regression_loss: 1.5598 - classification_loss: 0.5045\n",
            "31/86 [=========>....................] - ETA: 51s - loss: 2.0601 - regression_loss: 1.5566 - classification_loss: 0.5034\n",
            "32/86 [==========>...................] - ETA: 50s - loss: 2.0686 - regression_loss: 1.5657 - classification_loss: 0.5029\n",
            "33/86 [==========>...................] - ETA: 49s - loss: 2.0644 - regression_loss: 1.5627 - classification_loss: 0.5017\n",
            "34/86 [==========>...................] - ETA: 48s - loss: 2.0688 - regression_loss: 1.5660 - classification_loss: 0.5028\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.0762 - regression_loss: 1.5741 - classification_loss: 0.5021\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.0784 - regression_loss: 1.5763 - classification_loss: 0.5021\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.0795 - regression_loss: 1.5777 - classification_loss: 0.5018\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.0812 - regression_loss: 1.5784 - classification_loss: 0.5028\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.0890 - regression_loss: 1.5844 - classification_loss: 0.5046\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.0898 - regression_loss: 1.5859 - classification_loss: 0.5040\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.0913 - regression_loss: 1.5873 - classification_loss: 0.5040\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.0887 - regression_loss: 1.5844 - classification_loss: 0.5043\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.0928 - regression_loss: 1.5851 - classification_loss: 0.5078\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.0909 - regression_loss: 1.5847 - classification_loss: 0.5062\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.0888 - regression_loss: 1.5829 - classification_loss: 0.5060\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.0916 - regression_loss: 1.5868 - classification_loss: 0.5047\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.0930 - regression_loss: 1.5877 - classification_loss: 0.5053\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.0895 - regression_loss: 1.5823 - classification_loss: 0.5072\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.0842 - regression_loss: 1.5786 - classification_loss: 0.5056\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.0786 - regression_loss: 1.5739 - classification_loss: 0.5046\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.0883 - regression_loss: 1.5816 - classification_loss: 0.5067\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.0837 - regression_loss: 1.5775 - classification_loss: 0.5062\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0795 - regression_loss: 1.5748 - classification_loss: 0.5048\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0853 - regression_loss: 1.5797 - classification_loss: 0.5055\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0833 - regression_loss: 1.5781 - classification_loss: 0.5052\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0841 - regression_loss: 1.5786 - classification_loss: 0.5055\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0889 - regression_loss: 1.5821 - classification_loss: 0.5068\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0905 - regression_loss: 1.5837 - classification_loss: 0.5069\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0894 - regression_loss: 1.5815 - classification_loss: 0.5078\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0915 - regression_loss: 1.5838 - classification_loss: 0.5077\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0947 - regression_loss: 1.5867 - classification_loss: 0.5080\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.0934 - regression_loss: 1.5855 - classification_loss: 0.5079\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.0963 - regression_loss: 1.5880 - classification_loss: 0.5082\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.0882 - regression_loss: 1.5822 - classification_loss: 0.5060\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.0904 - regression_loss: 1.5834 - classification_loss: 0.5070\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0929 - regression_loss: 1.5849 - classification_loss: 0.5079\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0927 - regression_loss: 1.5847 - classification_loss: 0.5081\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0963 - regression_loss: 1.5870 - classification_loss: 0.5093\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0957 - regression_loss: 1.5868 - classification_loss: 0.5089\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0878 - regression_loss: 1.5810 - classification_loss: 0.5068\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0914 - regression_loss: 1.5825 - classification_loss: 0.5088\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0897 - regression_loss: 1.5819 - classification_loss: 0.5078\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0918 - regression_loss: 1.5841 - classification_loss: 0.5077\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0924 - regression_loss: 1.5848 - classification_loss: 0.5076\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0915 - regression_loss: 1.5838 - classification_loss: 0.5077\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0892 - regression_loss: 1.5822 - classification_loss: 0.5070 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0875 - regression_loss: 1.5813 - classification_loss: 0.5062\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0920 - regression_loss: 1.5849 - classification_loss: 0.5072\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0904 - regression_loss: 1.5842 - classification_loss: 0.5062\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0864 - regression_loss: 1.5817 - classification_loss: 0.5046\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0864 - regression_loss: 1.5815 - classification_loss: 0.5049\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0849 - regression_loss: 1.5798 - classification_loss: 0.5051\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0855 - regression_loss: 1.5803 - classification_loss: 0.5052\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0827 - regression_loss: 1.5788 - classification_loss: 0.5039\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0816 - regression_loss: 1.5774 - classification_loss: 0.5042\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0844 - regression_loss: 1.5802 - classification_loss: 0.5042\n",
            "Epoch 74: saving model to ./snapshots\\resnet50_csv_74.h5\n",
            "\n",
            "Epoch 74: ReduceLROnPlateau reducing learning rate to 9.999999319067318e-35.\n",
            "\n",
            "86/86 [==============================] - 83s 957ms/step - loss: 2.0844 - regression_loss: 1.5802 - classification_loss: 0.5042 - lr: 1.0000e-33\n",
            "Epoch 75/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:41 - loss: 1.6734 - regression_loss: 1.2843 - classification_loss: 0.3891\n",
            " 2/86 [..............................] - ETA: 1:36 - loss: 1.8378 - regression_loss: 1.4205 - classification_loss: 0.4173\n",
            " 3/86 [>.............................] - ETA: 1:25 - loss: 1.9616 - regression_loss: 1.4923 - classification_loss: 0.4693\n",
            " 4/86 [>.............................] - ETA: 1:23 - loss: 1.9945 - regression_loss: 1.5205 - classification_loss: 0.4740\n",
            " 5/86 [>.............................] - ETA: 1:18 - loss: 2.0480 - regression_loss: 1.5567 - classification_loss: 0.4913\n",
            " 6/86 [=>............................] - ETA: 1:15 - loss: 2.0902 - regression_loss: 1.5875 - classification_loss: 0.5026\n",
            " 7/86 [=>............................] - ETA: 1:12 - loss: 2.1503 - regression_loss: 1.6336 - classification_loss: 0.5167\n",
            " 8/86 [=>............................] - ETA: 1:10 - loss: 2.1565 - regression_loss: 1.6393 - classification_loss: 0.5172\n",
            " 9/86 [==>...........................] - ETA: 1:10 - loss: 2.1498 - regression_loss: 1.6381 - classification_loss: 0.5118\n",
            "10/86 [==>...........................] - ETA: 1:08 - loss: 2.1439 - regression_loss: 1.6347 - classification_loss: 0.5092\n",
            "11/86 [==>...........................] - ETA: 1:07 - loss: 2.1553 - regression_loss: 1.6447 - classification_loss: 0.5106\n",
            "12/86 [===>..........................] - ETA: 1:05 - loss: 2.1365 - regression_loss: 1.6294 - classification_loss: 0.5071\n",
            "13/86 [===>..........................] - ETA: 1:08 - loss: 2.1380 - regression_loss: 1.6325 - classification_loss: 0.5055\n",
            "14/86 [===>..........................] - ETA: 1:06 - loss: 2.1515 - regression_loss: 1.6414 - classification_loss: 0.5100\n",
            "15/86 [====>.........................] - ETA: 1:07 - loss: 2.1458 - regression_loss: 1.6389 - classification_loss: 0.5069\n",
            "16/86 [====>.........................] - ETA: 1:05 - loss: 2.1493 - regression_loss: 1.6431 - classification_loss: 0.5061\n",
            "17/86 [====>.........................] - ETA: 1:05 - loss: 2.1579 - regression_loss: 1.6508 - classification_loss: 0.5071\n",
            "18/86 [=====>........................] - ETA: 1:04 - loss: 2.1392 - regression_loss: 1.6355 - classification_loss: 0.5037\n",
            "19/86 [=====>........................] - ETA: 1:03 - loss: 2.1389 - regression_loss: 1.6326 - classification_loss: 0.5063\n",
            "20/86 [=====>........................] - ETA: 1:02 - loss: 2.1276 - regression_loss: 1.6220 - classification_loss: 0.5056\n",
            "21/86 [======>.......................] - ETA: 1:00 - loss: 2.1287 - regression_loss: 1.6205 - classification_loss: 0.5082\n",
            "22/86 [======>.......................] - ETA: 59s - loss: 2.1257 - regression_loss: 1.6194 - classification_loss: 0.5063 \n",
            "23/86 [=======>......................] - ETA: 58s - loss: 2.1299 - regression_loss: 1.6243 - classification_loss: 0.5057\n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.1090 - regression_loss: 1.6087 - classification_loss: 0.5003\n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.1108 - regression_loss: 1.6100 - classification_loss: 0.5008\n",
            "26/86 [========>.....................] - ETA: 55s - loss: 2.1006 - regression_loss: 1.6015 - classification_loss: 0.4991\n",
            "27/86 [========>.....................] - ETA: 54s - loss: 2.1002 - regression_loss: 1.6018 - classification_loss: 0.4984\n",
            "28/86 [========>.....................] - ETA: 53s - loss: 2.0997 - regression_loss: 1.5990 - classification_loss: 0.5007\n",
            "29/86 [=========>....................] - ETA: 52s - loss: 2.1010 - regression_loss: 1.6000 - classification_loss: 0.5010\n",
            "30/86 [=========>....................] - ETA: 51s - loss: 2.1000 - regression_loss: 1.5997 - classification_loss: 0.5003\n",
            "31/86 [=========>....................] - ETA: 50s - loss: 2.1112 - regression_loss: 1.6081 - classification_loss: 0.5031\n",
            "32/86 [==========>...................] - ETA: 49s - loss: 2.1159 - regression_loss: 1.6114 - classification_loss: 0.5045\n",
            "33/86 [==========>...................] - ETA: 49s - loss: 2.1291 - regression_loss: 1.6221 - classification_loss: 0.5069\n",
            "34/86 [==========>...................] - ETA: 48s - loss: 2.1372 - regression_loss: 1.6279 - classification_loss: 0.5093\n",
            "35/86 [===========>..................] - ETA: 47s - loss: 2.1400 - regression_loss: 1.6292 - classification_loss: 0.5109\n",
            "36/86 [===========>..................] - ETA: 46s - loss: 2.1419 - regression_loss: 1.6312 - classification_loss: 0.5108\n",
            "37/86 [===========>..................] - ETA: 45s - loss: 2.1386 - regression_loss: 1.6299 - classification_loss: 0.5087\n",
            "38/86 [============>.................] - ETA: 44s - loss: 2.1412 - regression_loss: 1.6317 - classification_loss: 0.5095\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.1300 - regression_loss: 1.6230 - classification_loss: 0.5070\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.1254 - regression_loss: 1.6188 - classification_loss: 0.5066\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.1238 - regression_loss: 1.6177 - classification_loss: 0.5062\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.1206 - regression_loss: 1.6144 - classification_loss: 0.5062\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.1226 - regression_loss: 1.6155 - classification_loss: 0.5071\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.1221 - regression_loss: 1.6155 - classification_loss: 0.5065\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.1259 - regression_loss: 1.6180 - classification_loss: 0.5079\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1305 - regression_loss: 1.6195 - classification_loss: 0.5109\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.1272 - regression_loss: 1.6181 - classification_loss: 0.5092\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.1220 - regression_loss: 1.6138 - classification_loss: 0.5082\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1177 - regression_loss: 1.6111 - classification_loss: 0.5066\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1158 - regression_loss: 1.6087 - classification_loss: 0.5071\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1172 - regression_loss: 1.6098 - classification_loss: 0.5073\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1159 - regression_loss: 1.6079 - classification_loss: 0.5080\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1151 - regression_loss: 1.6087 - classification_loss: 0.5064\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1115 - regression_loss: 1.6058 - classification_loss: 0.5057\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1066 - regression_loss: 1.6016 - classification_loss: 0.5050\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1031 - regression_loss: 1.5985 - classification_loss: 0.5046\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1053 - regression_loss: 1.6004 - classification_loss: 0.5049\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1025 - regression_loss: 1.5987 - classification_loss: 0.5038\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1034 - regression_loss: 1.5996 - classification_loss: 0.5038\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1007 - regression_loss: 1.5972 - classification_loss: 0.5035\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1037 - regression_loss: 1.5991 - classification_loss: 0.5047\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1005 - regression_loss: 1.5968 - classification_loss: 0.5037\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1015 - regression_loss: 1.5966 - classification_loss: 0.5049\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.0984 - regression_loss: 1.5946 - classification_loss: 0.5038\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1004 - regression_loss: 1.5951 - classification_loss: 0.5053\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0957 - regression_loss: 1.5912 - classification_loss: 0.5045\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0943 - regression_loss: 1.5902 - classification_loss: 0.5041\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0932 - regression_loss: 1.5893 - classification_loss: 0.5039\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0995 - regression_loss: 1.5927 - classification_loss: 0.5068\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0994 - regression_loss: 1.5925 - classification_loss: 0.5069\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1021 - regression_loss: 1.5952 - classification_loss: 0.5069\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1008 - regression_loss: 1.5948 - classification_loss: 0.5060\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1034 - regression_loss: 1.5977 - classification_loss: 0.5056\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1036 - regression_loss: 1.5967 - classification_loss: 0.5069\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1034 - regression_loss: 1.5959 - classification_loss: 0.5075\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0996 - regression_loss: 1.5933 - classification_loss: 0.5063 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0992 - regression_loss: 1.5924 - classification_loss: 0.5068\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0989 - regression_loss: 1.5923 - classification_loss: 0.5066\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0979 - regression_loss: 1.5917 - classification_loss: 0.5062\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0967 - regression_loss: 1.5906 - classification_loss: 0.5062\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0940 - regression_loss: 1.5882 - classification_loss: 0.5058\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0913 - regression_loss: 1.5855 - classification_loss: 0.5057\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0881 - regression_loss: 1.5830 - classification_loss: 0.5051\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0895 - regression_loss: 1.5841 - classification_loss: 0.5054\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0888 - regression_loss: 1.5841 - classification_loss: 0.5047\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0896 - regression_loss: 1.5856 - classification_loss: 0.5040\n",
            "Epoch 75: saving model to ./snapshots\\resnet50_csv_75.h5\n",
            "\n",
            "86/86 [==============================] - 82s 952ms/step - loss: 2.0896 - regression_loss: 1.5856 - classification_loss: 0.5040 - lr: 1.0000e-34\n",
            "Epoch 76/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:15 - loss: 1.9191 - regression_loss: 1.4485 - classification_loss: 0.4706\n",
            " 2/86 [..............................] - ETA: 1:21 - loss: 2.0257 - regression_loss: 1.5442 - classification_loss: 0.4815\n",
            " 3/86 [>.............................] - ETA: 1:28 - loss: 2.0085 - regression_loss: 1.5284 - classification_loss: 0.4801\n",
            " 4/86 [>.............................] - ETA: 1:19 - loss: 2.0674 - regression_loss: 1.5680 - classification_loss: 0.4995\n",
            " 5/86 [>.............................] - ETA: 1:15 - loss: 2.1153 - regression_loss: 1.6120 - classification_loss: 0.5033\n",
            " 6/86 [=>............................] - ETA: 1:14 - loss: 2.1207 - regression_loss: 1.6083 - classification_loss: 0.5124\n",
            " 7/86 [=>............................] - ETA: 1:14 - loss: 2.1375 - regression_loss: 1.6210 - classification_loss: 0.5164\n",
            " 8/86 [=>............................] - ETA: 1:16 - loss: 2.1289 - regression_loss: 1.6192 - classification_loss: 0.5097\n",
            " 9/86 [==>...........................] - ETA: 1:14 - loss: 2.1444 - regression_loss: 1.6366 - classification_loss: 0.5079\n",
            "10/86 [==>...........................] - ETA: 1:13 - loss: 2.1384 - regression_loss: 1.6344 - classification_loss: 0.5040\n",
            "11/86 [==>...........................] - ETA: 1:13 - loss: 2.1327 - regression_loss: 1.6288 - classification_loss: 0.5038\n",
            "12/86 [===>..........................] - ETA: 1:12 - loss: 2.1544 - regression_loss: 1.6467 - classification_loss: 0.5078\n",
            "13/86 [===>..........................] - ETA: 1:10 - loss: 2.1408 - regression_loss: 1.6362 - classification_loss: 0.5046\n",
            "14/86 [===>..........................] - ETA: 1:11 - loss: 2.1334 - regression_loss: 1.6257 - classification_loss: 0.5077\n",
            "15/86 [====>.........................] - ETA: 1:09 - loss: 2.1253 - regression_loss: 1.6208 - classification_loss: 0.5046\n",
            "16/86 [====>.........................] - ETA: 1:07 - loss: 2.1199 - regression_loss: 1.6156 - classification_loss: 0.5043\n",
            "17/86 [====>.........................] - ETA: 1:08 - loss: 2.1148 - regression_loss: 1.6116 - classification_loss: 0.5032\n",
            "18/86 [=====>........................] - ETA: 1:06 - loss: 2.1173 - regression_loss: 1.6107 - classification_loss: 0.5066\n",
            "19/86 [=====>........................] - ETA: 1:06 - loss: 2.1121 - regression_loss: 1.6071 - classification_loss: 0.5051\n",
            "20/86 [=====>........................] - ETA: 1:05 - loss: 2.1161 - regression_loss: 1.6091 - classification_loss: 0.5070\n",
            "21/86 [======>.......................] - ETA: 1:03 - loss: 2.1126 - regression_loss: 1.6057 - classification_loss: 0.5069\n",
            "22/86 [======>.......................] - ETA: 1:02 - loss: 2.1249 - regression_loss: 1.6138 - classification_loss: 0.5111\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.1119 - regression_loss: 1.6037 - classification_loss: 0.5082\n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.1009 - regression_loss: 1.5945 - classification_loss: 0.5064 \n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.0879 - regression_loss: 1.5865 - classification_loss: 0.5014\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.0970 - regression_loss: 1.5920 - classification_loss: 0.5049\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.0921 - regression_loss: 1.5885 - classification_loss: 0.5036\n",
            "28/86 [========>.....................] - ETA: 56s - loss: 2.0953 - regression_loss: 1.5906 - classification_loss: 0.5047\n",
            "29/86 [=========>....................] - ETA: 56s - loss: 2.0931 - regression_loss: 1.5908 - classification_loss: 0.5023\n",
            "30/86 [=========>....................] - ETA: 54s - loss: 2.0881 - regression_loss: 1.5858 - classification_loss: 0.5023\n",
            "31/86 [=========>....................] - ETA: 53s - loss: 2.0952 - regression_loss: 1.5915 - classification_loss: 0.5037\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.1070 - regression_loss: 1.5988 - classification_loss: 0.5082\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.1108 - regression_loss: 1.5993 - classification_loss: 0.5115\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.1052 - regression_loss: 1.5949 - classification_loss: 0.5103\n",
            "35/86 [===========>..................] - ETA: 50s - loss: 2.1056 - regression_loss: 1.5962 - classification_loss: 0.5094\n",
            "36/86 [===========>..................] - ETA: 49s - loss: 2.0989 - regression_loss: 1.5908 - classification_loss: 0.5082\n",
            "37/86 [===========>..................] - ETA: 48s - loss: 2.0942 - regression_loss: 1.5870 - classification_loss: 0.5071\n",
            "38/86 [============>.................] - ETA: 47s - loss: 2.0992 - regression_loss: 1.5912 - classification_loss: 0.5079\n",
            "39/86 [============>.................] - ETA: 46s - loss: 2.0923 - regression_loss: 1.5847 - classification_loss: 0.5076\n",
            "40/86 [============>.................] - ETA: 45s - loss: 2.0927 - regression_loss: 1.5840 - classification_loss: 0.5087\n",
            "41/86 [=============>................] - ETA: 44s - loss: 2.0874 - regression_loss: 1.5794 - classification_loss: 0.5079\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.0856 - regression_loss: 1.5786 - classification_loss: 0.5070\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.0858 - regression_loss: 1.5793 - classification_loss: 0.5065\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.0868 - regression_loss: 1.5807 - classification_loss: 0.5061\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.0874 - regression_loss: 1.5806 - classification_loss: 0.5068\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.0896 - regression_loss: 1.5843 - classification_loss: 0.5053\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.0923 - regression_loss: 1.5863 - classification_loss: 0.5060\n",
            "48/86 [===============>..............] - ETA: 37s - loss: 2.0989 - regression_loss: 1.5899 - classification_loss: 0.5090\n",
            "49/86 [================>.............] - ETA: 36s - loss: 2.0996 - regression_loss: 1.5908 - classification_loss: 0.5088\n",
            "50/86 [================>.............] - ETA: 35s - loss: 2.1018 - regression_loss: 1.5913 - classification_loss: 0.5105\n",
            "51/86 [================>.............] - ETA: 34s - loss: 2.0960 - regression_loss: 1.5857 - classification_loss: 0.5103\n",
            "52/86 [=================>............] - ETA: 33s - loss: 2.0908 - regression_loss: 1.5824 - classification_loss: 0.5084\n",
            "53/86 [=================>............] - ETA: 32s - loss: 2.0891 - regression_loss: 1.5806 - classification_loss: 0.5085\n",
            "54/86 [=================>............] - ETA: 31s - loss: 2.0903 - regression_loss: 1.5809 - classification_loss: 0.5094\n",
            "55/86 [==================>...........] - ETA: 30s - loss: 2.0930 - regression_loss: 1.5822 - classification_loss: 0.5108\n",
            "56/86 [==================>...........] - ETA: 29s - loss: 2.0976 - regression_loss: 1.5853 - classification_loss: 0.5123\n",
            "57/86 [==================>...........] - ETA: 28s - loss: 2.0953 - regression_loss: 1.5844 - classification_loss: 0.5110\n",
            "58/86 [===================>..........] - ETA: 27s - loss: 2.0962 - regression_loss: 1.5856 - classification_loss: 0.5106\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.0963 - regression_loss: 1.5855 - classification_loss: 0.5108\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.0929 - regression_loss: 1.5830 - classification_loss: 0.5099\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.0958 - regression_loss: 1.5861 - classification_loss: 0.5096\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.0952 - regression_loss: 1.5847 - classification_loss: 0.5105\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.0939 - regression_loss: 1.5831 - classification_loss: 0.5108\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.0970 - regression_loss: 1.5859 - classification_loss: 0.5111\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.0890 - regression_loss: 1.5799 - classification_loss: 0.5091\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0880 - regression_loss: 1.5796 - classification_loss: 0.5084\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0901 - regression_loss: 1.5808 - classification_loss: 0.5093\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0924 - regression_loss: 1.5829 - classification_loss: 0.5094\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0933 - regression_loss: 1.5830 - classification_loss: 0.5103\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0935 - regression_loss: 1.5836 - classification_loss: 0.5099\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0944 - regression_loss: 1.5845 - classification_loss: 0.5099\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0961 - regression_loss: 1.5860 - classification_loss: 0.5102\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0962 - regression_loss: 1.5859 - classification_loss: 0.5103\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0953 - regression_loss: 1.5823 - classification_loss: 0.5130\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0941 - regression_loss: 1.5814 - classification_loss: 0.5127\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0970 - regression_loss: 1.5830 - classification_loss: 0.5140 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0933 - regression_loss: 1.5795 - classification_loss: 0.5138\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0910 - regression_loss: 1.5777 - classification_loss: 0.5133\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0898 - regression_loss: 1.5769 - classification_loss: 0.5129\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0952 - regression_loss: 1.5800 - classification_loss: 0.5153\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0968 - regression_loss: 1.5804 - classification_loss: 0.5165\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0958 - regression_loss: 1.5792 - classification_loss: 0.5166\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0985 - regression_loss: 1.5817 - classification_loss: 0.5168\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0977 - regression_loss: 1.5812 - classification_loss: 0.5166\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0977 - regression_loss: 1.5813 - classification_loss: 0.5164\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0976 - regression_loss: 1.5811 - classification_loss: 0.5166\n",
            "Epoch 76: saving model to ./snapshots\\resnet50_csv_76.h5\n",
            "\n",
            "Epoch 76: ReduceLROnPlateau reducing learning rate to 9.999999319067319e-36.\n",
            "\n",
            "86/86 [==============================] - 84s 967ms/step - loss: 2.0976 - regression_loss: 1.5811 - classification_loss: 0.5166 - lr: 1.0000e-34\n",
            "Epoch 77/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:48 - loss: 1.8778 - regression_loss: 1.4207 - classification_loss: 0.4571\n",
            " 2/86 [..............................] - ETA: 1:30 - loss: 2.0123 - regression_loss: 1.5271 - classification_loss: 0.4852\n",
            " 3/86 [>.............................] - ETA: 1:19 - loss: 2.0536 - regression_loss: 1.5598 - classification_loss: 0.4938\n",
            " 4/86 [>.............................] - ETA: 1:15 - loss: 2.1045 - regression_loss: 1.5838 - classification_loss: 0.5207\n",
            " 5/86 [>.............................] - ETA: 1:15 - loss: 2.1304 - regression_loss: 1.6050 - classification_loss: 0.5254\n",
            " 6/86 [=>............................] - ETA: 1:13 - loss: 2.1133 - regression_loss: 1.5962 - classification_loss: 0.5171\n",
            " 7/86 [=>............................] - ETA: 1:12 - loss: 2.1034 - regression_loss: 1.5914 - classification_loss: 0.5120\n",
            " 8/86 [=>............................] - ETA: 1:13 - loss: 2.0977 - regression_loss: 1.5875 - classification_loss: 0.5102\n",
            " 9/86 [==>...........................] - ETA: 1:11 - loss: 2.0769 - regression_loss: 1.5714 - classification_loss: 0.5055\n",
            "10/86 [==>...........................] - ETA: 1:10 - loss: 2.0689 - regression_loss: 1.5667 - classification_loss: 0.5021\n",
            "11/86 [==>...........................] - ETA: 1:11 - loss: 2.0672 - regression_loss: 1.5627 - classification_loss: 0.5045\n",
            "12/86 [===>..........................] - ETA: 1:12 - loss: 2.0589 - regression_loss: 1.5566 - classification_loss: 0.5023\n",
            "13/86 [===>..........................] - ETA: 1:09 - loss: 2.0471 - regression_loss: 1.5455 - classification_loss: 0.5016\n",
            "14/86 [===>..........................] - ETA: 1:08 - loss: 2.0688 - regression_loss: 1.5613 - classification_loss: 0.5075\n",
            "15/86 [====>.........................] - ETA: 1:08 - loss: 2.0779 - regression_loss: 1.5656 - classification_loss: 0.5123\n",
            "16/86 [====>.........................] - ETA: 1:06 - loss: 2.0586 - regression_loss: 1.5523 - classification_loss: 0.5063\n",
            "17/86 [====>.........................] - ETA: 1:05 - loss: 2.0622 - regression_loss: 1.5547 - classification_loss: 0.5075\n",
            "18/86 [=====>........................] - ETA: 1:03 - loss: 2.0686 - regression_loss: 1.5586 - classification_loss: 0.5100\n",
            "19/86 [=====>........................] - ETA: 1:02 - loss: 2.0835 - regression_loss: 1.5694 - classification_loss: 0.5142\n",
            "20/86 [=====>........................] - ETA: 1:01 - loss: 2.0788 - regression_loss: 1.5663 - classification_loss: 0.5125\n",
            "21/86 [======>.......................] - ETA: 1:00 - loss: 2.0833 - regression_loss: 1.5681 - classification_loss: 0.5151\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.0780 - regression_loss: 1.5643 - classification_loss: 0.5138\n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.0795 - regression_loss: 1.5650 - classification_loss: 0.5144 \n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.0811 - regression_loss: 1.5668 - classification_loss: 0.5143\n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.0742 - regression_loss: 1.5628 - classification_loss: 0.5114\n",
            "26/86 [========>.....................] - ETA: 56s - loss: 2.0720 - regression_loss: 1.5625 - classification_loss: 0.5095\n",
            "27/86 [========>.....................] - ETA: 55s - loss: 2.0739 - regression_loss: 1.5624 - classification_loss: 0.5115\n",
            "28/86 [========>.....................] - ETA: 54s - loss: 2.0784 - regression_loss: 1.5648 - classification_loss: 0.5136\n",
            "29/86 [=========>....................] - ETA: 53s - loss: 2.0650 - regression_loss: 1.5556 - classification_loss: 0.5094\n",
            "30/86 [=========>....................] - ETA: 52s - loss: 2.0630 - regression_loss: 1.5549 - classification_loss: 0.5080\n",
            "31/86 [=========>....................] - ETA: 51s - loss: 2.0781 - regression_loss: 1.5665 - classification_loss: 0.5116\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.0731 - regression_loss: 1.5644 - classification_loss: 0.5088\n",
            "33/86 [==========>...................] - ETA: 49s - loss: 2.0702 - regression_loss: 1.5627 - classification_loss: 0.5075\n",
            "34/86 [==========>...................] - ETA: 48s - loss: 2.0721 - regression_loss: 1.5651 - classification_loss: 0.5070\n",
            "35/86 [===========>..................] - ETA: 47s - loss: 2.0672 - regression_loss: 1.5614 - classification_loss: 0.5058\n",
            "36/86 [===========>..................] - ETA: 46s - loss: 2.0591 - regression_loss: 1.5552 - classification_loss: 0.5039\n",
            "37/86 [===========>..................] - ETA: 45s - loss: 2.0546 - regression_loss: 1.5532 - classification_loss: 0.5014\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.0612 - regression_loss: 1.5584 - classification_loss: 0.5028\n",
            "39/86 [============>.................] - ETA: 43s - loss: 2.0604 - regression_loss: 1.5586 - classification_loss: 0.5018\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.0637 - regression_loss: 1.5613 - classification_loss: 0.5024\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.0718 - regression_loss: 1.5665 - classification_loss: 0.5053\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.0658 - regression_loss: 1.5626 - classification_loss: 0.5032\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.0669 - regression_loss: 1.5626 - classification_loss: 0.5043\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.0677 - regression_loss: 1.5634 - classification_loss: 0.5043\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.0679 - regression_loss: 1.5649 - classification_loss: 0.5030\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.0629 - regression_loss: 1.5612 - classification_loss: 0.5017\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.0589 - regression_loss: 1.5587 - classification_loss: 0.5002\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.0570 - regression_loss: 1.5572 - classification_loss: 0.4998\n",
            "49/86 [================>.............] - ETA: 34s - loss: 2.0572 - regression_loss: 1.5576 - classification_loss: 0.4996\n",
            "50/86 [================>.............] - ETA: 33s - loss: 2.0552 - regression_loss: 1.5556 - classification_loss: 0.4997\n",
            "51/86 [================>.............] - ETA: 32s - loss: 2.0534 - regression_loss: 1.5543 - classification_loss: 0.4991\n",
            "52/86 [=================>............] - ETA: 31s - loss: 2.0567 - regression_loss: 1.5577 - classification_loss: 0.4990\n",
            "53/86 [=================>............] - ETA: 30s - loss: 2.0497 - regression_loss: 1.5526 - classification_loss: 0.4971\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0491 - regression_loss: 1.5512 - classification_loss: 0.4978\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0528 - regression_loss: 1.5526 - classification_loss: 0.5002\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0556 - regression_loss: 1.5537 - classification_loss: 0.5019\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0568 - regression_loss: 1.5510 - classification_loss: 0.5058\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0464 - regression_loss: 1.5431 - classification_loss: 0.5033\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0479 - regression_loss: 1.5455 - classification_loss: 0.5024\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0457 - regression_loss: 1.5432 - classification_loss: 0.5025\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0498 - regression_loss: 1.5461 - classification_loss: 0.5037\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.0542 - regression_loss: 1.5493 - classification_loss: 0.5049\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.0552 - regression_loss: 1.5505 - classification_loss: 0.5048\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.0609 - regression_loss: 1.5537 - classification_loss: 0.5072\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.0619 - regression_loss: 1.5547 - classification_loss: 0.5072\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.0607 - regression_loss: 1.5540 - classification_loss: 0.5067\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.0579 - regression_loss: 1.5518 - classification_loss: 0.5061\n",
            "68/86 [======================>.......] - ETA: 16s - loss: 2.0599 - regression_loss: 1.5537 - classification_loss: 0.5063\n",
            "69/86 [=======================>......] - ETA: 15s - loss: 2.0641 - regression_loss: 1.5572 - classification_loss: 0.5069\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0660 - regression_loss: 1.5583 - classification_loss: 0.5077\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0689 - regression_loss: 1.5611 - classification_loss: 0.5078\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0698 - regression_loss: 1.5622 - classification_loss: 0.5076\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0724 - regression_loss: 1.5648 - classification_loss: 0.5076\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0718 - regression_loss: 1.5638 - classification_loss: 0.5079\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0746 - regression_loss: 1.5665 - classification_loss: 0.5082\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0748 - regression_loss: 1.5664 - classification_loss: 0.5085 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0825 - regression_loss: 1.5727 - classification_loss: 0.5098\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0818 - regression_loss: 1.5719 - classification_loss: 0.5099\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0807 - regression_loss: 1.5708 - classification_loss: 0.5099\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0810 - regression_loss: 1.5714 - classification_loss: 0.5096\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0862 - regression_loss: 1.5756 - classification_loss: 0.5106\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0851 - regression_loss: 1.5746 - classification_loss: 0.5105\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0853 - regression_loss: 1.5749 - classification_loss: 0.5104\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0860 - regression_loss: 1.5760 - classification_loss: 0.5099\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0832 - regression_loss: 1.5732 - classification_loss: 0.5100\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0819 - regression_loss: 1.5725 - classification_loss: 0.5095\n",
            "Epoch 77: saving model to ./snapshots\\resnet50_csv_77.h5\n",
            "\n",
            "86/86 [==============================] - 81s 941ms/step - loss: 2.0819 - regression_loss: 1.5725 - classification_loss: 0.5095 - lr: 1.0000e-35\n",
            "Epoch 78/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:58 - loss: 2.3924 - regression_loss: 1.7557 - classification_loss: 0.6367\n",
            " 2/86 [..............................] - ETA: 1:11 - loss: 2.3375 - regression_loss: 1.7411 - classification_loss: 0.5964\n",
            " 3/86 [>.............................] - ETA: 1:17 - loss: 2.3163 - regression_loss: 1.7183 - classification_loss: 0.5980\n",
            " 4/86 [>.............................] - ETA: 1:15 - loss: 2.3027 - regression_loss: 1.7180 - classification_loss: 0.5846\n",
            " 5/86 [>.............................] - ETA: 1:13 - loss: 2.2265 - regression_loss: 1.6667 - classification_loss: 0.5597\n",
            " 6/86 [=>............................] - ETA: 1:15 - loss: 2.2285 - regression_loss: 1.6612 - classification_loss: 0.5673\n",
            " 7/86 [=>............................] - ETA: 1:13 - loss: 2.2139 - regression_loss: 1.6483 - classification_loss: 0.5656\n",
            " 8/86 [=>............................] - ETA: 1:10 - loss: 2.1912 - regression_loss: 1.6378 - classification_loss: 0.5534\n",
            " 9/86 [==>...........................] - ETA: 1:13 - loss: 2.1769 - regression_loss: 1.6352 - classification_loss: 0.5417\n",
            "10/86 [==>...........................] - ETA: 1:12 - loss: 2.1996 - regression_loss: 1.6578 - classification_loss: 0.5419\n",
            "11/86 [==>...........................] - ETA: 1:10 - loss: 2.1670 - regression_loss: 1.6336 - classification_loss: 0.5334\n",
            "12/86 [===>..........................] - ETA: 1:10 - loss: 2.1716 - regression_loss: 1.6291 - classification_loss: 0.5425\n",
            "13/86 [===>..........................] - ETA: 1:08 - loss: 2.1945 - regression_loss: 1.6464 - classification_loss: 0.5481\n",
            "14/86 [===>..........................] - ETA: 1:09 - loss: 2.1876 - regression_loss: 1.6410 - classification_loss: 0.5465\n",
            "15/86 [====>.........................] - ETA: 1:08 - loss: 2.1855 - regression_loss: 1.6410 - classification_loss: 0.5445\n",
            "16/86 [====>.........................] - ETA: 1:07 - loss: 2.1789 - regression_loss: 1.6385 - classification_loss: 0.5404\n",
            "17/86 [====>.........................] - ETA: 1:06 - loss: 2.1711 - regression_loss: 1.6326 - classification_loss: 0.5385\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.1861 - regression_loss: 1.6444 - classification_loss: 0.5417\n",
            "19/86 [=====>........................] - ETA: 1:05 - loss: 2.1883 - regression_loss: 1.6455 - classification_loss: 0.5428\n",
            "20/86 [=====>........................] - ETA: 1:04 - loss: 2.1823 - regression_loss: 1.6415 - classification_loss: 0.5407\n",
            "21/86 [======>.......................] - ETA: 1:03 - loss: 2.1788 - regression_loss: 1.6410 - classification_loss: 0.5378\n",
            "22/86 [======>.......................] - ETA: 1:02 - loss: 2.1779 - regression_loss: 1.6416 - classification_loss: 0.5363\n",
            "23/86 [=======>......................] - ETA: 1:01 - loss: 2.1689 - regression_loss: 1.6349 - classification_loss: 0.5340\n",
            "24/86 [=======>......................] - ETA: 1:00 - loss: 2.1595 - regression_loss: 1.6291 - classification_loss: 0.5305\n",
            "25/86 [=======>......................] - ETA: 59s - loss: 2.1591 - regression_loss: 1.6286 - classification_loss: 0.5305 \n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.1473 - regression_loss: 1.6204 - classification_loss: 0.5269\n",
            "27/86 [========>.....................] - ETA: 57s - loss: 2.1453 - regression_loss: 1.6178 - classification_loss: 0.5275\n",
            "28/86 [========>.....................] - ETA: 56s - loss: 2.1518 - regression_loss: 1.6229 - classification_loss: 0.5289\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.1514 - regression_loss: 1.6208 - classification_loss: 0.5306\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.1511 - regression_loss: 1.6203 - classification_loss: 0.5307\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.1533 - regression_loss: 1.6214 - classification_loss: 0.5320\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.1511 - regression_loss: 1.6216 - classification_loss: 0.5295\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.1375 - regression_loss: 1.6120 - classification_loss: 0.5255\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.1382 - regression_loss: 1.6130 - classification_loss: 0.5252\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.1299 - regression_loss: 1.6074 - classification_loss: 0.5225\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.1304 - regression_loss: 1.6099 - classification_loss: 0.5205\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.1236 - regression_loss: 1.6044 - classification_loss: 0.5193\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.1252 - regression_loss: 1.6055 - classification_loss: 0.5196\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.1264 - regression_loss: 1.6051 - classification_loss: 0.5213\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.1255 - regression_loss: 1.6048 - classification_loss: 0.5207\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.1227 - regression_loss: 1.6016 - classification_loss: 0.5211\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.1203 - regression_loss: 1.5998 - classification_loss: 0.5204\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.1212 - regression_loss: 1.6009 - classification_loss: 0.5204\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.1165 - regression_loss: 1.5959 - classification_loss: 0.5206\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.1168 - regression_loss: 1.5969 - classification_loss: 0.5199\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.1129 - regression_loss: 1.5939 - classification_loss: 0.5191\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.1155 - regression_loss: 1.5964 - classification_loss: 0.5191\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1169 - regression_loss: 1.5980 - classification_loss: 0.5189\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1158 - regression_loss: 1.5977 - classification_loss: 0.5181\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1120 - regression_loss: 1.5948 - classification_loss: 0.5172\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1074 - regression_loss: 1.5903 - classification_loss: 0.5171\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1055 - regression_loss: 1.5886 - classification_loss: 0.5169\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1034 - regression_loss: 1.5864 - classification_loss: 0.5170\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0974 - regression_loss: 1.5825 - classification_loss: 0.5149\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0935 - regression_loss: 1.5800 - classification_loss: 0.5135\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0934 - regression_loss: 1.5802 - classification_loss: 0.5132\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0928 - regression_loss: 1.5796 - classification_loss: 0.5133\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0900 - regression_loss: 1.5771 - classification_loss: 0.5129\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0924 - regression_loss: 1.5799 - classification_loss: 0.5126\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0877 - regression_loss: 1.5756 - classification_loss: 0.5121\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0854 - regression_loss: 1.5734 - classification_loss: 0.5120\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.0843 - regression_loss: 1.5719 - classification_loss: 0.5124\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.0855 - regression_loss: 1.5726 - classification_loss: 0.5130\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.0846 - regression_loss: 1.5720 - classification_loss: 0.5126\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.0860 - regression_loss: 1.5725 - classification_loss: 0.5135\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.0847 - regression_loss: 1.5716 - classification_loss: 0.5130\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.0832 - regression_loss: 1.5720 - classification_loss: 0.5112\n",
            "68/86 [======================>.......] - ETA: 16s - loss: 2.0829 - regression_loss: 1.5719 - classification_loss: 0.5110\n",
            "69/86 [=======================>......] - ETA: 15s - loss: 2.0901 - regression_loss: 1.5777 - classification_loss: 0.5124\n",
            "70/86 [=======================>......] - ETA: 14s - loss: 2.0916 - regression_loss: 1.5787 - classification_loss: 0.5129\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0894 - regression_loss: 1.5774 - classification_loss: 0.5120\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0893 - regression_loss: 1.5770 - classification_loss: 0.5123\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0898 - regression_loss: 1.5780 - classification_loss: 0.5118\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0930 - regression_loss: 1.5816 - classification_loss: 0.5115\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0911 - regression_loss: 1.5803 - classification_loss: 0.5108\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0939 - regression_loss: 1.5821 - classification_loss: 0.5118 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0939 - regression_loss: 1.5822 - classification_loss: 0.5116\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0949 - regression_loss: 1.5841 - classification_loss: 0.5108\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0916 - regression_loss: 1.5817 - classification_loss: 0.5099\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0917 - regression_loss: 1.5821 - classification_loss: 0.5096\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0886 - regression_loss: 1.5796 - classification_loss: 0.5090\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0884 - regression_loss: 1.5800 - classification_loss: 0.5084\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0838 - regression_loss: 1.5765 - classification_loss: 0.5074\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0856 - regression_loss: 1.5780 - classification_loss: 0.5077\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0879 - regression_loss: 1.5805 - classification_loss: 0.5075\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0817 - regression_loss: 1.5760 - classification_loss: 0.5057\n",
            "Epoch 78: saving model to ./snapshots\\resnet50_csv_78.h5\n",
            "\n",
            "Epoch 78: ReduceLROnPlateau reducing learning rate to 9.999999462560281e-37.\n",
            "\n",
            "86/86 [==============================] - 82s 944ms/step - loss: 2.0817 - regression_loss: 1.5760 - classification_loss: 0.5057 - lr: 1.0000e-35\n",
            "Epoch 79/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:43 - loss: 2.5394 - regression_loss: 1.9478 - classification_loss: 0.5916\n",
            " 2/86 [..............................] - ETA: 1:33 - loss: 2.2920 - regression_loss: 1.7265 - classification_loss: 0.5654\n",
            " 3/86 [>.............................] - ETA: 1:26 - loss: 2.0382 - regression_loss: 1.5363 - classification_loss: 0.5019\n",
            " 4/86 [>.............................] - ETA: 1:19 - loss: 2.0288 - regression_loss: 1.5430 - classification_loss: 0.4858\n",
            " 5/86 [>.............................] - ETA: 1:17 - loss: 2.0568 - regression_loss: 1.5692 - classification_loss: 0.4876\n",
            " 6/86 [=>............................] - ETA: 1:18 - loss: 2.1094 - regression_loss: 1.6115 - classification_loss: 0.4979\n",
            " 7/86 [=>............................] - ETA: 1:15 - loss: 2.0751 - regression_loss: 1.5788 - classification_loss: 0.4963\n",
            " 8/86 [=>............................] - ETA: 1:16 - loss: 2.1032 - regression_loss: 1.5926 - classification_loss: 0.5106\n",
            " 9/86 [==>...........................] - ETA: 1:14 - loss: 2.0967 - regression_loss: 1.5937 - classification_loss: 0.5030\n",
            "10/86 [==>...........................] - ETA: 1:12 - loss: 2.0904 - regression_loss: 1.5882 - classification_loss: 0.5022\n",
            "11/86 [==>...........................] - ETA: 1:13 - loss: 2.0946 - regression_loss: 1.5928 - classification_loss: 0.5018\n",
            "12/86 [===>..........................] - ETA: 1:13 - loss: 2.0962 - regression_loss: 1.5985 - classification_loss: 0.4977\n",
            "13/86 [===>..........................] - ETA: 1:10 - loss: 2.1059 - regression_loss: 1.6088 - classification_loss: 0.4971\n",
            "14/86 [===>..........................] - ETA: 1:13 - loss: 2.1228 - regression_loss: 1.6239 - classification_loss: 0.4989\n",
            "15/86 [====>.........................] - ETA: 1:10 - loss: 2.1198 - regression_loss: 1.6212 - classification_loss: 0.4986\n",
            "16/86 [====>.........................] - ETA: 1:09 - loss: 2.1183 - regression_loss: 1.6197 - classification_loss: 0.4986\n",
            "17/86 [====>.........................] - ETA: 1:07 - loss: 2.1111 - regression_loss: 1.6107 - classification_loss: 0.5004\n",
            "18/86 [=====>........................] - ETA: 1:06 - loss: 2.1166 - regression_loss: 1.6127 - classification_loss: 0.5039\n",
            "19/86 [=====>........................] - ETA: 1:05 - loss: 2.1270 - regression_loss: 1.6239 - classification_loss: 0.5031\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.1303 - regression_loss: 1.6260 - classification_loss: 0.5043\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.1285 - regression_loss: 1.6226 - classification_loss: 0.5058\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.1346 - regression_loss: 1.6278 - classification_loss: 0.5068\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.1227 - regression_loss: 1.6155 - classification_loss: 0.5071\n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.1344 - regression_loss: 1.6236 - classification_loss: 0.5108 \n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.1250 - regression_loss: 1.6152 - classification_loss: 0.5098\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.1376 - regression_loss: 1.6217 - classification_loss: 0.5159\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.1354 - regression_loss: 1.6214 - classification_loss: 0.5139\n",
            "28/86 [========>.....................] - ETA: 54s - loss: 2.1365 - regression_loss: 1.6217 - classification_loss: 0.5147\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.1328 - regression_loss: 1.6188 - classification_loss: 0.5139\n",
            "30/86 [=========>....................] - ETA: 52s - loss: 2.1308 - regression_loss: 1.6177 - classification_loss: 0.5131\n",
            "31/86 [=========>....................] - ETA: 51s - loss: 2.1264 - regression_loss: 1.6152 - classification_loss: 0.5111\n",
            "32/86 [==========>...................] - ETA: 50s - loss: 2.1124 - regression_loss: 1.6045 - classification_loss: 0.5079\n",
            "33/86 [==========>...................] - ETA: 49s - loss: 2.1106 - regression_loss: 1.6026 - classification_loss: 0.5080\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.1119 - regression_loss: 1.6035 - classification_loss: 0.5084\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.1098 - regression_loss: 1.6007 - classification_loss: 0.5091\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.1066 - regression_loss: 1.5979 - classification_loss: 0.5087\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.1079 - regression_loss: 1.5994 - classification_loss: 0.5085\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.1105 - regression_loss: 1.6024 - classification_loss: 0.5081\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.1103 - regression_loss: 1.6000 - classification_loss: 0.5103\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.1099 - regression_loss: 1.5998 - classification_loss: 0.5101\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.1072 - regression_loss: 1.5970 - classification_loss: 0.5101\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.1072 - regression_loss: 1.5964 - classification_loss: 0.5108\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.1051 - regression_loss: 1.5958 - classification_loss: 0.5092\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.1004 - regression_loss: 1.5921 - classification_loss: 0.5083\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.1078 - regression_loss: 1.5978 - classification_loss: 0.5100\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.1083 - regression_loss: 1.5976 - classification_loss: 0.5107\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.1042 - regression_loss: 1.5942 - classification_loss: 0.5100\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.1023 - regression_loss: 1.5902 - classification_loss: 0.5121\n",
            "49/86 [================>.............] - ETA: 34s - loss: 2.1050 - regression_loss: 1.5930 - classification_loss: 0.5120\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1036 - regression_loss: 1.5923 - classification_loss: 0.5113\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1113 - regression_loss: 1.5985 - classification_loss: 0.5129\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1080 - regression_loss: 1.5952 - classification_loss: 0.5128\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1070 - regression_loss: 1.5941 - classification_loss: 0.5129\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1103 - regression_loss: 1.5967 - classification_loss: 0.5137\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1075 - regression_loss: 1.5944 - classification_loss: 0.5130\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1080 - regression_loss: 1.5944 - classification_loss: 0.5136\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1086 - regression_loss: 1.5953 - classification_loss: 0.5133\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1104 - regression_loss: 1.5978 - classification_loss: 0.5126\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1083 - regression_loss: 1.5963 - classification_loss: 0.5119\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.1165 - regression_loss: 1.6028 - classification_loss: 0.5137\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1203 - regression_loss: 1.6053 - classification_loss: 0.5150\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1251 - regression_loss: 1.6089 - classification_loss: 0.5162\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1225 - regression_loss: 1.6075 - classification_loss: 0.5150\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1239 - regression_loss: 1.6083 - classification_loss: 0.5156\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1267 - regression_loss: 1.6106 - classification_loss: 0.5161\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1294 - regression_loss: 1.6130 - classification_loss: 0.5164\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1329 - regression_loss: 1.6161 - classification_loss: 0.5167\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1360 - regression_loss: 1.6192 - classification_loss: 0.5168\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1362 - regression_loss: 1.6199 - classification_loss: 0.5163\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1364 - regression_loss: 1.6195 - classification_loss: 0.5169\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1345 - regression_loss: 1.6184 - classification_loss: 0.5161\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1302 - regression_loss: 1.6150 - classification_loss: 0.5152\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1295 - regression_loss: 1.6143 - classification_loss: 0.5151\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1316 - regression_loss: 1.6156 - classification_loss: 0.5160\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1323 - regression_loss: 1.6156 - classification_loss: 0.5167\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1340 - regression_loss: 1.6164 - classification_loss: 0.5175 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1329 - regression_loss: 1.6157 - classification_loss: 0.5173\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1342 - regression_loss: 1.6165 - classification_loss: 0.5177\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1310 - regression_loss: 1.6143 - classification_loss: 0.5167\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1319 - regression_loss: 1.6153 - classification_loss: 0.5166\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1294 - regression_loss: 1.6136 - classification_loss: 0.5158\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1258 - regression_loss: 1.6109 - classification_loss: 0.5149\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1235 - regression_loss: 1.6098 - classification_loss: 0.5137\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1230 - regression_loss: 1.6095 - classification_loss: 0.5135\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1159 - regression_loss: 1.6044 - classification_loss: 0.5115\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1143 - regression_loss: 1.6033 - classification_loss: 0.5110\n",
            "Epoch 79: saving model to ./snapshots\\resnet50_csv_79.h5\n",
            "\n",
            "86/86 [==============================] - 82s 954ms/step - loss: 2.1143 - regression_loss: 1.6033 - classification_loss: 0.5110 - lr: 1.0000e-36\n",
            "Epoch 80/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:52 - loss: 2.1666 - regression_loss: 1.5846 - classification_loss: 0.5820\n",
            " 2/86 [..............................] - ETA: 1:19 - loss: 2.0831 - regression_loss: 1.5749 - classification_loss: 0.5082\n",
            " 3/86 [>.............................] - ETA: 1:17 - loss: 2.0488 - regression_loss: 1.5590 - classification_loss: 0.4898\n",
            " 4/86 [>.............................] - ETA: 1:17 - loss: 2.0670 - regression_loss: 1.5661 - classification_loss: 0.5009\n",
            " 5/86 [>.............................] - ETA: 1:14 - loss: 2.0830 - regression_loss: 1.5626 - classification_loss: 0.5204\n",
            " 6/86 [=>............................] - ETA: 1:11 - loss: 2.1456 - regression_loss: 1.6127 - classification_loss: 0.5328\n",
            " 7/86 [=>............................] - ETA: 1:12 - loss: 2.1288 - regression_loss: 1.5987 - classification_loss: 0.5301\n",
            " 8/86 [=>............................] - ETA: 1:10 - loss: 2.1283 - regression_loss: 1.5983 - classification_loss: 0.5300\n",
            " 9/86 [==>...........................] - ETA: 1:09 - loss: 2.1271 - regression_loss: 1.5975 - classification_loss: 0.5296\n",
            "10/86 [==>...........................] - ETA: 1:07 - loss: 2.1247 - regression_loss: 1.5952 - classification_loss: 0.5294\n",
            "11/86 [==>...........................] - ETA: 1:06 - loss: 2.0893 - regression_loss: 1.5734 - classification_loss: 0.5160\n",
            "12/86 [===>..........................] - ETA: 1:05 - loss: 2.0791 - regression_loss: 1.5634 - classification_loss: 0.5158\n",
            "13/86 [===>..........................] - ETA: 1:04 - loss: 2.0778 - regression_loss: 1.5640 - classification_loss: 0.5138\n",
            "14/86 [===>..........................] - ETA: 1:04 - loss: 2.0802 - regression_loss: 1.5664 - classification_loss: 0.5138\n",
            "15/86 [====>.........................] - ETA: 1:03 - loss: 2.1017 - regression_loss: 1.5842 - classification_loss: 0.5175\n",
            "16/86 [====>.........................] - ETA: 1:02 - loss: 2.0882 - regression_loss: 1.5749 - classification_loss: 0.5133\n",
            "17/86 [====>.........................] - ETA: 1:02 - loss: 2.0891 - regression_loss: 1.5741 - classification_loss: 0.5150\n",
            "18/86 [=====>........................] - ETA: 1:01 - loss: 2.0731 - regression_loss: 1.5652 - classification_loss: 0.5079\n",
            "19/86 [=====>........................] - ETA: 1:00 - loss: 2.0692 - regression_loss: 1.5654 - classification_loss: 0.5037\n",
            "20/86 [=====>........................] - ETA: 1:00 - loss: 2.0801 - regression_loss: 1.5721 - classification_loss: 0.5080\n",
            "21/86 [======>.......................] - ETA: 1:01 - loss: 2.0787 - regression_loss: 1.5714 - classification_loss: 0.5073\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.0882 - regression_loss: 1.5809 - classification_loss: 0.5074\n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.1037 - regression_loss: 1.5930 - classification_loss: 0.5107 \n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.0923 - regression_loss: 1.5809 - classification_loss: 0.5114\n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.0975 - regression_loss: 1.5859 - classification_loss: 0.5116\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.0946 - regression_loss: 1.5839 - classification_loss: 0.5107\n",
            "27/86 [========>.....................] - ETA: 55s - loss: 2.0937 - regression_loss: 1.5850 - classification_loss: 0.5087\n",
            "28/86 [========>.....................] - ETA: 54s - loss: 2.0823 - regression_loss: 1.5774 - classification_loss: 0.5049\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.0889 - regression_loss: 1.5805 - classification_loss: 0.5084\n",
            "30/86 [=========>....................] - ETA: 52s - loss: 2.0838 - regression_loss: 1.5754 - classification_loss: 0.5083\n",
            "31/86 [=========>....................] - ETA: 51s - loss: 2.0795 - regression_loss: 1.5713 - classification_loss: 0.5081\n",
            "32/86 [==========>...................] - ETA: 50s - loss: 2.0786 - regression_loss: 1.5706 - classification_loss: 0.5079\n",
            "33/86 [==========>...................] - ETA: 49s - loss: 2.0774 - regression_loss: 1.5699 - classification_loss: 0.5075\n",
            "34/86 [==========>...................] - ETA: 48s - loss: 2.0716 - regression_loss: 1.5658 - classification_loss: 0.5058\n",
            "35/86 [===========>..................] - ETA: 47s - loss: 2.0689 - regression_loss: 1.5636 - classification_loss: 0.5054\n",
            "36/86 [===========>..................] - ETA: 46s - loss: 2.0640 - regression_loss: 1.5588 - classification_loss: 0.5052\n",
            "37/86 [===========>..................] - ETA: 45s - loss: 2.0617 - regression_loss: 1.5574 - classification_loss: 0.5043\n",
            "38/86 [============>.................] - ETA: 44s - loss: 2.0589 - regression_loss: 1.5534 - classification_loss: 0.5055\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.0612 - regression_loss: 1.5551 - classification_loss: 0.5061\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.0668 - regression_loss: 1.5590 - classification_loss: 0.5078\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.0754 - regression_loss: 1.5666 - classification_loss: 0.5088\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.0715 - regression_loss: 1.5640 - classification_loss: 0.5075\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.0761 - regression_loss: 1.5680 - classification_loss: 0.5081\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.0860 - regression_loss: 1.5758 - classification_loss: 0.5101\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.0838 - regression_loss: 1.5746 - classification_loss: 0.5092\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.0837 - regression_loss: 1.5755 - classification_loss: 0.5082\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.0780 - regression_loss: 1.5705 - classification_loss: 0.5075\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.0762 - regression_loss: 1.5686 - classification_loss: 0.5076\n",
            "49/86 [================>.............] - ETA: 34s - loss: 2.0801 - regression_loss: 1.5711 - classification_loss: 0.5090\n",
            "50/86 [================>.............] - ETA: 33s - loss: 2.0786 - regression_loss: 1.5701 - classification_loss: 0.5086\n",
            "51/86 [================>.............] - ETA: 32s - loss: 2.0741 - regression_loss: 1.5671 - classification_loss: 0.5070\n",
            "52/86 [=================>............] - ETA: 31s - loss: 2.0774 - regression_loss: 1.5694 - classification_loss: 0.5080\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0777 - regression_loss: 1.5696 - classification_loss: 0.5080\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0791 - regression_loss: 1.5712 - classification_loss: 0.5078\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0832 - regression_loss: 1.5761 - classification_loss: 0.5071\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0856 - regression_loss: 1.5768 - classification_loss: 0.5088\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0902 - regression_loss: 1.5810 - classification_loss: 0.5093\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0848 - regression_loss: 1.5772 - classification_loss: 0.5076\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0862 - regression_loss: 1.5783 - classification_loss: 0.5079\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0876 - regression_loss: 1.5780 - classification_loss: 0.5096\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0877 - regression_loss: 1.5789 - classification_loss: 0.5088\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.0904 - regression_loss: 1.5806 - classification_loss: 0.5098\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.0895 - regression_loss: 1.5793 - classification_loss: 0.5102\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.0901 - regression_loss: 1.5797 - classification_loss: 0.5103\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.0935 - regression_loss: 1.5809 - classification_loss: 0.5126\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.0900 - regression_loss: 1.5780 - classification_loss: 0.5119\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.0870 - regression_loss: 1.5764 - classification_loss: 0.5106\n",
            "68/86 [======================>.......] - ETA: 16s - loss: 2.0855 - regression_loss: 1.5747 - classification_loss: 0.5108\n",
            "69/86 [=======================>......] - ETA: 15s - loss: 2.0908 - regression_loss: 1.5790 - classification_loss: 0.5118\n",
            "70/86 [=======================>......] - ETA: 14s - loss: 2.0896 - regression_loss: 1.5782 - classification_loss: 0.5114\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0930 - regression_loss: 1.5796 - classification_loss: 0.5133\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0925 - regression_loss: 1.5794 - classification_loss: 0.5131\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0917 - regression_loss: 1.5772 - classification_loss: 0.5145\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0872 - regression_loss: 1.5741 - classification_loss: 0.5132\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0853 - regression_loss: 1.5733 - classification_loss: 0.5120\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0888 - regression_loss: 1.5771 - classification_loss: 0.5117 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0881 - regression_loss: 1.5763 - classification_loss: 0.5118\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0862 - regression_loss: 1.5751 - classification_loss: 0.5111\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0872 - regression_loss: 1.5759 - classification_loss: 0.5113\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0886 - regression_loss: 1.5761 - classification_loss: 0.5125\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0913 - regression_loss: 1.5785 - classification_loss: 0.5127\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0874 - regression_loss: 1.5753 - classification_loss: 0.5120\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0888 - regression_loss: 1.5761 - classification_loss: 0.5127\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0885 - regression_loss: 1.5763 - classification_loss: 0.5122\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0849 - regression_loss: 1.5741 - classification_loss: 0.5108\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0840 - regression_loss: 1.5735 - classification_loss: 0.5104\n",
            "Epoch 80: saving model to ./snapshots\\resnet50_csv_80.h5\n",
            "\n",
            "Epoch 80: ReduceLROnPlateau reducing learning rate to 9.99999946256028e-38.\n",
            "\n",
            "86/86 [==============================] - 82s 946ms/step - loss: 2.0840 - regression_loss: 1.5735 - classification_loss: 0.5104 - lr: 1.0000e-36\n",
            "Epoch 81/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:51 - loss: 1.8704 - regression_loss: 1.4149 - classification_loss: 0.4555\n",
            " 2/86 [..............................] - ETA: 1:16 - loss: 2.0118 - regression_loss: 1.5206 - classification_loss: 0.4913\n",
            " 3/86 [>.............................] - ETA: 1:18 - loss: 2.0954 - regression_loss: 1.6029 - classification_loss: 0.4925\n",
            " 4/86 [>.............................] - ETA: 1:15 - loss: 2.1418 - regression_loss: 1.6338 - classification_loss: 0.5080\n",
            " 5/86 [>.............................] - ETA: 1:13 - loss: 2.1545 - regression_loss: 1.6400 - classification_loss: 0.5145\n",
            " 6/86 [=>............................] - ETA: 1:12 - loss: 2.1525 - regression_loss: 1.6404 - classification_loss: 0.5122\n",
            " 7/86 [=>............................] - ETA: 1:10 - loss: 2.1254 - regression_loss: 1.6163 - classification_loss: 0.5091\n",
            " 8/86 [=>............................] - ETA: 1:11 - loss: 2.1411 - regression_loss: 1.6228 - classification_loss: 0.5183\n",
            " 9/86 [==>...........................] - ETA: 1:10 - loss: 2.1367 - regression_loss: 1.6171 - classification_loss: 0.5196\n",
            "10/86 [==>...........................] - ETA: 1:09 - loss: 2.1146 - regression_loss: 1.6019 - classification_loss: 0.5127\n",
            "11/86 [==>...........................] - ETA: 1:11 - loss: 2.1152 - regression_loss: 1.5995 - classification_loss: 0.5157\n",
            "12/86 [===>..........................] - ETA: 1:12 - loss: 2.1144 - regression_loss: 1.5996 - classification_loss: 0.5148\n",
            "13/86 [===>..........................] - ETA: 1:10 - loss: 2.1157 - regression_loss: 1.6002 - classification_loss: 0.5155\n",
            "14/86 [===>..........................] - ETA: 1:11 - loss: 2.1060 - regression_loss: 1.5929 - classification_loss: 0.5131\n",
            "15/86 [====>.........................] - ETA: 1:09 - loss: 2.1058 - regression_loss: 1.5945 - classification_loss: 0.5113\n",
            "16/86 [====>.........................] - ETA: 1:08 - loss: 2.1001 - regression_loss: 1.5907 - classification_loss: 0.5094\n",
            "17/86 [====>.........................] - ETA: 1:07 - loss: 2.0949 - regression_loss: 1.5895 - classification_loss: 0.5053\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.0744 - regression_loss: 1.5758 - classification_loss: 0.4986\n",
            "19/86 [=====>........................] - ETA: 1:04 - loss: 2.0891 - regression_loss: 1.5821 - classification_loss: 0.5071\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.0714 - regression_loss: 1.5703 - classification_loss: 0.5011\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.0465 - regression_loss: 1.5518 - classification_loss: 0.4947\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.0585 - regression_loss: 1.5571 - classification_loss: 0.5015\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.0704 - regression_loss: 1.5647 - classification_loss: 0.5058\n",
            "24/86 [=======>......................] - ETA: 1:00 - loss: 2.0731 - regression_loss: 1.5687 - classification_loss: 0.5044\n",
            "25/86 [=======>......................] - ETA: 59s - loss: 2.0715 - regression_loss: 1.5680 - classification_loss: 0.5035 \n",
            "26/86 [========>.....................] - ETA: 58s - loss: 2.0693 - regression_loss: 1.5669 - classification_loss: 0.5024\n",
            "27/86 [========>.....................] - ETA: 57s - loss: 2.0643 - regression_loss: 1.5631 - classification_loss: 0.5013\n",
            "28/86 [========>.....................] - ETA: 56s - loss: 2.0689 - regression_loss: 1.5667 - classification_loss: 0.5023\n",
            "29/86 [=========>....................] - ETA: 55s - loss: 2.0658 - regression_loss: 1.5637 - classification_loss: 0.5021\n",
            "30/86 [=========>....................] - ETA: 55s - loss: 2.0645 - regression_loss: 1.5640 - classification_loss: 0.5005\n",
            "31/86 [=========>....................] - ETA: 53s - loss: 2.0669 - regression_loss: 1.5645 - classification_loss: 0.5025\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.0638 - regression_loss: 1.5624 - classification_loss: 0.5015\n",
            "33/86 [==========>...................] - ETA: 52s - loss: 2.0720 - regression_loss: 1.5687 - classification_loss: 0.5033\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.0694 - regression_loss: 1.5652 - classification_loss: 0.5043\n",
            "35/86 [===========>..................] - ETA: 50s - loss: 2.0741 - regression_loss: 1.5693 - classification_loss: 0.5049\n",
            "36/86 [===========>..................] - ETA: 49s - loss: 2.0827 - regression_loss: 1.5750 - classification_loss: 0.5077\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.0741 - regression_loss: 1.5681 - classification_loss: 0.5060\n",
            "38/86 [============>.................] - ETA: 47s - loss: 2.0765 - regression_loss: 1.5703 - classification_loss: 0.5063\n",
            "39/86 [============>.................] - ETA: 46s - loss: 2.0705 - regression_loss: 1.5664 - classification_loss: 0.5041\n",
            "40/86 [============>.................] - ETA: 45s - loss: 2.0666 - regression_loss: 1.5619 - classification_loss: 0.5047\n",
            "41/86 [=============>................] - ETA: 44s - loss: 2.0698 - regression_loss: 1.5646 - classification_loss: 0.5052\n",
            "42/86 [=============>................] - ETA: 43s - loss: 2.0751 - regression_loss: 1.5683 - classification_loss: 0.5068\n",
            "43/86 [==============>...............] - ETA: 42s - loss: 2.0774 - regression_loss: 1.5723 - classification_loss: 0.5051\n",
            "44/86 [==============>...............] - ETA: 41s - loss: 2.0818 - regression_loss: 1.5776 - classification_loss: 0.5042\n",
            "45/86 [==============>...............] - ETA: 40s - loss: 2.0691 - regression_loss: 1.5684 - classification_loss: 0.5006\n",
            "46/86 [===============>..............] - ETA: 39s - loss: 2.0643 - regression_loss: 1.5652 - classification_loss: 0.4991\n",
            "47/86 [===============>..............] - ETA: 38s - loss: 2.0652 - regression_loss: 1.5669 - classification_loss: 0.4982\n",
            "48/86 [===============>..............] - ETA: 37s - loss: 2.0664 - regression_loss: 1.5679 - classification_loss: 0.4985\n",
            "49/86 [================>.............] - ETA: 36s - loss: 2.0735 - regression_loss: 1.5723 - classification_loss: 0.5012\n",
            "50/86 [================>.............] - ETA: 35s - loss: 2.0749 - regression_loss: 1.5736 - classification_loss: 0.5013\n",
            "51/86 [================>.............] - ETA: 34s - loss: 2.0752 - regression_loss: 1.5741 - classification_loss: 0.5011\n",
            "52/86 [=================>............] - ETA: 33s - loss: 2.0725 - regression_loss: 1.5716 - classification_loss: 0.5009\n",
            "53/86 [=================>............] - ETA: 32s - loss: 2.0723 - regression_loss: 1.5711 - classification_loss: 0.5012\n",
            "54/86 [=================>............] - ETA: 31s - loss: 2.0748 - regression_loss: 1.5732 - classification_loss: 0.5015\n",
            "55/86 [==================>...........] - ETA: 30s - loss: 2.0799 - regression_loss: 1.5769 - classification_loss: 0.5030\n",
            "56/86 [==================>...........] - ETA: 29s - loss: 2.0728 - regression_loss: 1.5714 - classification_loss: 0.5014\n",
            "57/86 [==================>...........] - ETA: 28s - loss: 2.0783 - regression_loss: 1.5763 - classification_loss: 0.5020\n",
            "58/86 [===================>..........] - ETA: 27s - loss: 2.0809 - regression_loss: 1.5778 - classification_loss: 0.5031\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.0824 - regression_loss: 1.5775 - classification_loss: 0.5050\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.0850 - regression_loss: 1.5801 - classification_loss: 0.5050\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.0810 - regression_loss: 1.5777 - classification_loss: 0.5033\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.0820 - regression_loss: 1.5792 - classification_loss: 0.5028\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.0829 - regression_loss: 1.5767 - classification_loss: 0.5062\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.0782 - regression_loss: 1.5726 - classification_loss: 0.5056\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.0864 - regression_loss: 1.5791 - classification_loss: 0.5073\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0861 - regression_loss: 1.5793 - classification_loss: 0.5068\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0895 - regression_loss: 1.5822 - classification_loss: 0.5074\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0880 - regression_loss: 1.5811 - classification_loss: 0.5070\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0880 - regression_loss: 1.5809 - classification_loss: 0.5070\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0882 - regression_loss: 1.5814 - classification_loss: 0.5069\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0868 - regression_loss: 1.5795 - classification_loss: 0.5072\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0855 - regression_loss: 1.5791 - classification_loss: 0.5064\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0863 - regression_loss: 1.5802 - classification_loss: 0.5061\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0858 - regression_loss: 1.5809 - classification_loss: 0.5049\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0902 - regression_loss: 1.5836 - classification_loss: 0.5067\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0905 - regression_loss: 1.5840 - classification_loss: 0.5065 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0934 - regression_loss: 1.5866 - classification_loss: 0.5068\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0974 - regression_loss: 1.5902 - classification_loss: 0.5071\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0963 - regression_loss: 1.5890 - classification_loss: 0.5073\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0973 - regression_loss: 1.5900 - classification_loss: 0.5072\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0943 - regression_loss: 1.5875 - classification_loss: 0.5068\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0963 - regression_loss: 1.5887 - classification_loss: 0.5076\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0995 - regression_loss: 1.5922 - classification_loss: 0.5072\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0981 - regression_loss: 1.5910 - classification_loss: 0.5071\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0990 - regression_loss: 1.5923 - classification_loss: 0.5066\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1036 - regression_loss: 1.5960 - classification_loss: 0.5076\n",
            "Epoch 81: saving model to ./snapshots\\resnet50_csv_81.h5\n",
            "\n",
            "86/86 [==============================] - 84s 973ms/step - loss: 2.1036 - regression_loss: 1.5960 - classification_loss: 0.5076 - lr: 1.0000e-37\n",
            "Epoch 82/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:49 - loss: 2.2316 - regression_loss: 1.6744 - classification_loss: 0.5573\n",
            " 2/86 [..............................] - ETA: 1:18 - loss: 2.0105 - regression_loss: 1.4841 - classification_loss: 0.5263\n",
            " 3/86 [>.............................] - ETA: 1:11 - loss: 1.9831 - regression_loss: 1.4815 - classification_loss: 0.5016\n",
            " 4/86 [>.............................] - ETA: 1:17 - loss: 2.0090 - regression_loss: 1.4958 - classification_loss: 0.5133\n",
            " 5/86 [>.............................] - ETA: 1:24 - loss: 2.0297 - regression_loss: 1.5217 - classification_loss: 0.5080\n",
            " 6/86 [=>............................] - ETA: 1:20 - loss: 2.0241 - regression_loss: 1.5180 - classification_loss: 0.5061\n",
            " 7/86 [=>............................] - ETA: 1:17 - loss: 2.0449 - regression_loss: 1.5388 - classification_loss: 0.5061\n",
            " 8/86 [=>............................] - ETA: 1:16 - loss: 2.0611 - regression_loss: 1.5565 - classification_loss: 0.5046\n",
            " 9/86 [==>...........................] - ETA: 1:17 - loss: 2.0677 - regression_loss: 1.5617 - classification_loss: 0.5060\n",
            "10/86 [==>...........................] - ETA: 1:14 - loss: 2.0812 - regression_loss: 1.5701 - classification_loss: 0.5111\n",
            "11/86 [==>...........................] - ETA: 1:12 - loss: 2.0853 - regression_loss: 1.5755 - classification_loss: 0.5098\n",
            "12/86 [===>..........................] - ETA: 1:12 - loss: 2.1032 - regression_loss: 1.5977 - classification_loss: 0.5055\n",
            "13/86 [===>..........................] - ETA: 1:10 - loss: 2.1167 - regression_loss: 1.6113 - classification_loss: 0.5054\n",
            "14/86 [===>..........................] - ETA: 1:10 - loss: 2.1234 - regression_loss: 1.6178 - classification_loss: 0.5057\n",
            "15/86 [====>.........................] - ETA: 1:08 - loss: 2.0943 - regression_loss: 1.5938 - classification_loss: 0.5005\n",
            "16/86 [====>.........................] - ETA: 1:08 - loss: 2.0982 - regression_loss: 1.5946 - classification_loss: 0.5036\n",
            "17/86 [====>.........................] - ETA: 1:06 - loss: 2.0969 - regression_loss: 1.5943 - classification_loss: 0.5026\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.0917 - regression_loss: 1.5890 - classification_loss: 0.5027\n",
            "19/86 [=====>........................] - ETA: 1:05 - loss: 2.1030 - regression_loss: 1.5943 - classification_loss: 0.5088\n",
            "20/86 [=====>........................] - ETA: 1:02 - loss: 2.1155 - regression_loss: 1.6069 - classification_loss: 0.5086\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.1100 - regression_loss: 1.6041 - classification_loss: 0.5059\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.1017 - regression_loss: 1.5980 - classification_loss: 0.5037\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.1039 - regression_loss: 1.5997 - classification_loss: 0.5042\n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.0992 - regression_loss: 1.5965 - classification_loss: 0.5027 \n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.0950 - regression_loss: 1.5933 - classification_loss: 0.5017\n",
            "26/86 [========>.....................] - ETA: 58s - loss: 2.0969 - regression_loss: 1.5934 - classification_loss: 0.5035\n",
            "27/86 [========>.....................] - ETA: 57s - loss: 2.0951 - regression_loss: 1.5915 - classification_loss: 0.5036\n",
            "28/86 [========>.....................] - ETA: 56s - loss: 2.0941 - regression_loss: 1.5909 - classification_loss: 0.5032\n",
            "29/86 [=========>....................] - ETA: 55s - loss: 2.1031 - regression_loss: 1.5949 - classification_loss: 0.5082\n",
            "30/86 [=========>....................] - ETA: 54s - loss: 2.1110 - regression_loss: 1.5975 - classification_loss: 0.5134\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.1071 - regression_loss: 1.5933 - classification_loss: 0.5138\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.1101 - regression_loss: 1.5936 - classification_loss: 0.5165\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.1216 - regression_loss: 1.6010 - classification_loss: 0.5206\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.1151 - regression_loss: 1.5971 - classification_loss: 0.5180\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.1147 - regression_loss: 1.5974 - classification_loss: 0.5174\n",
            "36/86 [===========>..................] - ETA: 48s - loss: 2.1180 - regression_loss: 1.6008 - classification_loss: 0.5172\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.1208 - regression_loss: 1.6024 - classification_loss: 0.5185\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.1134 - regression_loss: 1.5947 - classification_loss: 0.5186\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.1128 - regression_loss: 1.5933 - classification_loss: 0.5194\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.1117 - regression_loss: 1.5923 - classification_loss: 0.5194\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.1129 - regression_loss: 1.5940 - classification_loss: 0.5189\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1121 - regression_loss: 1.5948 - classification_loss: 0.5173\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1095 - regression_loss: 1.5928 - classification_loss: 0.5167\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1050 - regression_loss: 1.5888 - classification_loss: 0.5162\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.1051 - regression_loss: 1.5890 - classification_loss: 0.5160\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.0937 - regression_loss: 1.5806 - classification_loss: 0.5132\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.0879 - regression_loss: 1.5746 - classification_loss: 0.5133\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.0887 - regression_loss: 1.5769 - classification_loss: 0.5118\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.0848 - regression_loss: 1.5746 - classification_loss: 0.5102\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.0826 - regression_loss: 1.5725 - classification_loss: 0.5101\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.0751 - regression_loss: 1.5661 - classification_loss: 0.5090\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.0779 - regression_loss: 1.5696 - classification_loss: 0.5083\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0779 - regression_loss: 1.5697 - classification_loss: 0.5081\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0671 - regression_loss: 1.5621 - classification_loss: 0.5049\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0673 - regression_loss: 1.5634 - classification_loss: 0.5039\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0699 - regression_loss: 1.5654 - classification_loss: 0.5045\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0718 - regression_loss: 1.5659 - classification_loss: 0.5059\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0700 - regression_loss: 1.5641 - classification_loss: 0.5059\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0684 - regression_loss: 1.5634 - classification_loss: 0.5050\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0700 - regression_loss: 1.5648 - classification_loss: 0.5053\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0721 - regression_loss: 1.5662 - classification_loss: 0.5059\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.0711 - regression_loss: 1.5655 - classification_loss: 0.5056\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.0691 - regression_loss: 1.5644 - classification_loss: 0.5048\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.0729 - regression_loss: 1.5667 - classification_loss: 0.5062\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.0744 - regression_loss: 1.5682 - classification_loss: 0.5061\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0723 - regression_loss: 1.5661 - classification_loss: 0.5062\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0702 - regression_loss: 1.5656 - classification_loss: 0.5047\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0758 - regression_loss: 1.5705 - classification_loss: 0.5053\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0771 - regression_loss: 1.5725 - classification_loss: 0.5045\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0704 - regression_loss: 1.5677 - classification_loss: 0.5026\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0686 - regression_loss: 1.5664 - classification_loss: 0.5022\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0676 - regression_loss: 1.5661 - classification_loss: 0.5014\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0679 - regression_loss: 1.5664 - classification_loss: 0.5015\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0667 - regression_loss: 1.5655 - classification_loss: 0.5012\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0668 - regression_loss: 1.5647 - classification_loss: 0.5021\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0636 - regression_loss: 1.5619 - classification_loss: 0.5017 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0621 - regression_loss: 1.5609 - classification_loss: 0.5013\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0582 - regression_loss: 1.5577 - classification_loss: 0.5006\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0554 - regression_loss: 1.5557 - classification_loss: 0.4997\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0569 - regression_loss: 1.5575 - classification_loss: 0.4994\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0604 - regression_loss: 1.5607 - classification_loss: 0.4997\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0611 - regression_loss: 1.5608 - classification_loss: 0.5003\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0635 - regression_loss: 1.5626 - classification_loss: 0.5010\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0617 - regression_loss: 1.5614 - classification_loss: 0.5003\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0607 - regression_loss: 1.5603 - classification_loss: 0.5005\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0608 - regression_loss: 1.5598 - classification_loss: 0.5009\n",
            "Epoch 82: saving model to ./snapshots\\resnet50_csv_82.h5\n",
            "\n",
            "86/86 [==============================] - 83s 958ms/step - loss: 2.0608 - regression_loss: 1.5598 - classification_loss: 0.5009 - lr: 1.0000e-37\n",
            "Epoch 83/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:50 - loss: 2.0425 - regression_loss: 1.5119 - classification_loss: 0.5306\n",
            " 2/86 [..............................] - ETA: 1:08 - loss: 2.2023 - regression_loss: 1.6535 - classification_loss: 0.5488\n",
            " 3/86 [>.............................] - ETA: 1:05 - loss: 2.1182 - regression_loss: 1.5880 - classification_loss: 0.5302\n",
            " 4/86 [>.............................] - ETA: 1:09 - loss: 2.1252 - regression_loss: 1.6108 - classification_loss: 0.5144\n",
            " 5/86 [>.............................] - ETA: 1:15 - loss: 2.1652 - regression_loss: 1.6577 - classification_loss: 0.5075\n",
            " 6/86 [=>............................] - ETA: 1:13 - loss: 2.1536 - regression_loss: 1.6493 - classification_loss: 0.5043\n",
            " 7/86 [=>............................] - ETA: 1:13 - loss: 2.1114 - regression_loss: 1.6135 - classification_loss: 0.4978\n",
            " 8/86 [=>............................] - ETA: 1:12 - loss: 2.0929 - regression_loss: 1.6011 - classification_loss: 0.4919\n",
            " 9/86 [==>...........................] - ETA: 1:10 - loss: 2.0717 - regression_loss: 1.5827 - classification_loss: 0.4890\n",
            "10/86 [==>...........................] - ETA: 1:10 - loss: 2.0679 - regression_loss: 1.5856 - classification_loss: 0.4823\n",
            "11/86 [==>...........................] - ETA: 1:09 - loss: 2.0488 - regression_loss: 1.5691 - classification_loss: 0.4797\n",
            "12/86 [===>..........................] - ETA: 1:08 - loss: 2.0386 - regression_loss: 1.5585 - classification_loss: 0.4801\n",
            "13/86 [===>..........................] - ETA: 1:08 - loss: 2.0555 - regression_loss: 1.5711 - classification_loss: 0.4844\n",
            "14/86 [===>..........................] - ETA: 1:07 - loss: 2.0616 - regression_loss: 1.5792 - classification_loss: 0.4824\n",
            "15/86 [====>.........................] - ETA: 1:06 - loss: 2.0826 - regression_loss: 1.5931 - classification_loss: 0.4895\n",
            "16/86 [====>.........................] - ETA: 1:06 - loss: 2.0543 - regression_loss: 1.5722 - classification_loss: 0.4821\n",
            "17/86 [====>.........................] - ETA: 1:05 - loss: 2.0677 - regression_loss: 1.5854 - classification_loss: 0.4823\n",
            "18/86 [=====>........................] - ETA: 1:04 - loss: 2.0523 - regression_loss: 1.5708 - classification_loss: 0.4815\n",
            "19/86 [=====>........................] - ETA: 1:03 - loss: 2.0408 - regression_loss: 1.5586 - classification_loss: 0.4821\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.0234 - regression_loss: 1.5453 - classification_loss: 0.4782\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.0264 - regression_loss: 1.5464 - classification_loss: 0.4801\n",
            "22/86 [======>.......................] - ETA: 1:02 - loss: 2.0264 - regression_loss: 1.5479 - classification_loss: 0.4785\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.0325 - regression_loss: 1.5519 - classification_loss: 0.4806\n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.0483 - regression_loss: 1.5636 - classification_loss: 0.4848 \n",
            "25/86 [=======>......................] - ETA: 59s - loss: 2.0560 - regression_loss: 1.5725 - classification_loss: 0.4835\n",
            "26/86 [========>.....................] - ETA: 58s - loss: 2.0673 - regression_loss: 1.5802 - classification_loss: 0.4871\n",
            "27/86 [========>.....................] - ETA: 57s - loss: 2.0779 - regression_loss: 1.5918 - classification_loss: 0.4861\n",
            "28/86 [========>.....................] - ETA: 57s - loss: 2.0780 - regression_loss: 1.5901 - classification_loss: 0.4880\n",
            "29/86 [=========>....................] - ETA: 55s - loss: 2.0864 - regression_loss: 1.5960 - classification_loss: 0.4903\n",
            "30/86 [=========>....................] - ETA: 54s - loss: 2.0954 - regression_loss: 1.6021 - classification_loss: 0.4932\n",
            "31/86 [=========>....................] - ETA: 53s - loss: 2.0957 - regression_loss: 1.6008 - classification_loss: 0.4949\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.0953 - regression_loss: 1.6001 - classification_loss: 0.4952\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.0972 - regression_loss: 1.6020 - classification_loss: 0.4951\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.0983 - regression_loss: 1.6012 - classification_loss: 0.4971\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.0938 - regression_loss: 1.5977 - classification_loss: 0.4961\n",
            "36/86 [===========>..................] - ETA: 48s - loss: 2.1066 - regression_loss: 1.6080 - classification_loss: 0.4985\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.1117 - regression_loss: 1.6122 - classification_loss: 0.4996\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.1068 - regression_loss: 1.6080 - classification_loss: 0.4988\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.1122 - regression_loss: 1.6127 - classification_loss: 0.4996\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.1060 - regression_loss: 1.6086 - classification_loss: 0.4974\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.1103 - regression_loss: 1.6103 - classification_loss: 0.5000\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1123 - regression_loss: 1.6107 - classification_loss: 0.5016\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1063 - regression_loss: 1.6066 - classification_loss: 0.4997\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1009 - regression_loss: 1.6015 - classification_loss: 0.4994\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.1020 - regression_loss: 1.6015 - classification_loss: 0.5005\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.0952 - regression_loss: 1.5962 - classification_loss: 0.4990\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.0955 - regression_loss: 1.5963 - classification_loss: 0.4992\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.0870 - regression_loss: 1.5895 - classification_loss: 0.4975\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.0924 - regression_loss: 1.5940 - classification_loss: 0.4984\n",
            "50/86 [================>.............] - ETA: 35s - loss: 2.0943 - regression_loss: 1.5946 - classification_loss: 0.4996\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.0834 - regression_loss: 1.5865 - classification_loss: 0.4970\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.0769 - regression_loss: 1.5807 - classification_loss: 0.4963\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0773 - regression_loss: 1.5811 - classification_loss: 0.4962\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0830 - regression_loss: 1.5838 - classification_loss: 0.4992\n",
            "55/86 [==================>...........] - ETA: 30s - loss: 2.0883 - regression_loss: 1.5879 - classification_loss: 0.5004\n",
            "56/86 [==================>...........] - ETA: 29s - loss: 2.0904 - regression_loss: 1.5903 - classification_loss: 0.5001\n",
            "57/86 [==================>...........] - ETA: 28s - loss: 2.0924 - regression_loss: 1.5925 - classification_loss: 0.4999\n",
            "58/86 [===================>..........] - ETA: 27s - loss: 2.0946 - regression_loss: 1.5947 - classification_loss: 0.4999\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.0916 - regression_loss: 1.5919 - classification_loss: 0.4997\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.0913 - regression_loss: 1.5920 - classification_loss: 0.4993\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.0917 - regression_loss: 1.5923 - classification_loss: 0.4995\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.0962 - regression_loss: 1.5941 - classification_loss: 0.5021\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.0957 - regression_loss: 1.5931 - classification_loss: 0.5025\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.0956 - regression_loss: 1.5928 - classification_loss: 0.5028\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.0915 - regression_loss: 1.5897 - classification_loss: 0.5018\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0937 - regression_loss: 1.5917 - classification_loss: 0.5020\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0862 - regression_loss: 1.5860 - classification_loss: 0.5001\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0867 - regression_loss: 1.5863 - classification_loss: 0.5004\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0888 - regression_loss: 1.5882 - classification_loss: 0.5006\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0879 - regression_loss: 1.5878 - classification_loss: 0.5001\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0865 - regression_loss: 1.5864 - classification_loss: 0.5000\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0900 - regression_loss: 1.5893 - classification_loss: 0.5008\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0900 - regression_loss: 1.5885 - classification_loss: 0.5015\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0948 - regression_loss: 1.5927 - classification_loss: 0.5022\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0937 - regression_loss: 1.5913 - classification_loss: 0.5024\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0917 - regression_loss: 1.5891 - classification_loss: 0.5026 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0922 - regression_loss: 1.5897 - classification_loss: 0.5025\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0943 - regression_loss: 1.5910 - classification_loss: 0.5033\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0965 - regression_loss: 1.5932 - classification_loss: 0.5033\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0927 - regression_loss: 1.5902 - classification_loss: 0.5025\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0933 - regression_loss: 1.5907 - classification_loss: 0.5026\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0918 - regression_loss: 1.5891 - classification_loss: 0.5026\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0902 - regression_loss: 1.5872 - classification_loss: 0.5030\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0909 - regression_loss: 1.5876 - classification_loss: 0.5032\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0896 - regression_loss: 1.5869 - classification_loss: 0.5028\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0914 - regression_loss: 1.5883 - classification_loss: 0.5031\n",
            "Epoch 83: saving model to ./snapshots\\resnet50_csv_83.h5\n",
            "\n",
            "86/86 [==============================] - 83s 961ms/step - loss: 2.0914 - regression_loss: 1.5883 - classification_loss: 0.5031 - lr: 1.0000e-37\n",
            "Epoch 84/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:49 - loss: 2.1078 - regression_loss: 1.6199 - classification_loss: 0.4879\n",
            " 2/86 [..............................] - ETA: 1:19 - loss: 2.1846 - regression_loss: 1.6729 - classification_loss: 0.5117\n",
            " 3/86 [>.............................] - ETA: 1:29 - loss: 2.1210 - regression_loss: 1.6046 - classification_loss: 0.5164\n",
            " 4/86 [>.............................] - ETA: 1:22 - loss: 2.0473 - regression_loss: 1.5442 - classification_loss: 0.5031\n",
            " 5/86 [>.............................] - ETA: 1:20 - loss: 2.0624 - regression_loss: 1.5688 - classification_loss: 0.4936\n",
            " 6/86 [=>............................] - ETA: 1:16 - loss: 2.1385 - regression_loss: 1.6300 - classification_loss: 0.5085\n",
            " 7/86 [=>............................] - ETA: 1:14 - loss: 2.1171 - regression_loss: 1.6096 - classification_loss: 0.5075\n",
            " 8/86 [=>............................] - ETA: 1:18 - loss: 2.1054 - regression_loss: 1.5992 - classification_loss: 0.5061\n",
            " 9/86 [==>...........................] - ETA: 1:14 - loss: 2.0654 - regression_loss: 1.5731 - classification_loss: 0.4923\n",
            "10/86 [==>...........................] - ETA: 1:12 - loss: 2.0742 - regression_loss: 1.5773 - classification_loss: 0.4970\n",
            "11/86 [==>...........................] - ETA: 1:11 - loss: 2.0650 - regression_loss: 1.5708 - classification_loss: 0.4943\n",
            "12/86 [===>..........................] - ETA: 1:11 - loss: 2.0686 - regression_loss: 1.5782 - classification_loss: 0.4904\n",
            "13/86 [===>..........................] - ETA: 1:10 - loss: 2.0496 - regression_loss: 1.5614 - classification_loss: 0.4882\n",
            "14/86 [===>..........................] - ETA: 1:08 - loss: 2.0887 - regression_loss: 1.5853 - classification_loss: 0.5035\n",
            "15/86 [====>.........................] - ETA: 1:07 - loss: 2.0941 - regression_loss: 1.5912 - classification_loss: 0.5029\n",
            "16/86 [====>.........................] - ETA: 1:06 - loss: 2.0820 - regression_loss: 1.5813 - classification_loss: 0.5007\n",
            "17/86 [====>.........................] - ETA: 1:05 - loss: 2.1045 - regression_loss: 1.5913 - classification_loss: 0.5132\n",
            "18/86 [=====>........................] - ETA: 1:04 - loss: 2.1027 - regression_loss: 1.5896 - classification_loss: 0.5131\n",
            "19/86 [=====>........................] - ETA: 1:03 - loss: 2.0961 - regression_loss: 1.5857 - classification_loss: 0.5104\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.0916 - regression_loss: 1.5843 - classification_loss: 0.5074\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.0993 - regression_loss: 1.5907 - classification_loss: 0.5085\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.1086 - regression_loss: 1.5958 - classification_loss: 0.5128\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.1110 - regression_loss: 1.5978 - classification_loss: 0.5132\n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.1244 - regression_loss: 1.6065 - classification_loss: 0.5179 \n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.1276 - regression_loss: 1.6080 - classification_loss: 0.5196\n",
            "26/86 [========>.....................] - ETA: 58s - loss: 2.1171 - regression_loss: 1.5993 - classification_loss: 0.5178\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.1166 - regression_loss: 1.5990 - classification_loss: 0.5176\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.1138 - regression_loss: 1.5979 - classification_loss: 0.5159\n",
            "29/86 [=========>....................] - ETA: 55s - loss: 2.1204 - regression_loss: 1.6013 - classification_loss: 0.5191\n",
            "30/86 [=========>....................] - ETA: 54s - loss: 2.1194 - regression_loss: 1.6011 - classification_loss: 0.5183\n",
            "31/86 [=========>....................] - ETA: 53s - loss: 2.1158 - regression_loss: 1.5989 - classification_loss: 0.5169\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.1184 - regression_loss: 1.6006 - classification_loss: 0.5177\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.1191 - regression_loss: 1.6014 - classification_loss: 0.5177\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.1179 - regression_loss: 1.6004 - classification_loss: 0.5175\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.1165 - regression_loss: 1.5998 - classification_loss: 0.5168\n",
            "36/86 [===========>..................] - ETA: 48s - loss: 2.1113 - regression_loss: 1.5944 - classification_loss: 0.5169\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.1076 - regression_loss: 1.5928 - classification_loss: 0.5148\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.1060 - regression_loss: 1.5920 - classification_loss: 0.5139\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.1065 - regression_loss: 1.5934 - classification_loss: 0.5131\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.1034 - regression_loss: 1.5916 - classification_loss: 0.5118\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.0950 - regression_loss: 1.5860 - classification_loss: 0.5090\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.0944 - regression_loss: 1.5855 - classification_loss: 0.5088\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.0872 - regression_loss: 1.5797 - classification_loss: 0.5075\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.0853 - regression_loss: 1.5790 - classification_loss: 0.5063\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.0809 - regression_loss: 1.5764 - classification_loss: 0.5046\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.0790 - regression_loss: 1.5747 - classification_loss: 0.5042\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.0740 - regression_loss: 1.5707 - classification_loss: 0.5033\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.0732 - regression_loss: 1.5706 - classification_loss: 0.5026\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.0743 - regression_loss: 1.5699 - classification_loss: 0.5043\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.0688 - regression_loss: 1.5655 - classification_loss: 0.5033\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.0648 - regression_loss: 1.5621 - classification_loss: 0.5027\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.0695 - regression_loss: 1.5653 - classification_loss: 0.5042\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0751 - regression_loss: 1.5700 - classification_loss: 0.5051\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0733 - regression_loss: 1.5696 - classification_loss: 0.5036\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0692 - regression_loss: 1.5658 - classification_loss: 0.5034\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0703 - regression_loss: 1.5665 - classification_loss: 0.5038\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0682 - regression_loss: 1.5655 - classification_loss: 0.5027\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0690 - regression_loss: 1.5661 - classification_loss: 0.5029\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0651 - regression_loss: 1.5630 - classification_loss: 0.5021\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0612 - regression_loss: 1.5606 - classification_loss: 0.5007\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0667 - regression_loss: 1.5644 - classification_loss: 0.5022\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.0672 - regression_loss: 1.5652 - classification_loss: 0.5020\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.0689 - regression_loss: 1.5655 - classification_loss: 0.5034\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.0707 - regression_loss: 1.5668 - classification_loss: 0.5039\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.0695 - regression_loss: 1.5659 - classification_loss: 0.5037\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0699 - regression_loss: 1.5651 - classification_loss: 0.5049\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0741 - regression_loss: 1.5683 - classification_loss: 0.5058\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0667 - regression_loss: 1.5629 - classification_loss: 0.5038\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0661 - regression_loss: 1.5622 - classification_loss: 0.5039\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0683 - regression_loss: 1.5635 - classification_loss: 0.5048\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0710 - regression_loss: 1.5659 - classification_loss: 0.5051\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0719 - regression_loss: 1.5667 - classification_loss: 0.5052\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0746 - regression_loss: 1.5691 - classification_loss: 0.5055\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0734 - regression_loss: 1.5684 - classification_loss: 0.5050\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0776 - regression_loss: 1.5717 - classification_loss: 0.5059\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0785 - regression_loss: 1.5729 - classification_loss: 0.5056 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0768 - regression_loss: 1.5722 - classification_loss: 0.5046\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0768 - regression_loss: 1.5717 - classification_loss: 0.5051\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0753 - regression_loss: 1.5706 - classification_loss: 0.5047\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0702 - regression_loss: 1.5669 - classification_loss: 0.5033\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0756 - regression_loss: 1.5703 - classification_loss: 0.5053\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0709 - regression_loss: 1.5668 - classification_loss: 0.5041\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0720 - regression_loss: 1.5676 - classification_loss: 0.5044\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0726 - regression_loss: 1.5682 - classification_loss: 0.5045\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0717 - regression_loss: 1.5672 - classification_loss: 0.5045\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0720 - regression_loss: 1.5678 - classification_loss: 0.5042\n",
            "Epoch 84: saving model to ./snapshots\\resnet50_csv_84.h5\n",
            "\n",
            "Epoch 84: ReduceLROnPlateau reducing learning rate to 9.99999991097579e-39.\n",
            "\n",
            "86/86 [==============================] - 83s 955ms/step - loss: 2.0720 - regression_loss: 1.5678 - classification_loss: 0.5042 - lr: 1.0000e-37\n",
            "Epoch 85/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:49 - loss: 2.0930 - regression_loss: 1.5395 - classification_loss: 0.5535\n",
            " 2/86 [..............................] - ETA: 1:11 - loss: 2.0884 - regression_loss: 1.5665 - classification_loss: 0.5219\n",
            " 3/86 [>.............................] - ETA: 1:14 - loss: 2.1227 - regression_loss: 1.5875 - classification_loss: 0.5352\n",
            " 4/86 [>.............................] - ETA: 1:16 - loss: 2.1535 - regression_loss: 1.6274 - classification_loss: 0.5261\n",
            " 5/86 [>.............................] - ETA: 1:18 - loss: 2.1391 - regression_loss: 1.6297 - classification_loss: 0.5094\n",
            " 6/86 [=>............................] - ETA: 1:16 - loss: 2.1300 - regression_loss: 1.6253 - classification_loss: 0.5047\n",
            " 7/86 [=>............................] - ETA: 1:18 - loss: 2.1199 - regression_loss: 1.6115 - classification_loss: 0.5084\n",
            " 8/86 [=>............................] - ETA: 1:16 - loss: 2.0982 - regression_loss: 1.5918 - classification_loss: 0.5064\n",
            " 9/86 [==>...........................] - ETA: 1:15 - loss: 2.0789 - regression_loss: 1.5776 - classification_loss: 0.5014\n",
            "10/86 [==>...........................] - ETA: 1:13 - loss: 2.0636 - regression_loss: 1.5662 - classification_loss: 0.4974\n",
            "11/86 [==>...........................] - ETA: 1:13 - loss: 2.0766 - regression_loss: 1.5709 - classification_loss: 0.5057\n",
            "12/86 [===>..........................] - ETA: 1:11 - loss: 2.0595 - regression_loss: 1.5524 - classification_loss: 0.5071\n",
            "13/86 [===>..........................] - ETA: 1:12 - loss: 2.0710 - regression_loss: 1.5617 - classification_loss: 0.5093\n",
            "14/86 [===>..........................] - ETA: 1:10 - loss: 2.0708 - regression_loss: 1.5615 - classification_loss: 0.5093\n",
            "15/86 [====>.........................] - ETA: 1:11 - loss: 2.0686 - regression_loss: 1.5590 - classification_loss: 0.5096\n",
            "16/86 [====>.........................] - ETA: 1:09 - loss: 2.0733 - regression_loss: 1.5622 - classification_loss: 0.5111\n",
            "17/86 [====>.........................] - ETA: 1:08 - loss: 2.0666 - regression_loss: 1.5571 - classification_loss: 0.5095\n",
            "18/86 [=====>........................] - ETA: 1:07 - loss: 2.0727 - regression_loss: 1.5640 - classification_loss: 0.5088\n",
            "19/86 [=====>........................] - ETA: 1:06 - loss: 2.0810 - regression_loss: 1.5700 - classification_loss: 0.5110\n",
            "20/86 [=====>........................] - ETA: 1:05 - loss: 2.0929 - regression_loss: 1.5821 - classification_loss: 0.5108\n",
            "21/86 [======>.......................] - ETA: 1:04 - loss: 2.1061 - regression_loss: 1.5884 - classification_loss: 0.5177\n",
            "22/86 [======>.......................] - ETA: 1:03 - loss: 2.1105 - regression_loss: 1.5930 - classification_loss: 0.5175\n",
            "23/86 [=======>......................] - ETA: 1:01 - loss: 2.1091 - regression_loss: 1.5927 - classification_loss: 0.5164\n",
            "24/86 [=======>......................] - ETA: 1:01 - loss: 2.1116 - regression_loss: 1.5906 - classification_loss: 0.5210\n",
            "25/86 [=======>......................] - ETA: 59s - loss: 2.0923 - regression_loss: 1.5773 - classification_loss: 0.5150 \n",
            "26/86 [========>.....................] - ETA: 58s - loss: 2.0915 - regression_loss: 1.5800 - classification_loss: 0.5115\n",
            "27/86 [========>.....................] - ETA: 57s - loss: 2.0714 - regression_loss: 1.5655 - classification_loss: 0.5060\n",
            "28/86 [========>.....................] - ETA: 56s - loss: 2.0631 - regression_loss: 1.5585 - classification_loss: 0.5046\n",
            "29/86 [=========>....................] - ETA: 55s - loss: 2.0708 - regression_loss: 1.5644 - classification_loss: 0.5064\n",
            "30/86 [=========>....................] - ETA: 54s - loss: 2.0750 - regression_loss: 1.5696 - classification_loss: 0.5055\n",
            "31/86 [=========>....................] - ETA: 53s - loss: 2.0722 - regression_loss: 1.5684 - classification_loss: 0.5038\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.0724 - regression_loss: 1.5703 - classification_loss: 0.5021\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.0800 - regression_loss: 1.5767 - classification_loss: 0.5033\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.0951 - regression_loss: 1.5888 - classification_loss: 0.5063\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.0956 - regression_loss: 1.5878 - classification_loss: 0.5078\n",
            "36/86 [===========>..................] - ETA: 48s - loss: 2.1016 - regression_loss: 1.5922 - classification_loss: 0.5094\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.1117 - regression_loss: 1.6006 - classification_loss: 0.5110\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.1040 - regression_loss: 1.5947 - classification_loss: 0.5092\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.1089 - regression_loss: 1.5973 - classification_loss: 0.5115\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.1125 - regression_loss: 1.5997 - classification_loss: 0.5128\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.1140 - regression_loss: 1.6005 - classification_loss: 0.5134\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1091 - regression_loss: 1.5959 - classification_loss: 0.5132\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.1078 - regression_loss: 1.5950 - classification_loss: 0.5128\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.1114 - regression_loss: 1.5973 - classification_loss: 0.5140\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.1086 - regression_loss: 1.5972 - classification_loss: 0.5114\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1114 - regression_loss: 1.5995 - classification_loss: 0.5119\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1133 - regression_loss: 1.5999 - classification_loss: 0.5135\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1159 - regression_loss: 1.6023 - classification_loss: 0.5136\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1180 - regression_loss: 1.6031 - classification_loss: 0.5149\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1160 - regression_loss: 1.6016 - classification_loss: 0.5144\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1106 - regression_loss: 1.5972 - classification_loss: 0.5133\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1158 - regression_loss: 1.6011 - classification_loss: 0.5147\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1174 - regression_loss: 1.6030 - classification_loss: 0.5144\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1162 - regression_loss: 1.6031 - classification_loss: 0.5131\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1128 - regression_loss: 1.6014 - classification_loss: 0.5114\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1161 - regression_loss: 1.6041 - classification_loss: 0.5121\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1120 - regression_loss: 1.6009 - classification_loss: 0.5112\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1084 - regression_loss: 1.5985 - classification_loss: 0.5099\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1101 - regression_loss: 1.6001 - classification_loss: 0.5100\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.1083 - regression_loss: 1.5986 - classification_loss: 0.5097\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.1082 - regression_loss: 1.5986 - classification_loss: 0.5096\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1070 - regression_loss: 1.5976 - classification_loss: 0.5094\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.1023 - regression_loss: 1.5943 - classification_loss: 0.5080\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.1025 - regression_loss: 1.5940 - classification_loss: 0.5085\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1032 - regression_loss: 1.5944 - classification_loss: 0.5088\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1107 - regression_loss: 1.6008 - classification_loss: 0.5099\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1161 - regression_loss: 1.6041 - classification_loss: 0.5120\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1172 - regression_loss: 1.6046 - classification_loss: 0.5126\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1185 - regression_loss: 1.6055 - classification_loss: 0.5129\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1209 - regression_loss: 1.6083 - classification_loss: 0.5126\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1227 - regression_loss: 1.6093 - classification_loss: 0.5134\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1233 - regression_loss: 1.6102 - classification_loss: 0.5131\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1231 - regression_loss: 1.6102 - classification_loss: 0.5129\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1207 - regression_loss: 1.6085 - classification_loss: 0.5121\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1181 - regression_loss: 1.6067 - classification_loss: 0.5115\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1205 - regression_loss: 1.6087 - classification_loss: 0.5118 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1224 - regression_loss: 1.6097 - classification_loss: 0.5127\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1227 - regression_loss: 1.6098 - classification_loss: 0.5128\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1179 - regression_loss: 1.6057 - classification_loss: 0.5121\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1235 - regression_loss: 1.6102 - classification_loss: 0.5134\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1239 - regression_loss: 1.6105 - classification_loss: 0.5134\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1254 - regression_loss: 1.6118 - classification_loss: 0.5136\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1221 - regression_loss: 1.6099 - classification_loss: 0.5122\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1178 - regression_loss: 1.6066 - classification_loss: 0.5112\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1152 - regression_loss: 1.6043 - classification_loss: 0.5109\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1143 - regression_loss: 1.6037 - classification_loss: 0.5105\n",
            "Epoch 85: saving model to ./snapshots\\resnet50_csv_85.h5\n",
            "\n",
            "86/86 [==============================] - 84s 969ms/step - loss: 2.1143 - regression_loss: 1.6037 - classification_loss: 0.5105 - lr: 1.0000e-38\n",
            "Epoch 86/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:51 - loss: 2.0235 - regression_loss: 1.5282 - classification_loss: 0.4953\n",
            " 2/86 [..............................] - ETA: 1:22 - loss: 1.9268 - regression_loss: 1.4261 - classification_loss: 0.5008\n",
            " 3/86 [>.............................] - ETA: 1:32 - loss: 1.9635 - regression_loss: 1.4558 - classification_loss: 0.5077\n",
            " 4/86 [>.............................] - ETA: 1:30 - loss: 2.0341 - regression_loss: 1.4932 - classification_loss: 0.5409\n",
            " 5/86 [>.............................] - ETA: 1:25 - loss: 2.0238 - regression_loss: 1.4958 - classification_loss: 0.5280\n",
            " 6/86 [=>............................] - ETA: 1:24 - loss: 2.0201 - regression_loss: 1.5056 - classification_loss: 0.5144\n",
            " 7/86 [=>............................] - ETA: 1:24 - loss: 2.0241 - regression_loss: 1.5055 - classification_loss: 0.5186\n",
            " 8/86 [=>............................] - ETA: 1:22 - loss: 2.0678 - regression_loss: 1.5485 - classification_loss: 0.5193\n",
            " 9/86 [==>...........................] - ETA: 1:20 - loss: 2.0636 - regression_loss: 1.5472 - classification_loss: 0.5164\n",
            "10/86 [==>...........................] - ETA: 1:17 - loss: 2.0417 - regression_loss: 1.5293 - classification_loss: 0.5125\n",
            "11/86 [==>...........................] - ETA: 1:16 - loss: 2.0464 - regression_loss: 1.5158 - classification_loss: 0.5306\n",
            "12/86 [===>..........................] - ETA: 1:17 - loss: 2.0261 - regression_loss: 1.5012 - classification_loss: 0.5250\n",
            "13/86 [===>..........................] - ETA: 1:15 - loss: 2.0289 - regression_loss: 1.5015 - classification_loss: 0.5274\n",
            "14/86 [===>..........................] - ETA: 1:13 - loss: 2.0107 - regression_loss: 1.4921 - classification_loss: 0.5186\n",
            "15/86 [====>.........................] - ETA: 1:12 - loss: 2.0128 - regression_loss: 1.4956 - classification_loss: 0.5172\n",
            "16/86 [====>.........................] - ETA: 1:10 - loss: 2.0067 - regression_loss: 1.4931 - classification_loss: 0.5137\n",
            "17/86 [====>.........................] - ETA: 1:09 - loss: 2.0064 - regression_loss: 1.4946 - classification_loss: 0.5118\n",
            "18/86 [=====>........................] - ETA: 1:09 - loss: 2.0064 - regression_loss: 1.4969 - classification_loss: 0.5095\n",
            "19/86 [=====>........................] - ETA: 1:07 - loss: 2.0205 - regression_loss: 1.5078 - classification_loss: 0.5127\n",
            "20/86 [=====>........................] - ETA: 1:06 - loss: 2.0221 - regression_loss: 1.5108 - classification_loss: 0.5112\n",
            "21/86 [======>.......................] - ETA: 1:06 - loss: 2.0256 - regression_loss: 1.5156 - classification_loss: 0.5100\n",
            "22/86 [======>.......................] - ETA: 1:04 - loss: 2.0281 - regression_loss: 1.5167 - classification_loss: 0.5114\n",
            "23/86 [=======>......................] - ETA: 1:03 - loss: 2.0362 - regression_loss: 1.5203 - classification_loss: 0.5160\n",
            "24/86 [=======>......................] - ETA: 1:02 - loss: 2.0175 - regression_loss: 1.5080 - classification_loss: 0.5096\n",
            "25/86 [=======>......................] - ETA: 1:01 - loss: 2.0117 - regression_loss: 1.5033 - classification_loss: 0.5084\n",
            "26/86 [========>.....................] - ETA: 59s - loss: 2.0203 - regression_loss: 1.5103 - classification_loss: 0.5100 \n",
            "27/86 [========>.....................] - ETA: 59s - loss: 2.0400 - regression_loss: 1.5273 - classification_loss: 0.5128\n",
            "28/86 [========>.....................] - ETA: 58s - loss: 2.0348 - regression_loss: 1.5248 - classification_loss: 0.5100\n",
            "29/86 [=========>....................] - ETA: 56s - loss: 2.0307 - regression_loss: 1.5221 - classification_loss: 0.5086\n",
            "30/86 [=========>....................] - ETA: 55s - loss: 2.0297 - regression_loss: 1.5214 - classification_loss: 0.5083\n",
            "31/86 [=========>....................] - ETA: 54s - loss: 2.0252 - regression_loss: 1.5194 - classification_loss: 0.5059\n",
            "32/86 [==========>...................] - ETA: 53s - loss: 2.0241 - regression_loss: 1.5206 - classification_loss: 0.5035\n",
            "33/86 [==========>...................] - ETA: 52s - loss: 2.0255 - regression_loss: 1.5219 - classification_loss: 0.5037\n",
            "34/86 [==========>...................] - ETA: 51s - loss: 2.0340 - regression_loss: 1.5288 - classification_loss: 0.5052\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.0319 - regression_loss: 1.5284 - classification_loss: 0.5035\n",
            "36/86 [===========>..................] - ETA: 49s - loss: 2.0299 - regression_loss: 1.5270 - classification_loss: 0.5029\n",
            "37/86 [===========>..................] - ETA: 48s - loss: 2.0259 - regression_loss: 1.5243 - classification_loss: 0.5016\n",
            "38/86 [============>.................] - ETA: 47s - loss: 2.0304 - regression_loss: 1.5286 - classification_loss: 0.5018\n",
            "39/86 [============>.................] - ETA: 46s - loss: 2.0291 - regression_loss: 1.5275 - classification_loss: 0.5016\n",
            "40/86 [============>.................] - ETA: 45s - loss: 2.0335 - regression_loss: 1.5321 - classification_loss: 0.5014\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.0402 - regression_loss: 1.5373 - classification_loss: 0.5028\n",
            "42/86 [=============>................] - ETA: 43s - loss: 2.0396 - regression_loss: 1.5365 - classification_loss: 0.5031\n",
            "43/86 [==============>...............] - ETA: 42s - loss: 2.0459 - regression_loss: 1.5412 - classification_loss: 0.5046\n",
            "44/86 [==============>...............] - ETA: 41s - loss: 2.0425 - regression_loss: 1.5378 - classification_loss: 0.5047\n",
            "45/86 [==============>...............] - ETA: 40s - loss: 2.0499 - regression_loss: 1.5436 - classification_loss: 0.5063\n",
            "46/86 [===============>..............] - ETA: 39s - loss: 2.0521 - regression_loss: 1.5461 - classification_loss: 0.5060\n",
            "47/86 [===============>..............] - ETA: 38s - loss: 2.0530 - regression_loss: 1.5473 - classification_loss: 0.5058\n",
            "48/86 [===============>..............] - ETA: 37s - loss: 2.0515 - regression_loss: 1.5475 - classification_loss: 0.5040\n",
            "49/86 [================>.............] - ETA: 36s - loss: 2.0569 - regression_loss: 1.5515 - classification_loss: 0.5055\n",
            "50/86 [================>.............] - ETA: 35s - loss: 2.0621 - regression_loss: 1.5536 - classification_loss: 0.5085\n",
            "51/86 [================>.............] - ETA: 34s - loss: 2.0571 - regression_loss: 1.5490 - classification_loss: 0.5081\n",
            "52/86 [=================>............] - ETA: 33s - loss: 2.0533 - regression_loss: 1.5458 - classification_loss: 0.5076\n",
            "53/86 [=================>............] - ETA: 32s - loss: 2.0550 - regression_loss: 1.5470 - classification_loss: 0.5080\n",
            "54/86 [=================>............] - ETA: 31s - loss: 2.0569 - regression_loss: 1.5487 - classification_loss: 0.5082\n",
            "55/86 [==================>...........] - ETA: 30s - loss: 2.0520 - regression_loss: 1.5447 - classification_loss: 0.5073\n",
            "56/86 [==================>...........] - ETA: 29s - loss: 2.0535 - regression_loss: 1.5465 - classification_loss: 0.5071\n",
            "57/86 [==================>...........] - ETA: 28s - loss: 2.0563 - regression_loss: 1.5490 - classification_loss: 0.5073\n",
            "58/86 [===================>..........] - ETA: 27s - loss: 2.0557 - regression_loss: 1.5475 - classification_loss: 0.5082\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.0530 - regression_loss: 1.5455 - classification_loss: 0.5074\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.0537 - regression_loss: 1.5450 - classification_loss: 0.5086\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.0628 - regression_loss: 1.5527 - classification_loss: 0.5101\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.0635 - regression_loss: 1.5535 - classification_loss: 0.5101\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.0602 - regression_loss: 1.5513 - classification_loss: 0.5089\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.0626 - regression_loss: 1.5537 - classification_loss: 0.5089\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.0630 - regression_loss: 1.5547 - classification_loss: 0.5083\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0629 - regression_loss: 1.5555 - classification_loss: 0.5074\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0645 - regression_loss: 1.5556 - classification_loss: 0.5089\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0645 - regression_loss: 1.5553 - classification_loss: 0.5092\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0698 - regression_loss: 1.5594 - classification_loss: 0.5103\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0675 - regression_loss: 1.5570 - classification_loss: 0.5105\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0679 - regression_loss: 1.5572 - classification_loss: 0.5108\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0615 - regression_loss: 1.5530 - classification_loss: 0.5085\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0652 - regression_loss: 1.5560 - classification_loss: 0.5092\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0648 - regression_loss: 1.5556 - classification_loss: 0.5092\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0646 - regression_loss: 1.5555 - classification_loss: 0.5091\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0665 - regression_loss: 1.5573 - classification_loss: 0.5092 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0653 - regression_loss: 1.5570 - classification_loss: 0.5083\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0663 - regression_loss: 1.5578 - classification_loss: 0.5085\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0660 - regression_loss: 1.5577 - classification_loss: 0.5084\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0603 - regression_loss: 1.5534 - classification_loss: 0.5069\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0603 - regression_loss: 1.5536 - classification_loss: 0.5067\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0613 - regression_loss: 1.5541 - classification_loss: 0.5072\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0577 - regression_loss: 1.5519 - classification_loss: 0.5059\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0577 - regression_loss: 1.5516 - classification_loss: 0.5061\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0610 - regression_loss: 1.5538 - classification_loss: 0.5072\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0631 - regression_loss: 1.5550 - classification_loss: 0.5081\n",
            "Epoch 86: saving model to ./snapshots\\resnet50_csv_86.h5\n",
            "\n",
            "Epoch 86: ReduceLROnPlateau reducing learning rate to 9.999999350456405e-40.\n",
            "\n",
            "86/86 [==============================] - 84s 968ms/step - loss: 2.0631 - regression_loss: 1.5550 - classification_loss: 0.5081 - lr: 1.0000e-38\n",
            "Epoch 87/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:00 - loss: 1.8912 - regression_loss: 1.4309 - classification_loss: 0.4603\n",
            " 2/86 [..............................] - ETA: 1:33 - loss: 2.0812 - regression_loss: 1.5497 - classification_loss: 0.5316\n",
            " 3/86 [>.............................] - ETA: 1:22 - loss: 2.2231 - regression_loss: 1.6365 - classification_loss: 0.5866\n",
            " 4/86 [>.............................] - ETA: 1:17 - loss: 2.2119 - regression_loss: 1.6348 - classification_loss: 0.5770\n",
            " 5/86 [>.............................] - ETA: 1:17 - loss: 2.1933 - regression_loss: 1.6334 - classification_loss: 0.5599\n",
            " 6/86 [=>............................] - ETA: 1:18 - loss: 2.2624 - regression_loss: 1.6939 - classification_loss: 0.5686\n",
            " 7/86 [=>............................] - ETA: 1:15 - loss: 2.2578 - regression_loss: 1.6847 - classification_loss: 0.5731\n",
            " 8/86 [=>............................] - ETA: 1:13 - loss: 2.2356 - regression_loss: 1.6672 - classification_loss: 0.5685\n",
            " 9/86 [==>...........................] - ETA: 1:12 - loss: 2.1921 - regression_loss: 1.6336 - classification_loss: 0.5586\n",
            "10/86 [==>...........................] - ETA: 1:10 - loss: 2.1785 - regression_loss: 1.6266 - classification_loss: 0.5519\n",
            "11/86 [==>...........................] - ETA: 1:09 - loss: 2.1594 - regression_loss: 1.6223 - classification_loss: 0.5372\n",
            "12/86 [===>..........................] - ETA: 1:08 - loss: 2.1481 - regression_loss: 1.6137 - classification_loss: 0.5344\n",
            "13/86 [===>..........................] - ETA: 1:06 - loss: 2.1502 - regression_loss: 1.6145 - classification_loss: 0.5356\n",
            "14/86 [===>..........................] - ETA: 1:07 - loss: 2.1394 - regression_loss: 1.6042 - classification_loss: 0.5352\n",
            "15/86 [====>.........................] - ETA: 1:06 - loss: 2.1246 - regression_loss: 1.5958 - classification_loss: 0.5289\n",
            "16/86 [====>.........................] - ETA: 1:05 - loss: 2.1025 - regression_loss: 1.5816 - classification_loss: 0.5209\n",
            "17/86 [====>.........................] - ETA: 1:05 - loss: 2.1007 - regression_loss: 1.5792 - classification_loss: 0.5215\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.0921 - regression_loss: 1.5735 - classification_loss: 0.5187\n",
            "19/86 [=====>........................] - ETA: 1:04 - loss: 2.0892 - regression_loss: 1.5707 - classification_loss: 0.5185\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.0779 - regression_loss: 1.5620 - classification_loss: 0.5159\n",
            "21/86 [======>.......................] - ETA: 1:01 - loss: 2.0819 - regression_loss: 1.5653 - classification_loss: 0.5166\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.0986 - regression_loss: 1.5791 - classification_loss: 0.5195\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.1056 - regression_loss: 1.5860 - classification_loss: 0.5196\n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.0999 - regression_loss: 1.5803 - classification_loss: 0.5196 \n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.1124 - regression_loss: 1.5886 - classification_loss: 0.5239\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.1093 - regression_loss: 1.5860 - classification_loss: 0.5233\n",
            "27/86 [========>.....................] - ETA: 57s - loss: 2.1244 - regression_loss: 1.5993 - classification_loss: 0.5251\n",
            "28/86 [========>.....................] - ETA: 56s - loss: 2.1251 - regression_loss: 1.5979 - classification_loss: 0.5272\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.1338 - regression_loss: 1.6055 - classification_loss: 0.5283\n",
            "30/86 [=========>....................] - ETA: 54s - loss: 2.1371 - regression_loss: 1.6088 - classification_loss: 0.5283\n",
            "31/86 [=========>....................] - ETA: 53s - loss: 2.1377 - regression_loss: 1.6096 - classification_loss: 0.5281\n",
            "32/86 [==========>...................] - ETA: 53s - loss: 2.1392 - regression_loss: 1.6102 - classification_loss: 0.5289\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.1408 - regression_loss: 1.6120 - classification_loss: 0.5288\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.1367 - regression_loss: 1.6068 - classification_loss: 0.5299\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.1277 - regression_loss: 1.5979 - classification_loss: 0.5297\n",
            "36/86 [===========>..................] - ETA: 48s - loss: 2.1275 - regression_loss: 1.5984 - classification_loss: 0.5292\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.1254 - regression_loss: 1.5959 - classification_loss: 0.5295\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.1158 - regression_loss: 1.5903 - classification_loss: 0.5255\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.1100 - regression_loss: 1.5856 - classification_loss: 0.5245\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.1070 - regression_loss: 1.5805 - classification_loss: 0.5265\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.1126 - regression_loss: 1.5861 - classification_loss: 0.5264\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1139 - regression_loss: 1.5879 - classification_loss: 0.5260\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1084 - regression_loss: 1.5845 - classification_loss: 0.5240\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1096 - regression_loss: 1.5863 - classification_loss: 0.5233\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.1112 - regression_loss: 1.5890 - classification_loss: 0.5222\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1150 - regression_loss: 1.5925 - classification_loss: 0.5225\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1199 - regression_loss: 1.5964 - classification_loss: 0.5235\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1218 - regression_loss: 1.5974 - classification_loss: 0.5244\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1215 - regression_loss: 1.5975 - classification_loss: 0.5240\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1225 - regression_loss: 1.5987 - classification_loss: 0.5239\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1289 - regression_loss: 1.6015 - classification_loss: 0.5273\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1280 - regression_loss: 1.6007 - classification_loss: 0.5273\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1344 - regression_loss: 1.6061 - classification_loss: 0.5283\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1340 - regression_loss: 1.6060 - classification_loss: 0.5280\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1294 - regression_loss: 1.6016 - classification_loss: 0.5278\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1291 - regression_loss: 1.6011 - classification_loss: 0.5280\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1300 - regression_loss: 1.6023 - classification_loss: 0.5277\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1319 - regression_loss: 1.6038 - classification_loss: 0.5282\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1300 - regression_loss: 1.6034 - classification_loss: 0.5266\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.1337 - regression_loss: 1.6061 - classification_loss: 0.5275\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.1344 - regression_loss: 1.6071 - classification_loss: 0.5274\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.1349 - regression_loss: 1.6082 - classification_loss: 0.5267\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.1319 - regression_loss: 1.6061 - classification_loss: 0.5257\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.1335 - regression_loss: 1.6079 - classification_loss: 0.5256\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1348 - regression_loss: 1.6090 - classification_loss: 0.5257\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1352 - regression_loss: 1.6093 - classification_loss: 0.5259\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1340 - regression_loss: 1.6084 - classification_loss: 0.5256\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1295 - regression_loss: 1.6040 - classification_loss: 0.5255\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1313 - regression_loss: 1.6065 - classification_loss: 0.5248\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1288 - regression_loss: 1.6043 - classification_loss: 0.5245\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1263 - regression_loss: 1.6017 - classification_loss: 0.5246\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1281 - regression_loss: 1.6030 - classification_loss: 0.5251\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1233 - regression_loss: 1.5999 - classification_loss: 0.5234\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1232 - regression_loss: 1.6001 - classification_loss: 0.5231\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1198 - regression_loss: 1.5980 - classification_loss: 0.5218\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1160 - regression_loss: 1.5952 - classification_loss: 0.5207 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1122 - regression_loss: 1.5927 - classification_loss: 0.5195\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1107 - regression_loss: 1.5917 - classification_loss: 0.5190\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1104 - regression_loss: 1.5915 - classification_loss: 0.5189\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1105 - regression_loss: 1.5921 - classification_loss: 0.5184\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1100 - regression_loss: 1.5917 - classification_loss: 0.5183\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1075 - regression_loss: 1.5903 - classification_loss: 0.5172\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1052 - regression_loss: 1.5883 - classification_loss: 0.5169\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1022 - regression_loss: 1.5863 - classification_loss: 0.5159\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1054 - regression_loss: 1.5885 - classification_loss: 0.5169\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1007 - regression_loss: 1.5851 - classification_loss: 0.5156\n",
            "Epoch 87: saving model to ./snapshots\\resnet50_csv_87.h5\n",
            "\n",
            "86/86 [==============================] - 83s 958ms/step - loss: 2.1007 - regression_loss: 1.5851 - classification_loss: 0.5156 - lr: 1.0000e-39\n",
            "Epoch 88/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:45 - loss: 2.1372 - regression_loss: 1.5402 - classification_loss: 0.5970\n",
            " 2/86 [..............................] - ETA: 1:47 - loss: 2.1284 - regression_loss: 1.5956 - classification_loss: 0.5328\n",
            " 3/86 [>.............................] - ETA: 1:30 - loss: 2.1564 - regression_loss: 1.6264 - classification_loss: 0.5299\n",
            " 4/86 [>.............................] - ETA: 1:26 - loss: 2.2161 - regression_loss: 1.6743 - classification_loss: 0.5419\n",
            " 5/86 [>.............................] - ETA: 1:21 - loss: 2.1782 - regression_loss: 1.6446 - classification_loss: 0.5335\n",
            " 6/86 [=>............................] - ETA: 1:18 - loss: 2.1942 - regression_loss: 1.6557 - classification_loss: 0.5385\n",
            " 7/86 [=>............................] - ETA: 1:17 - loss: 2.1064 - regression_loss: 1.5884 - classification_loss: 0.5180\n",
            " 8/86 [=>............................] - ETA: 1:14 - loss: 2.1257 - regression_loss: 1.6029 - classification_loss: 0.5229\n",
            " 9/86 [==>...........................] - ETA: 1:12 - loss: 2.0924 - regression_loss: 1.5795 - classification_loss: 0.5129\n",
            "10/86 [==>...........................] - ETA: 1:11 - loss: 2.1018 - regression_loss: 1.5948 - classification_loss: 0.5070\n",
            "11/86 [==>...........................] - ETA: 1:09 - loss: 2.0858 - regression_loss: 1.5820 - classification_loss: 0.5038\n",
            "12/86 [===>..........................] - ETA: 1:08 - loss: 2.0774 - regression_loss: 1.5712 - classification_loss: 0.5061\n",
            "13/86 [===>..........................] - ETA: 1:07 - loss: 2.0961 - regression_loss: 1.5866 - classification_loss: 0.5095\n",
            "14/86 [===>..........................] - ETA: 1:06 - loss: 2.0769 - regression_loss: 1.5750 - classification_loss: 0.5018\n",
            "15/86 [====>.........................] - ETA: 1:07 - loss: 2.0869 - regression_loss: 1.5840 - classification_loss: 0.5028\n",
            "16/86 [====>.........................] - ETA: 1:05 - loss: 2.0837 - regression_loss: 1.5816 - classification_loss: 0.5021\n",
            "17/86 [====>.........................] - ETA: 1:04 - loss: 2.0889 - regression_loss: 1.5875 - classification_loss: 0.5014\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.0977 - regression_loss: 1.5930 - classification_loss: 0.5047\n",
            "19/86 [=====>........................] - ETA: 1:04 - loss: 2.1121 - regression_loss: 1.6051 - classification_loss: 0.5069\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.1045 - regression_loss: 1.6011 - classification_loss: 0.5033\n",
            "21/86 [======>.......................] - ETA: 1:01 - loss: 2.1207 - regression_loss: 1.6120 - classification_loss: 0.5087\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.1277 - regression_loss: 1.6149 - classification_loss: 0.5129\n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.1292 - regression_loss: 1.6158 - classification_loss: 0.5134 \n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.1365 - regression_loss: 1.6201 - classification_loss: 0.5164\n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.1371 - regression_loss: 1.6208 - classification_loss: 0.5164\n",
            "26/86 [========>.....................] - ETA: 56s - loss: 2.1276 - regression_loss: 1.6145 - classification_loss: 0.5131\n",
            "27/86 [========>.....................] - ETA: 55s - loss: 2.1249 - regression_loss: 1.6115 - classification_loss: 0.5134\n",
            "28/86 [========>.....................] - ETA: 54s - loss: 2.1310 - regression_loss: 1.6162 - classification_loss: 0.5148\n",
            "29/86 [=========>....................] - ETA: 53s - loss: 2.1326 - regression_loss: 1.6171 - classification_loss: 0.5155\n",
            "30/86 [=========>....................] - ETA: 52s - loss: 2.1271 - regression_loss: 1.6138 - classification_loss: 0.5134\n",
            "31/86 [=========>....................] - ETA: 51s - loss: 2.1258 - regression_loss: 1.6142 - classification_loss: 0.5116\n",
            "32/86 [==========>...................] - ETA: 50s - loss: 2.1257 - regression_loss: 1.6140 - classification_loss: 0.5117\n",
            "33/86 [==========>...................] - ETA: 49s - loss: 2.1285 - regression_loss: 1.6148 - classification_loss: 0.5137\n",
            "34/86 [==========>...................] - ETA: 48s - loss: 2.1342 - regression_loss: 1.6198 - classification_loss: 0.5143\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.1343 - regression_loss: 1.6195 - classification_loss: 0.5149\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.1417 - regression_loss: 1.6258 - classification_loss: 0.5160\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.1368 - regression_loss: 1.6214 - classification_loss: 0.5154\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.1416 - regression_loss: 1.6242 - classification_loss: 0.5173\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.1422 - regression_loss: 1.6245 - classification_loss: 0.5177\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.1398 - regression_loss: 1.6215 - classification_loss: 0.5183\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.1386 - regression_loss: 1.6207 - classification_loss: 0.5179\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.1373 - regression_loss: 1.6201 - classification_loss: 0.5172\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.1329 - regression_loss: 1.6157 - classification_loss: 0.5172\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.1319 - regression_loss: 1.6146 - classification_loss: 0.5173\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.1385 - regression_loss: 1.6171 - classification_loss: 0.5214\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1409 - regression_loss: 1.6184 - classification_loss: 0.5225\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.1352 - regression_loss: 1.6148 - classification_loss: 0.5204\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.1360 - regression_loss: 1.6157 - classification_loss: 0.5203\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1331 - regression_loss: 1.6133 - classification_loss: 0.5198\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1298 - regression_loss: 1.6112 - classification_loss: 0.5186\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1258 - regression_loss: 1.6083 - classification_loss: 0.5175\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1249 - regression_loss: 1.6070 - classification_loss: 0.5179\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1357 - regression_loss: 1.6156 - classification_loss: 0.5201\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1350 - regression_loss: 1.6156 - classification_loss: 0.5194\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1374 - regression_loss: 1.6179 - classification_loss: 0.5196\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1359 - regression_loss: 1.6164 - classification_loss: 0.5195\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1297 - regression_loss: 1.6113 - classification_loss: 0.5183\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1306 - regression_loss: 1.6114 - classification_loss: 0.5192\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1325 - regression_loss: 1.6127 - classification_loss: 0.5199\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1377 - regression_loss: 1.6164 - classification_loss: 0.5212\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1343 - regression_loss: 1.6134 - classification_loss: 0.5210\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1322 - regression_loss: 1.6125 - classification_loss: 0.5197\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1300 - regression_loss: 1.6104 - classification_loss: 0.5196\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1285 - regression_loss: 1.6092 - classification_loss: 0.5193\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1274 - regression_loss: 1.6083 - classification_loss: 0.5191\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1274 - regression_loss: 1.6088 - classification_loss: 0.5186\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1252 - regression_loss: 1.6078 - classification_loss: 0.5175\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1219 - regression_loss: 1.6041 - classification_loss: 0.5178\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1161 - regression_loss: 1.6002 - classification_loss: 0.5159\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1115 - regression_loss: 1.5966 - classification_loss: 0.5150\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1122 - regression_loss: 1.5974 - classification_loss: 0.5148\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1112 - regression_loss: 1.5971 - classification_loss: 0.5141\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1112 - regression_loss: 1.5969 - classification_loss: 0.5144\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1090 - regression_loss: 1.5956 - classification_loss: 0.5134\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1072 - regression_loss: 1.5941 - classification_loss: 0.5130\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1054 - regression_loss: 1.5922 - classification_loss: 0.5133 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1023 - regression_loss: 1.5893 - classification_loss: 0.5130\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1038 - regression_loss: 1.5895 - classification_loss: 0.5142\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1045 - regression_loss: 1.5899 - classification_loss: 0.5146\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1035 - regression_loss: 1.5893 - classification_loss: 0.5142\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1031 - regression_loss: 1.5889 - classification_loss: 0.5142\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1051 - regression_loss: 1.5905 - classification_loss: 0.5146\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1070 - regression_loss: 1.5920 - classification_loss: 0.5150\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1033 - regression_loss: 1.5897 - classification_loss: 0.5136\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1062 - regression_loss: 1.5920 - classification_loss: 0.5142\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1060 - regression_loss: 1.5921 - classification_loss: 0.5140\n",
            "Epoch 88: saving model to ./snapshots\\resnet50_csv_88.h5\n",
            "\n",
            "Epoch 88: ReduceLROnPlateau reducing learning rate to 1.0000002153053334e-40.\n",
            "\n",
            "86/86 [==============================] - 82s 953ms/step - loss: 2.1060 - regression_loss: 1.5921 - classification_loss: 0.5140 - lr: 1.0000e-39\n",
            "Epoch 89/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:56 - loss: 2.4995 - regression_loss: 1.8188 - classification_loss: 0.6807\n",
            " 2/86 [..............................] - ETA: 1:10 - loss: 2.2909 - regression_loss: 1.6829 - classification_loss: 0.6080\n",
            " 3/86 [>.............................] - ETA: 1:27 - loss: 2.2247 - regression_loss: 1.6351 - classification_loss: 0.5897\n",
            " 4/86 [>.............................] - ETA: 1:19 - loss: 2.1568 - regression_loss: 1.5940 - classification_loss: 0.5628\n",
            " 5/86 [>.............................] - ETA: 1:20 - loss: 2.0990 - regression_loss: 1.5672 - classification_loss: 0.5319\n",
            " 6/86 [=>............................] - ETA: 1:16 - loss: 2.1116 - regression_loss: 1.5752 - classification_loss: 0.5365\n",
            " 7/86 [=>............................] - ETA: 1:14 - loss: 2.0692 - regression_loss: 1.5527 - classification_loss: 0.5165\n",
            " 8/86 [=>............................] - ETA: 1:12 - loss: 2.0381 - regression_loss: 1.5326 - classification_loss: 0.5054\n",
            " 9/86 [==>...........................] - ETA: 1:12 - loss: 2.0482 - regression_loss: 1.5378 - classification_loss: 0.5104\n",
            "10/86 [==>...........................] - ETA: 1:10 - loss: 2.0773 - regression_loss: 1.5647 - classification_loss: 0.5125\n",
            "11/86 [==>...........................] - ETA: 1:11 - loss: 2.1188 - regression_loss: 1.5997 - classification_loss: 0.5191\n",
            "12/86 [===>..........................] - ETA: 1:10 - loss: 2.1298 - regression_loss: 1.6111 - classification_loss: 0.5187\n",
            "13/86 [===>..........................] - ETA: 1:11 - loss: 2.1193 - regression_loss: 1.5992 - classification_loss: 0.5200\n",
            "14/86 [===>..........................] - ETA: 1:10 - loss: 2.1063 - regression_loss: 1.5917 - classification_loss: 0.5146\n",
            "15/86 [====>.........................] - ETA: 1:10 - loss: 2.1125 - regression_loss: 1.5982 - classification_loss: 0.5144\n",
            "16/86 [====>.........................] - ETA: 1:08 - loss: 2.1096 - regression_loss: 1.5968 - classification_loss: 0.5128\n",
            "17/86 [====>.........................] - ETA: 1:09 - loss: 2.1138 - regression_loss: 1.6012 - classification_loss: 0.5126\n",
            "18/86 [=====>........................] - ETA: 1:07 - loss: 2.1155 - regression_loss: 1.6030 - classification_loss: 0.5125\n",
            "19/86 [=====>........................] - ETA: 1:05 - loss: 2.1280 - regression_loss: 1.6129 - classification_loss: 0.5152\n",
            "20/86 [=====>........................] - ETA: 1:04 - loss: 2.1315 - regression_loss: 1.6143 - classification_loss: 0.5172\n",
            "21/86 [======>.......................] - ETA: 1:03 - loss: 2.1376 - regression_loss: 1.6188 - classification_loss: 0.5187\n",
            "22/86 [======>.......................] - ETA: 1:02 - loss: 2.1452 - regression_loss: 1.6233 - classification_loss: 0.5219\n",
            "23/86 [=======>......................] - ETA: 1:02 - loss: 2.1491 - regression_loss: 1.6252 - classification_loss: 0.5239\n",
            "24/86 [=======>......................] - ETA: 1:00 - loss: 2.1495 - regression_loss: 1.6234 - classification_loss: 0.5261\n",
            "25/86 [=======>......................] - ETA: 59s - loss: 2.1489 - regression_loss: 1.6237 - classification_loss: 0.5252 \n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.1434 - regression_loss: 1.6204 - classification_loss: 0.5229\n",
            "27/86 [========>.....................] - ETA: 57s - loss: 2.1288 - regression_loss: 1.6105 - classification_loss: 0.5183\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.1250 - regression_loss: 1.6074 - classification_loss: 0.5176\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.1351 - regression_loss: 1.6158 - classification_loss: 0.5193\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.1404 - regression_loss: 1.6200 - classification_loss: 0.5203\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.1477 - regression_loss: 1.6240 - classification_loss: 0.5237\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.1516 - regression_loss: 1.6276 - classification_loss: 0.5240\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.1469 - regression_loss: 1.6233 - classification_loss: 0.5235\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.1385 - regression_loss: 1.6171 - classification_loss: 0.5215\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.1338 - regression_loss: 1.6138 - classification_loss: 0.5200\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.1385 - regression_loss: 1.6173 - classification_loss: 0.5212\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.1505 - regression_loss: 1.6271 - classification_loss: 0.5234\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.1393 - regression_loss: 1.6179 - classification_loss: 0.5213\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.1354 - regression_loss: 1.6134 - classification_loss: 0.5220\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.1367 - regression_loss: 1.6152 - classification_loss: 0.5215\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.1404 - regression_loss: 1.6188 - classification_loss: 0.5216\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.1407 - regression_loss: 1.6188 - classification_loss: 0.5219\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.1330 - regression_loss: 1.6137 - classification_loss: 0.5193\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.1287 - regression_loss: 1.6098 - classification_loss: 0.5190\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.1254 - regression_loss: 1.6048 - classification_loss: 0.5206\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.1244 - regression_loss: 1.6043 - classification_loss: 0.5201\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.1244 - regression_loss: 1.6051 - classification_loss: 0.5193\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.1191 - regression_loss: 1.6017 - classification_loss: 0.5174\n",
            "49/86 [================>.............] - ETA: 34s - loss: 2.1164 - regression_loss: 1.6001 - classification_loss: 0.5163\n",
            "50/86 [================>.............] - ETA: 33s - loss: 2.1188 - regression_loss: 1.6013 - classification_loss: 0.5175\n",
            "51/86 [================>.............] - ETA: 32s - loss: 2.1157 - regression_loss: 1.5986 - classification_loss: 0.5171\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1137 - regression_loss: 1.5973 - classification_loss: 0.5164\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1110 - regression_loss: 1.5967 - classification_loss: 0.5143\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1080 - regression_loss: 1.5931 - classification_loss: 0.5149\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1117 - regression_loss: 1.5961 - classification_loss: 0.5156\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1094 - regression_loss: 1.5950 - classification_loss: 0.5144\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1041 - regression_loss: 1.5903 - classification_loss: 0.5138\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1019 - regression_loss: 1.5887 - classification_loss: 0.5132\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1038 - regression_loss: 1.5902 - classification_loss: 0.5136\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1048 - regression_loss: 1.5914 - classification_loss: 0.5134\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1029 - regression_loss: 1.5902 - classification_loss: 0.5127\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1053 - regression_loss: 1.5915 - classification_loss: 0.5138\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1053 - regression_loss: 1.5914 - classification_loss: 0.5140\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1066 - regression_loss: 1.5923 - classification_loss: 0.5142\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1086 - regression_loss: 1.5933 - classification_loss: 0.5153\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.1079 - regression_loss: 1.5926 - classification_loss: 0.5152\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1049 - regression_loss: 1.5900 - classification_loss: 0.5150\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1093 - regression_loss: 1.5937 - classification_loss: 0.5155\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1119 - regression_loss: 1.5955 - classification_loss: 0.5164\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1146 - regression_loss: 1.5978 - classification_loss: 0.5169\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1164 - regression_loss: 1.5999 - classification_loss: 0.5165\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1158 - regression_loss: 1.5994 - classification_loss: 0.5164\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1206 - regression_loss: 1.6030 - classification_loss: 0.5177\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1207 - regression_loss: 1.6022 - classification_loss: 0.5186\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1209 - regression_loss: 1.6020 - classification_loss: 0.5190\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1182 - regression_loss: 1.6003 - classification_loss: 0.5179 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1158 - regression_loss: 1.5994 - classification_loss: 0.5165\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1120 - regression_loss: 1.5966 - classification_loss: 0.5154\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1075 - regression_loss: 1.5933 - classification_loss: 0.5142\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1073 - regression_loss: 1.5926 - classification_loss: 0.5146\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1088 - regression_loss: 1.5939 - classification_loss: 0.5149\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1025 - regression_loss: 1.5893 - classification_loss: 0.5132\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1016 - regression_loss: 1.5888 - classification_loss: 0.5128\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1059 - regression_loss: 1.5923 - classification_loss: 0.5136\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1064 - regression_loss: 1.5927 - classification_loss: 0.5137\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1058 - regression_loss: 1.5917 - classification_loss: 0.5141\n",
            "Epoch 89: saving model to ./snapshots\\resnet50_csv_89.h5\n",
            "\n",
            "86/86 [==============================] - 83s 956ms/step - loss: 2.1058 - regression_loss: 1.5917 - classification_loss: 0.5141 - lr: 9.9999e-41\n",
            "Epoch 90/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:41 - loss: 1.8878 - regression_loss: 1.4210 - classification_loss: 0.4667\n",
            " 2/86 [..............................] - ETA: 1:18 - loss: 1.9555 - regression_loss: 1.4862 - classification_loss: 0.4693\n",
            " 3/86 [>.............................] - ETA: 1:09 - loss: 2.0885 - regression_loss: 1.6051 - classification_loss: 0.4834\n",
            " 4/86 [>.............................] - ETA: 1:13 - loss: 2.0269 - regression_loss: 1.5570 - classification_loss: 0.4699\n",
            " 5/86 [>.............................] - ETA: 1:09 - loss: 2.0054 - regression_loss: 1.5331 - classification_loss: 0.4723\n",
            " 6/86 [=>............................] - ETA: 1:10 - loss: 2.0605 - regression_loss: 1.5752 - classification_loss: 0.4853\n",
            " 7/86 [=>............................] - ETA: 1:09 - loss: 2.0648 - regression_loss: 1.5755 - classification_loss: 0.4893\n",
            " 8/86 [=>............................] - ETA: 1:09 - loss: 2.0534 - regression_loss: 1.5644 - classification_loss: 0.4891\n",
            " 9/86 [==>...........................] - ETA: 1:07 - loss: 2.0422 - regression_loss: 1.5550 - classification_loss: 0.4872\n",
            "10/86 [==>...........................] - ETA: 1:08 - loss: 2.0225 - regression_loss: 1.5385 - classification_loss: 0.4841\n",
            "11/86 [==>...........................] - ETA: 1:07 - loss: 2.0229 - regression_loss: 1.5416 - classification_loss: 0.4813\n",
            "12/86 [===>..........................] - ETA: 1:07 - loss: 2.0370 - regression_loss: 1.5501 - classification_loss: 0.4870\n",
            "13/86 [===>..........................] - ETA: 1:06 - loss: 2.0619 - regression_loss: 1.5698 - classification_loss: 0.4921\n",
            "14/86 [===>..........................] - ETA: 1:05 - loss: 2.0626 - regression_loss: 1.5726 - classification_loss: 0.4901\n",
            "15/86 [====>.........................] - ETA: 1:05 - loss: 2.0536 - regression_loss: 1.5681 - classification_loss: 0.4855\n",
            "16/86 [====>.........................] - ETA: 1:03 - loss: 2.0475 - regression_loss: 1.5630 - classification_loss: 0.4845\n",
            "17/86 [====>.........................] - ETA: 1:02 - loss: 2.0462 - regression_loss: 1.5616 - classification_loss: 0.4845\n",
            "18/86 [=====>........................] - ETA: 1:01 - loss: 2.0536 - regression_loss: 1.5666 - classification_loss: 0.4870\n",
            "19/86 [=====>........................] - ETA: 1:02 - loss: 2.0594 - regression_loss: 1.5720 - classification_loss: 0.4875\n",
            "20/86 [=====>........................] - ETA: 1:02 - loss: 2.0616 - regression_loss: 1.5727 - classification_loss: 0.4888\n",
            "21/86 [======>.......................] - ETA: 1:00 - loss: 2.0665 - regression_loss: 1.5760 - classification_loss: 0.4905\n",
            "22/86 [======>.......................] - ETA: 59s - loss: 2.0557 - regression_loss: 1.5639 - classification_loss: 0.4918 \n",
            "23/86 [=======>......................] - ETA: 58s - loss: 2.0607 - regression_loss: 1.5659 - classification_loss: 0.4948\n",
            "24/86 [=======>......................] - ETA: 57s - loss: 2.0773 - regression_loss: 1.5771 - classification_loss: 0.5002\n",
            "25/86 [=======>......................] - ETA: 56s - loss: 2.0839 - regression_loss: 1.5816 - classification_loss: 0.5023\n",
            "26/86 [========>.....................] - ETA: 55s - loss: 2.0796 - regression_loss: 1.5773 - classification_loss: 0.5023\n",
            "27/86 [========>.....................] - ETA: 54s - loss: 2.0812 - regression_loss: 1.5770 - classification_loss: 0.5042\n",
            "28/86 [========>.....................] - ETA: 53s - loss: 2.0847 - regression_loss: 1.5781 - classification_loss: 0.5066\n",
            "29/86 [=========>....................] - ETA: 52s - loss: 2.0849 - regression_loss: 1.5792 - classification_loss: 0.5057\n",
            "30/86 [=========>....................] - ETA: 51s - loss: 2.0731 - regression_loss: 1.5712 - classification_loss: 0.5019\n",
            "31/86 [=========>....................] - ETA: 50s - loss: 2.0692 - regression_loss: 1.5690 - classification_loss: 0.5002\n",
            "32/86 [==========>...................] - ETA: 49s - loss: 2.0797 - regression_loss: 1.5770 - classification_loss: 0.5027\n",
            "33/86 [==========>...................] - ETA: 48s - loss: 2.0744 - regression_loss: 1.5734 - classification_loss: 0.5010\n",
            "34/86 [==========>...................] - ETA: 47s - loss: 2.0880 - regression_loss: 1.5812 - classification_loss: 0.5068\n",
            "35/86 [===========>..................] - ETA: 46s - loss: 2.0949 - regression_loss: 1.5870 - classification_loss: 0.5080\n",
            "36/86 [===========>..................] - ETA: 45s - loss: 2.1023 - regression_loss: 1.5927 - classification_loss: 0.5096\n",
            "37/86 [===========>..................] - ETA: 44s - loss: 2.0997 - regression_loss: 1.5914 - classification_loss: 0.5083\n",
            "38/86 [============>.................] - ETA: 43s - loss: 2.0934 - regression_loss: 1.5880 - classification_loss: 0.5054\n",
            "39/86 [============>.................] - ETA: 43s - loss: 2.0977 - regression_loss: 1.5932 - classification_loss: 0.5045\n",
            "40/86 [============>.................] - ETA: 42s - loss: 2.0934 - regression_loss: 1.5897 - classification_loss: 0.5038\n",
            "41/86 [=============>................] - ETA: 41s - loss: 2.0930 - regression_loss: 1.5896 - classification_loss: 0.5034\n",
            "42/86 [=============>................] - ETA: 40s - loss: 2.0854 - regression_loss: 1.5833 - classification_loss: 0.5020\n",
            "43/86 [==============>...............] - ETA: 39s - loss: 2.0894 - regression_loss: 1.5857 - classification_loss: 0.5036\n",
            "44/86 [==============>...............] - ETA: 38s - loss: 2.0956 - regression_loss: 1.5905 - classification_loss: 0.5051\n",
            "45/86 [==============>...............] - ETA: 37s - loss: 2.0955 - regression_loss: 1.5910 - classification_loss: 0.5045\n",
            "46/86 [===============>..............] - ETA: 36s - loss: 2.0904 - regression_loss: 1.5875 - classification_loss: 0.5028\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.0954 - regression_loss: 1.5906 - classification_loss: 0.5048\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.0939 - regression_loss: 1.5895 - classification_loss: 0.5043\n",
            "49/86 [================>.............] - ETA: 34s - loss: 2.0939 - regression_loss: 1.5895 - classification_loss: 0.5044\n",
            "50/86 [================>.............] - ETA: 33s - loss: 2.0925 - regression_loss: 1.5885 - classification_loss: 0.5041\n",
            "51/86 [================>.............] - ETA: 32s - loss: 2.0914 - regression_loss: 1.5852 - classification_loss: 0.5061\n",
            "52/86 [=================>............] - ETA: 31s - loss: 2.0919 - regression_loss: 1.5848 - classification_loss: 0.5071\n",
            "53/86 [=================>............] - ETA: 30s - loss: 2.0935 - regression_loss: 1.5867 - classification_loss: 0.5068\n",
            "54/86 [=================>............] - ETA: 29s - loss: 2.0968 - regression_loss: 1.5897 - classification_loss: 0.5071\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0945 - regression_loss: 1.5878 - classification_loss: 0.5067\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0946 - regression_loss: 1.5865 - classification_loss: 0.5080\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0931 - regression_loss: 1.5860 - classification_loss: 0.5071\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0995 - regression_loss: 1.5890 - classification_loss: 0.5105\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0995 - regression_loss: 1.5892 - classification_loss: 0.5103\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0969 - regression_loss: 1.5874 - classification_loss: 0.5095\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0971 - regression_loss: 1.5878 - classification_loss: 0.5094\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.0935 - regression_loss: 1.5841 - classification_loss: 0.5094\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.0924 - regression_loss: 1.5832 - classification_loss: 0.5092\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.0954 - regression_loss: 1.5854 - classification_loss: 0.5100\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.0984 - regression_loss: 1.5872 - classification_loss: 0.5112\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1038 - regression_loss: 1.5919 - classification_loss: 0.5119\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1068 - regression_loss: 1.5945 - classification_loss: 0.5123\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1138 - regression_loss: 1.6004 - classification_loss: 0.5134\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1081 - regression_loss: 1.5961 - classification_loss: 0.5120\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0999 - regression_loss: 1.5901 - classification_loss: 0.5098\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1004 - regression_loss: 1.5900 - classification_loss: 0.5105\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1076 - regression_loss: 1.5958 - classification_loss: 0.5118\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1082 - regression_loss: 1.5962 - classification_loss: 0.5120\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1109 - regression_loss: 1.5991 - classification_loss: 0.5118\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1110 - regression_loss: 1.5999 - classification_loss: 0.5111\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1121 - regression_loss: 1.6009 - classification_loss: 0.5112 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1085 - regression_loss: 1.5985 - classification_loss: 0.5100\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1097 - regression_loss: 1.5990 - classification_loss: 0.5107\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1087 - regression_loss: 1.5982 - classification_loss: 0.5105\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1086 - regression_loss: 1.5987 - classification_loss: 0.5099\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1089 - regression_loss: 1.5987 - classification_loss: 0.5102\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1078 - regression_loss: 1.5975 - classification_loss: 0.5104\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1066 - regression_loss: 1.5967 - classification_loss: 0.5099\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1037 - regression_loss: 1.5944 - classification_loss: 0.5093\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1034 - regression_loss: 1.5941 - classification_loss: 0.5093\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1030 - regression_loss: 1.5941 - classification_loss: 0.5089\n",
            "Epoch 90: saving model to ./snapshots\\resnet50_csv_90.h5\n",
            "\n",
            "Epoch 90: ReduceLROnPlateau reducing learning rate to 9.99994610111476e-42.\n",
            "\n",
            "86/86 [==============================] - 83s 958ms/step - loss: 2.1030 - regression_loss: 1.5941 - classification_loss: 0.5089 - lr: 9.9999e-41\n",
            "Epoch 91/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:50 - loss: 1.8634 - regression_loss: 1.3870 - classification_loss: 0.4764\n",
            " 2/86 [..............................] - ETA: 1:00 - loss: 2.0718 - regression_loss: 1.5514 - classification_loss: 0.5204\n",
            " 3/86 [>.............................] - ETA: 1:08 - loss: 2.1850 - regression_loss: 1.6448 - classification_loss: 0.5403\n",
            " 4/86 [>.............................] - ETA: 1:10 - loss: 2.1006 - regression_loss: 1.5726 - classification_loss: 0.5280\n",
            " 5/86 [>.............................] - ETA: 1:25 - loss: 2.1354 - regression_loss: 1.6130 - classification_loss: 0.5223\n",
            " 6/86 [=>............................] - ETA: 1:17 - loss: 2.1395 - regression_loss: 1.6120 - classification_loss: 0.5275\n",
            " 7/86 [=>............................] - ETA: 1:16 - loss: 2.1359 - regression_loss: 1.6021 - classification_loss: 0.5338\n",
            " 8/86 [=>............................] - ETA: 1:13 - loss: 2.1159 - regression_loss: 1.5880 - classification_loss: 0.5279\n",
            " 9/86 [==>...........................] - ETA: 1:12 - loss: 2.1205 - regression_loss: 1.5947 - classification_loss: 0.5258\n",
            "10/86 [==>...........................] - ETA: 1:12 - loss: 2.1147 - regression_loss: 1.5999 - classification_loss: 0.5148\n",
            "11/86 [==>...........................] - ETA: 1:10 - loss: 2.0856 - regression_loss: 1.5823 - classification_loss: 0.5033\n",
            "12/86 [===>..........................] - ETA: 1:09 - loss: 2.0840 - regression_loss: 1.5802 - classification_loss: 0.5038\n",
            "13/86 [===>..........................] - ETA: 1:08 - loss: 2.0793 - regression_loss: 1.5785 - classification_loss: 0.5008\n",
            "14/86 [===>..........................] - ETA: 1:07 - loss: 2.0686 - regression_loss: 1.5704 - classification_loss: 0.4982\n",
            "15/86 [====>.........................] - ETA: 1:05 - loss: 2.0876 - regression_loss: 1.5790 - classification_loss: 0.5087\n",
            "16/86 [====>.........................] - ETA: 1:04 - loss: 2.0874 - regression_loss: 1.5831 - classification_loss: 0.5044\n",
            "17/86 [====>.........................] - ETA: 1:03 - loss: 2.1025 - regression_loss: 1.5948 - classification_loss: 0.5077\n",
            "18/86 [=====>........................] - ETA: 1:03 - loss: 2.0926 - regression_loss: 1.5885 - classification_loss: 0.5041\n",
            "19/86 [=====>........................] - ETA: 1:02 - loss: 2.0800 - regression_loss: 1.5794 - classification_loss: 0.5006\n",
            "20/86 [=====>........................] - ETA: 1:01 - loss: 2.0822 - regression_loss: 1.5824 - classification_loss: 0.4998\n",
            "21/86 [======>.......................] - ETA: 1:00 - loss: 2.0851 - regression_loss: 1.5852 - classification_loss: 0.5000\n",
            "22/86 [======>.......................] - ETA: 59s - loss: 2.0772 - regression_loss: 1.5777 - classification_loss: 0.4995 \n",
            "23/86 [=======>......................] - ETA: 58s - loss: 2.0901 - regression_loss: 1.5871 - classification_loss: 0.5030\n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.0989 - regression_loss: 1.5900 - classification_loss: 0.5089\n",
            "25/86 [=======>......................] - ETA: 56s - loss: 2.0988 - regression_loss: 1.5897 - classification_loss: 0.5092\n",
            "26/86 [========>.....................] - ETA: 56s - loss: 2.1085 - regression_loss: 1.5981 - classification_loss: 0.5103\n",
            "27/86 [========>.....................] - ETA: 55s - loss: 2.1060 - regression_loss: 1.5945 - classification_loss: 0.5115\n",
            "28/86 [========>.....................] - ETA: 54s - loss: 2.1099 - regression_loss: 1.5982 - classification_loss: 0.5118\n",
            "29/86 [=========>....................] - ETA: 53s - loss: 2.0901 - regression_loss: 1.5832 - classification_loss: 0.5070\n",
            "30/86 [=========>....................] - ETA: 52s - loss: 2.0845 - regression_loss: 1.5778 - classification_loss: 0.5067\n",
            "31/86 [=========>....................] - ETA: 51s - loss: 2.0906 - regression_loss: 1.5828 - classification_loss: 0.5078\n",
            "32/86 [==========>...................] - ETA: 50s - loss: 2.1013 - regression_loss: 1.5911 - classification_loss: 0.5102\n",
            "33/86 [==========>...................] - ETA: 49s - loss: 2.1046 - regression_loss: 1.5951 - classification_loss: 0.5095\n",
            "34/86 [==========>...................] - ETA: 48s - loss: 2.0990 - regression_loss: 1.5891 - classification_loss: 0.5100\n",
            "35/86 [===========>..................] - ETA: 47s - loss: 2.1013 - regression_loss: 1.5907 - classification_loss: 0.5106\n",
            "36/86 [===========>..................] - ETA: 46s - loss: 2.1048 - regression_loss: 1.5927 - classification_loss: 0.5121\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.1028 - regression_loss: 1.5900 - classification_loss: 0.5129\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.1041 - regression_loss: 1.5890 - classification_loss: 0.5151\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.0964 - regression_loss: 1.5822 - classification_loss: 0.5143\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.0914 - regression_loss: 1.5775 - classification_loss: 0.5139\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.0931 - regression_loss: 1.5794 - classification_loss: 0.5137\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.0982 - regression_loss: 1.5840 - classification_loss: 0.5142\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.0996 - regression_loss: 1.5851 - classification_loss: 0.5146\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.0966 - regression_loss: 1.5829 - classification_loss: 0.5138\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.0995 - regression_loss: 1.5839 - classification_loss: 0.5156\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.0933 - regression_loss: 1.5800 - classification_loss: 0.5134\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.0895 - regression_loss: 1.5769 - classification_loss: 0.5126\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.0877 - regression_loss: 1.5749 - classification_loss: 0.5128\n",
            "49/86 [================>.............] - ETA: 34s - loss: 2.0848 - regression_loss: 1.5729 - classification_loss: 0.5119\n",
            "50/86 [================>.............] - ETA: 33s - loss: 2.0813 - regression_loss: 1.5700 - classification_loss: 0.5113\n",
            "51/86 [================>.............] - ETA: 32s - loss: 2.0852 - regression_loss: 1.5728 - classification_loss: 0.5123\n",
            "52/86 [=================>............] - ETA: 31s - loss: 2.0868 - regression_loss: 1.5741 - classification_loss: 0.5127\n",
            "53/86 [=================>............] - ETA: 30s - loss: 2.0898 - regression_loss: 1.5775 - classification_loss: 0.5123\n",
            "54/86 [=================>............] - ETA: 29s - loss: 2.0901 - regression_loss: 1.5792 - classification_loss: 0.5109\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0947 - regression_loss: 1.5831 - classification_loss: 0.5115\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0936 - regression_loss: 1.5828 - classification_loss: 0.5108\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0956 - regression_loss: 1.5849 - classification_loss: 0.5106\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0938 - regression_loss: 1.5832 - classification_loss: 0.5106\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0956 - regression_loss: 1.5839 - classification_loss: 0.5116\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1039 - regression_loss: 1.5905 - classification_loss: 0.5133\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1022 - regression_loss: 1.5893 - classification_loss: 0.5129\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1080 - regression_loss: 1.5946 - classification_loss: 0.5134\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1035 - regression_loss: 1.5915 - classification_loss: 0.5120\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1005 - regression_loss: 1.5892 - classification_loss: 0.5114\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.0985 - regression_loss: 1.5873 - classification_loss: 0.5112\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.1039 - regression_loss: 1.5903 - classification_loss: 0.5136\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.1045 - regression_loss: 1.5902 - classification_loss: 0.5144\n",
            "68/86 [======================>.......] - ETA: 16s - loss: 2.1078 - regression_loss: 1.5941 - classification_loss: 0.5137\n",
            "69/86 [=======================>......] - ETA: 15s - loss: 2.1058 - regression_loss: 1.5925 - classification_loss: 0.5133\n",
            "70/86 [=======================>......] - ETA: 14s - loss: 2.1034 - regression_loss: 1.5911 - classification_loss: 0.5123\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0996 - regression_loss: 1.5880 - classification_loss: 0.5116\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0988 - regression_loss: 1.5870 - classification_loss: 0.5118\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1003 - regression_loss: 1.5883 - classification_loss: 0.5119\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0997 - regression_loss: 1.5850 - classification_loss: 0.5147\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0993 - regression_loss: 1.5846 - classification_loss: 0.5147\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0994 - regression_loss: 1.5842 - classification_loss: 0.5152 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1008 - regression_loss: 1.5846 - classification_loss: 0.5162\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1050 - regression_loss: 1.5873 - classification_loss: 0.5176\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1079 - regression_loss: 1.5906 - classification_loss: 0.5173\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1074 - regression_loss: 1.5904 - classification_loss: 0.5170\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1042 - regression_loss: 1.5879 - classification_loss: 0.5162\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1056 - regression_loss: 1.5888 - classification_loss: 0.5167\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1108 - regression_loss: 1.5921 - classification_loss: 0.5187\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1070 - regression_loss: 1.5890 - classification_loss: 0.5179\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1064 - regression_loss: 1.5886 - classification_loss: 0.5178\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1097 - regression_loss: 1.5911 - classification_loss: 0.5186\n",
            "Epoch 91: saving model to ./snapshots\\resnet50_csv_91.h5\n",
            "\n",
            "86/86 [==============================] - 82s 945ms/step - loss: 2.1097 - regression_loss: 1.5911 - classification_loss: 0.5186 - lr: 9.9997e-42\n",
            "Epoch 92/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:43 - loss: 1.9174 - regression_loss: 1.4749 - classification_loss: 0.4425\n",
            " 2/86 [..............................] - ETA: 1:30 - loss: 2.0124 - regression_loss: 1.5299 - classification_loss: 0.4825\n",
            " 3/86 [>.............................] - ETA: 1:21 - loss: 2.0481 - regression_loss: 1.5608 - classification_loss: 0.4873\n",
            " 4/86 [>.............................] - ETA: 1:18 - loss: 2.0325 - regression_loss: 1.5452 - classification_loss: 0.4872\n",
            " 5/86 [>.............................] - ETA: 1:16 - loss: 1.9964 - regression_loss: 1.5126 - classification_loss: 0.4837\n",
            " 6/86 [=>............................] - ETA: 1:13 - loss: 2.0072 - regression_loss: 1.5140 - classification_loss: 0.4932\n",
            " 7/86 [=>............................] - ETA: 1:13 - loss: 2.0529 - regression_loss: 1.5412 - classification_loss: 0.5117\n",
            " 8/86 [=>............................] - ETA: 1:10 - loss: 2.0794 - regression_loss: 1.5561 - classification_loss: 0.5233\n",
            " 9/86 [==>...........................] - ETA: 1:10 - loss: 2.0751 - regression_loss: 1.5510 - classification_loss: 0.5241\n",
            "10/86 [==>...........................] - ETA: 1:12 - loss: 2.0670 - regression_loss: 1.5514 - classification_loss: 0.5156\n",
            "11/86 [==>...........................] - ETA: 1:12 - loss: 2.0822 - regression_loss: 1.5615 - classification_loss: 0.5208\n",
            "12/86 [===>..........................] - ETA: 1:12 - loss: 2.0658 - regression_loss: 1.5531 - classification_loss: 0.5128\n",
            "13/86 [===>..........................] - ETA: 1:09 - loss: 2.0622 - regression_loss: 1.5534 - classification_loss: 0.5088\n",
            "14/86 [===>..........................] - ETA: 1:08 - loss: 2.0613 - regression_loss: 1.5566 - classification_loss: 0.5048\n",
            "15/86 [====>.........................] - ETA: 1:08 - loss: 2.0680 - regression_loss: 1.5612 - classification_loss: 0.5068\n",
            "16/86 [====>.........................] - ETA: 1:07 - loss: 2.0810 - regression_loss: 1.5688 - classification_loss: 0.5122\n",
            "17/86 [====>.........................] - ETA: 1:05 - loss: 2.0862 - regression_loss: 1.5699 - classification_loss: 0.5162\n",
            "18/86 [=====>........................] - ETA: 1:04 - loss: 2.0723 - regression_loss: 1.5562 - classification_loss: 0.5160\n",
            "19/86 [=====>........................] - ETA: 1:04 - loss: 2.0677 - regression_loss: 1.5505 - classification_loss: 0.5172\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.0806 - regression_loss: 1.5571 - classification_loss: 0.5235\n",
            "21/86 [======>.......................] - ETA: 1:01 - loss: 2.0866 - regression_loss: 1.5591 - classification_loss: 0.5275\n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.0894 - regression_loss: 1.5621 - classification_loss: 0.5274\n",
            "23/86 [=======>......................] - ETA: 59s - loss: 2.0966 - regression_loss: 1.5692 - classification_loss: 0.5274 \n",
            "24/86 [=======>......................] - ETA: 58s - loss: 2.0905 - regression_loss: 1.5653 - classification_loss: 0.5252\n",
            "25/86 [=======>......................] - ETA: 57s - loss: 2.1035 - regression_loss: 1.5756 - classification_loss: 0.5279\n",
            "26/86 [========>.....................] - ETA: 56s - loss: 2.0961 - regression_loss: 1.5684 - classification_loss: 0.5278\n",
            "27/86 [========>.....................] - ETA: 55s - loss: 2.1034 - regression_loss: 1.5751 - classification_loss: 0.5283\n",
            "28/86 [========>.....................] - ETA: 54s - loss: 2.1070 - regression_loss: 1.5788 - classification_loss: 0.5283\n",
            "29/86 [=========>....................] - ETA: 53s - loss: 2.1036 - regression_loss: 1.5764 - classification_loss: 0.5272\n",
            "30/86 [=========>....................] - ETA: 52s - loss: 2.0995 - regression_loss: 1.5740 - classification_loss: 0.5256\n",
            "31/86 [=========>....................] - ETA: 51s - loss: 2.0960 - regression_loss: 1.5719 - classification_loss: 0.5241\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.0930 - regression_loss: 1.5716 - classification_loss: 0.5214\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.1004 - regression_loss: 1.5785 - classification_loss: 0.5218\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.1067 - regression_loss: 1.5808 - classification_loss: 0.5259\n",
            "35/86 [===========>..................] - ETA: 47s - loss: 2.1116 - regression_loss: 1.5855 - classification_loss: 0.5261\n",
            "36/86 [===========>..................] - ETA: 46s - loss: 2.1053 - regression_loss: 1.5795 - classification_loss: 0.5258\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.1091 - regression_loss: 1.5810 - classification_loss: 0.5280\n",
            "38/86 [============>.................] - ETA: 44s - loss: 2.1106 - regression_loss: 1.5845 - classification_loss: 0.5261\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.1059 - regression_loss: 1.5810 - classification_loss: 0.5249\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.1047 - regression_loss: 1.5804 - classification_loss: 0.5243\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.1048 - regression_loss: 1.5808 - classification_loss: 0.5240\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.1051 - regression_loss: 1.5800 - classification_loss: 0.5250\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.1061 - regression_loss: 1.5804 - classification_loss: 0.5257\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.1073 - regression_loss: 1.5822 - classification_loss: 0.5251\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.1058 - regression_loss: 1.5818 - classification_loss: 0.5240\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.1080 - regression_loss: 1.5842 - classification_loss: 0.5238\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.1031 - regression_loss: 1.5798 - classification_loss: 0.5232\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.1038 - regression_loss: 1.5806 - classification_loss: 0.5232\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1096 - regression_loss: 1.5856 - classification_loss: 0.5240\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1090 - regression_loss: 1.5851 - classification_loss: 0.5239\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1058 - regression_loss: 1.5843 - classification_loss: 0.5215\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1036 - regression_loss: 1.5832 - classification_loss: 0.5204\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1043 - regression_loss: 1.5833 - classification_loss: 0.5210\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1041 - regression_loss: 1.5823 - classification_loss: 0.5218\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1058 - regression_loss: 1.5819 - classification_loss: 0.5239\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1049 - regression_loss: 1.5817 - classification_loss: 0.5232\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1101 - regression_loss: 1.5857 - classification_loss: 0.5244\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1162 - regression_loss: 1.5910 - classification_loss: 0.5251\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1163 - regression_loss: 1.5907 - classification_loss: 0.5256\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1212 - regression_loss: 1.5953 - classification_loss: 0.5258\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1220 - regression_loss: 1.5963 - classification_loss: 0.5257\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1262 - regression_loss: 1.6007 - classification_loss: 0.5255\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1283 - regression_loss: 1.6016 - classification_loss: 0.5267\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1255 - regression_loss: 1.5993 - classification_loss: 0.5262\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1215 - regression_loss: 1.5965 - classification_loss: 0.5250\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.1206 - regression_loss: 1.5956 - classification_loss: 0.5250\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1242 - regression_loss: 1.5986 - classification_loss: 0.5256\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1241 - regression_loss: 1.5993 - classification_loss: 0.5247\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1211 - regression_loss: 1.5964 - classification_loss: 0.5247\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1183 - regression_loss: 1.5947 - classification_loss: 0.5236\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1167 - regression_loss: 1.5931 - classification_loss: 0.5236\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1132 - regression_loss: 1.5901 - classification_loss: 0.5232\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1100 - regression_loss: 1.5878 - classification_loss: 0.5223\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1095 - regression_loss: 1.5846 - classification_loss: 0.5249\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1081 - regression_loss: 1.5851 - classification_loss: 0.5230\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1074 - regression_loss: 1.5847 - classification_loss: 0.5227 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1057 - regression_loss: 1.5830 - classification_loss: 0.5226\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1066 - regression_loss: 1.5849 - classification_loss: 0.5217\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1049 - regression_loss: 1.5836 - classification_loss: 0.5213\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1025 - regression_loss: 1.5803 - classification_loss: 0.5222\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1051 - regression_loss: 1.5822 - classification_loss: 0.5230\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1014 - regression_loss: 1.5792 - classification_loss: 0.5222\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0984 - regression_loss: 1.5777 - classification_loss: 0.5207\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1005 - regression_loss: 1.5795 - classification_loss: 0.5211\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1032 - regression_loss: 1.5817 - classification_loss: 0.5215\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1040 - regression_loss: 1.5823 - classification_loss: 0.5218\n",
            "Epoch 92: saving model to ./snapshots\\resnet50_csv_92.h5\n",
            "\n",
            "Epoch 92: ReduceLROnPlateau reducing learning rate to 9.999665841421895e-43.\n",
            "\n",
            "86/86 [==============================] - 83s 964ms/step - loss: 2.1040 - regression_loss: 1.5823 - classification_loss: 0.5218 - lr: 9.9997e-42\n",
            "Epoch 93/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:36 - loss: 1.8566 - regression_loss: 1.3945 - classification_loss: 0.4621\n",
            " 2/86 [..............................] - ETA: 1:07 - loss: 1.9071 - regression_loss: 1.4289 - classification_loss: 0.4783\n",
            " 3/86 [>.............................] - ETA: 1:11 - loss: 1.9676 - regression_loss: 1.4879 - classification_loss: 0.4797\n",
            " 4/86 [>.............................] - ETA: 1:16 - loss: 1.9005 - regression_loss: 1.4438 - classification_loss: 0.4567\n",
            " 5/86 [>.............................] - ETA: 1:18 - loss: 1.9234 - regression_loss: 1.4762 - classification_loss: 0.4471\n",
            " 6/86 [=>............................] - ETA: 1:14 - loss: 1.9155 - regression_loss: 1.4652 - classification_loss: 0.4503\n",
            " 7/86 [=>............................] - ETA: 1:12 - loss: 1.9581 - regression_loss: 1.4991 - classification_loss: 0.4590\n",
            " 8/86 [=>............................] - ETA: 1:13 - loss: 1.9783 - regression_loss: 1.5125 - classification_loss: 0.4658\n",
            " 9/86 [==>...........................] - ETA: 1:13 - loss: 2.0057 - regression_loss: 1.5346 - classification_loss: 0.4711\n",
            "10/86 [==>...........................] - ETA: 1:11 - loss: 2.0303 - regression_loss: 1.5506 - classification_loss: 0.4797\n",
            "11/86 [==>...........................] - ETA: 1:10 - loss: 2.0289 - regression_loss: 1.5489 - classification_loss: 0.4800\n",
            "12/86 [===>..........................] - ETA: 1:09 - loss: 1.9901 - regression_loss: 1.5193 - classification_loss: 0.4708\n",
            "13/86 [===>..........................] - ETA: 1:07 - loss: 2.0227 - regression_loss: 1.5450 - classification_loss: 0.4778\n",
            "14/86 [===>..........................] - ETA: 1:06 - loss: 2.0206 - regression_loss: 1.5377 - classification_loss: 0.4829\n",
            "15/86 [====>.........................] - ETA: 1:06 - loss: 1.9972 - regression_loss: 1.5204 - classification_loss: 0.4768\n",
            "16/86 [====>.........................] - ETA: 1:04 - loss: 2.0110 - regression_loss: 1.5339 - classification_loss: 0.4771\n",
            "17/86 [====>.........................] - ETA: 1:03 - loss: 2.0204 - regression_loss: 1.5420 - classification_loss: 0.4784\n",
            "18/86 [=====>........................] - ETA: 1:03 - loss: 2.0474 - regression_loss: 1.5622 - classification_loss: 0.4851\n",
            "19/86 [=====>........................] - ETA: 1:02 - loss: 2.0451 - regression_loss: 1.5622 - classification_loss: 0.4829\n",
            "20/86 [=====>........................] - ETA: 1:01 - loss: 2.0534 - regression_loss: 1.5705 - classification_loss: 0.4829\n",
            "21/86 [======>.......................] - ETA: 59s - loss: 2.0424 - regression_loss: 1.5627 - classification_loss: 0.4797 \n",
            "22/86 [======>.......................] - ETA: 1:00 - loss: 2.0291 - regression_loss: 1.5526 - classification_loss: 0.4765\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.0395 - regression_loss: 1.5581 - classification_loss: 0.4814\n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.0347 - regression_loss: 1.5529 - classification_loss: 0.4819 \n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.0367 - regression_loss: 1.5527 - classification_loss: 0.4840\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.0499 - regression_loss: 1.5621 - classification_loss: 0.4878\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.0424 - regression_loss: 1.5575 - classification_loss: 0.4849\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.0483 - regression_loss: 1.5603 - classification_loss: 0.4880\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.0582 - regression_loss: 1.5695 - classification_loss: 0.4888\n",
            "30/86 [=========>....................] - ETA: 54s - loss: 2.0683 - regression_loss: 1.5743 - classification_loss: 0.4941\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.0803 - regression_loss: 1.5802 - classification_loss: 0.5001\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.0831 - regression_loss: 1.5806 - classification_loss: 0.5024\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.0828 - regression_loss: 1.5819 - classification_loss: 0.5010\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.0755 - regression_loss: 1.5746 - classification_loss: 0.5009\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.0597 - regression_loss: 1.5634 - classification_loss: 0.4963\n",
            "36/86 [===========>..................] - ETA: 48s - loss: 2.0670 - regression_loss: 1.5694 - classification_loss: 0.4977\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.0676 - regression_loss: 1.5706 - classification_loss: 0.4970\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.0612 - regression_loss: 1.5660 - classification_loss: 0.4952\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.0578 - regression_loss: 1.5634 - classification_loss: 0.4945\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.0693 - regression_loss: 1.5726 - classification_loss: 0.4967\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.0723 - regression_loss: 1.5746 - classification_loss: 0.4976\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.0713 - regression_loss: 1.5736 - classification_loss: 0.4978\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.0760 - regression_loss: 1.5774 - classification_loss: 0.4986\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.0795 - regression_loss: 1.5810 - classification_loss: 0.4985\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.0895 - regression_loss: 1.5881 - classification_loss: 0.5014\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.0859 - regression_loss: 1.5854 - classification_loss: 0.5006\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.0888 - regression_loss: 1.5862 - classification_loss: 0.5025\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.0923 - regression_loss: 1.5889 - classification_loss: 0.5034\n",
            "49/86 [================>.............] - ETA: 34s - loss: 2.0961 - regression_loss: 1.5925 - classification_loss: 0.5036\n",
            "50/86 [================>.............] - ETA: 33s - loss: 2.0936 - regression_loss: 1.5904 - classification_loss: 0.5031\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.0912 - regression_loss: 1.5884 - classification_loss: 0.5028\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.0855 - regression_loss: 1.5841 - classification_loss: 0.5014\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0872 - regression_loss: 1.5848 - classification_loss: 0.5024\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0854 - regression_loss: 1.5823 - classification_loss: 0.5030\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0797 - regression_loss: 1.5782 - classification_loss: 0.5015\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0785 - regression_loss: 1.5777 - classification_loss: 0.5009\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0834 - regression_loss: 1.5815 - classification_loss: 0.5019\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0839 - regression_loss: 1.5820 - classification_loss: 0.5019\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0847 - regression_loss: 1.5828 - classification_loss: 0.5020\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0870 - regression_loss: 1.5840 - classification_loss: 0.5030\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0828 - regression_loss: 1.5809 - classification_loss: 0.5019\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.0789 - regression_loss: 1.5767 - classification_loss: 0.5021\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.0708 - regression_loss: 1.5705 - classification_loss: 0.5004\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.0672 - regression_loss: 1.5663 - classification_loss: 0.5008\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.0714 - regression_loss: 1.5698 - classification_loss: 0.5016\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.0691 - regression_loss: 1.5679 - classification_loss: 0.5011\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.0764 - regression_loss: 1.5739 - classification_loss: 0.5025\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0759 - regression_loss: 1.5737 - classification_loss: 0.5022\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0725 - regression_loss: 1.5714 - classification_loss: 0.5010\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0736 - regression_loss: 1.5724 - classification_loss: 0.5011\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0749 - regression_loss: 1.5735 - classification_loss: 0.5014\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0735 - regression_loss: 1.5719 - classification_loss: 0.5016\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0730 - regression_loss: 1.5714 - classification_loss: 0.5017\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0720 - regression_loss: 1.5710 - classification_loss: 0.5010\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0725 - regression_loss: 1.5712 - classification_loss: 0.5013\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0744 - regression_loss: 1.5727 - classification_loss: 0.5017 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0771 - regression_loss: 1.5749 - classification_loss: 0.5022\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0781 - regression_loss: 1.5752 - classification_loss: 0.5029\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0774 - regression_loss: 1.5745 - classification_loss: 0.5028\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0758 - regression_loss: 1.5735 - classification_loss: 0.5022\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0806 - regression_loss: 1.5766 - classification_loss: 0.5040\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0873 - regression_loss: 1.5818 - classification_loss: 0.5055\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0850 - regression_loss: 1.5796 - classification_loss: 0.5053\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0855 - regression_loss: 1.5803 - classification_loss: 0.5052\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0854 - regression_loss: 1.5798 - classification_loss: 0.5056\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0867 - regression_loss: 1.5806 - classification_loss: 0.5061\n",
            "Epoch 93: saving model to ./snapshots\\resnet50_csv_93.h5\n",
            "\n",
            "86/86 [==============================] - 81s 942ms/step - loss: 2.0867 - regression_loss: 1.5806 - classification_loss: 0.5061 - lr: 1.0005e-42\n",
            "Epoch 94/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:00 - loss: 1.9265 - regression_loss: 1.4087 - classification_loss: 0.5178\n",
            " 2/86 [..............................] - ETA: 1:31 - loss: 1.9849 - regression_loss: 1.4744 - classification_loss: 0.5105\n",
            " 3/86 [>.............................] - ETA: 1:25 - loss: 2.1470 - regression_loss: 1.5790 - classification_loss: 0.5680\n",
            " 4/86 [>.............................] - ETA: 1:41 - loss: 2.1932 - regression_loss: 1.6374 - classification_loss: 0.5558\n",
            " 5/86 [>.............................] - ETA: 1:31 - loss: 2.2203 - regression_loss: 1.6696 - classification_loss: 0.5507\n",
            " 6/86 [=>............................] - ETA: 1:25 - loss: 2.1778 - regression_loss: 1.6423 - classification_loss: 0.5355\n",
            " 7/86 [=>............................] - ETA: 1:23 - loss: 2.1316 - regression_loss: 1.6118 - classification_loss: 0.5198\n",
            " 8/86 [=>............................] - ETA: 1:21 - loss: 2.0944 - regression_loss: 1.5764 - classification_loss: 0.5181\n",
            " 9/86 [==>...........................] - ETA: 1:19 - loss: 2.0808 - regression_loss: 1.5632 - classification_loss: 0.5176\n",
            "10/86 [==>...........................] - ETA: 1:19 - loss: 2.0879 - regression_loss: 1.5672 - classification_loss: 0.5207\n",
            "11/86 [==>...........................] - ETA: 1:15 - loss: 2.0732 - regression_loss: 1.5593 - classification_loss: 0.5138\n",
            "12/86 [===>..........................] - ETA: 1:14 - loss: 2.0974 - regression_loss: 1.5817 - classification_loss: 0.5156\n",
            "13/86 [===>..........................] - ETA: 1:12 - loss: 2.1057 - regression_loss: 1.5882 - classification_loss: 0.5175\n",
            "14/86 [===>..........................] - ETA: 1:11 - loss: 2.1096 - regression_loss: 1.5939 - classification_loss: 0.5157\n",
            "15/86 [====>.........................] - ETA: 1:09 - loss: 2.0852 - regression_loss: 1.5767 - classification_loss: 0.5084\n",
            "16/86 [====>.........................] - ETA: 1:08 - loss: 2.0721 - regression_loss: 1.5669 - classification_loss: 0.5052\n",
            "17/86 [====>.........................] - ETA: 1:07 - loss: 2.0567 - regression_loss: 1.5533 - classification_loss: 0.5034\n",
            "18/86 [=====>........................] - ETA: 1:06 - loss: 2.0755 - regression_loss: 1.5696 - classification_loss: 0.5059\n",
            "19/86 [=====>........................] - ETA: 1:04 - loss: 2.0616 - regression_loss: 1.5619 - classification_loss: 0.4997\n",
            "20/86 [=====>........................] - ETA: 1:04 - loss: 2.0730 - regression_loss: 1.5695 - classification_loss: 0.5035\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.0725 - regression_loss: 1.5693 - classification_loss: 0.5033\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.0732 - regression_loss: 1.5682 - classification_loss: 0.5051\n",
            "23/86 [=======>......................] - ETA: 1:01 - loss: 2.0748 - regression_loss: 1.5715 - classification_loss: 0.5033\n",
            "24/86 [=======>......................] - ETA: 1:00 - loss: 2.0591 - regression_loss: 1.5601 - classification_loss: 0.4989\n",
            "25/86 [=======>......................] - ETA: 59s - loss: 2.0631 - regression_loss: 1.5631 - classification_loss: 0.5000 \n",
            "26/86 [========>.....................] - ETA: 59s - loss: 2.0601 - regression_loss: 1.5607 - classification_loss: 0.4994\n",
            "27/86 [========>.....................] - ETA: 58s - loss: 2.0767 - regression_loss: 1.5746 - classification_loss: 0.5021\n",
            "28/86 [========>.....................] - ETA: 56s - loss: 2.0736 - regression_loss: 1.5699 - classification_loss: 0.5037\n",
            "29/86 [=========>....................] - ETA: 56s - loss: 2.0677 - regression_loss: 1.5665 - classification_loss: 0.5011\n",
            "30/86 [=========>....................] - ETA: 55s - loss: 2.0630 - regression_loss: 1.5656 - classification_loss: 0.4975\n",
            "31/86 [=========>....................] - ETA: 54s - loss: 2.0696 - regression_loss: 1.5682 - classification_loss: 0.5014\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.0702 - regression_loss: 1.5683 - classification_loss: 0.5019\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.0724 - regression_loss: 1.5697 - classification_loss: 0.5027\n",
            "34/86 [==========>...................] - ETA: 51s - loss: 2.0684 - regression_loss: 1.5659 - classification_loss: 0.5025\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.0674 - regression_loss: 1.5646 - classification_loss: 0.5028\n",
            "36/86 [===========>..................] - ETA: 48s - loss: 2.0644 - regression_loss: 1.5619 - classification_loss: 0.5025\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.0702 - regression_loss: 1.5660 - classification_loss: 0.5042\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.0740 - regression_loss: 1.5699 - classification_loss: 0.5041\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.0724 - regression_loss: 1.5682 - classification_loss: 0.5042\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.0716 - regression_loss: 1.5696 - classification_loss: 0.5019\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.0747 - regression_loss: 1.5722 - classification_loss: 0.5025\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.0753 - regression_loss: 1.5734 - classification_loss: 0.5019\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.0763 - regression_loss: 1.5740 - classification_loss: 0.5023\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.0772 - regression_loss: 1.5745 - classification_loss: 0.5027\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.0792 - regression_loss: 1.5760 - classification_loss: 0.5031\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.0810 - regression_loss: 1.5787 - classification_loss: 0.5023\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.0873 - regression_loss: 1.5839 - classification_loss: 0.5034\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.0977 - regression_loss: 1.5922 - classification_loss: 0.5056\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1014 - regression_loss: 1.5949 - classification_loss: 0.5064\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1004 - regression_loss: 1.5940 - classification_loss: 0.5064\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.0951 - regression_loss: 1.5897 - classification_loss: 0.5053\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.0969 - regression_loss: 1.5911 - classification_loss: 0.5058\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0941 - regression_loss: 1.5885 - classification_loss: 0.5056\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0966 - regression_loss: 1.5910 - classification_loss: 0.5056\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1044 - regression_loss: 1.5958 - classification_loss: 0.5086\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1056 - regression_loss: 1.5967 - classification_loss: 0.5089\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1007 - regression_loss: 1.5915 - classification_loss: 0.5093\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0997 - regression_loss: 1.5912 - classification_loss: 0.5085\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0925 - regression_loss: 1.5858 - classification_loss: 0.5067\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0942 - regression_loss: 1.5878 - classification_loss: 0.5065\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0927 - regression_loss: 1.5879 - classification_loss: 0.5048\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.0906 - regression_loss: 1.5863 - classification_loss: 0.5044\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.0914 - regression_loss: 1.5865 - classification_loss: 0.5050\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.0949 - regression_loss: 1.5888 - classification_loss: 0.5062\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.0970 - regression_loss: 1.5908 - classification_loss: 0.5062\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0967 - regression_loss: 1.5906 - classification_loss: 0.5061\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0985 - regression_loss: 1.5926 - classification_loss: 0.5059\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0951 - regression_loss: 1.5892 - classification_loss: 0.5059\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0962 - regression_loss: 1.5899 - classification_loss: 0.5063\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0926 - regression_loss: 1.5871 - classification_loss: 0.5055\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0890 - regression_loss: 1.5846 - classification_loss: 0.5044\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0897 - regression_loss: 1.5847 - classification_loss: 0.5050\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0883 - regression_loss: 1.5840 - classification_loss: 0.5044\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0891 - regression_loss: 1.5841 - classification_loss: 0.5050\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0907 - regression_loss: 1.5864 - classification_loss: 0.5044\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0944 - regression_loss: 1.5896 - classification_loss: 0.5049 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0958 - regression_loss: 1.5903 - classification_loss: 0.5056\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0950 - regression_loss: 1.5901 - classification_loss: 0.5048\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0989 - regression_loss: 1.5929 - classification_loss: 0.5060\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1017 - regression_loss: 1.5950 - classification_loss: 0.5066\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0999 - regression_loss: 1.5937 - classification_loss: 0.5063\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0994 - regression_loss: 1.5932 - classification_loss: 0.5062\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0979 - regression_loss: 1.5917 - classification_loss: 0.5061\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1002 - regression_loss: 1.5933 - classification_loss: 0.5069\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1000 - regression_loss: 1.5905 - classification_loss: 0.5094\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1029 - regression_loss: 1.5929 - classification_loss: 0.5100\n",
            "Epoch 94: saving model to ./snapshots\\resnet50_csv_94.h5\n",
            "\n",
            "Epoch 94: ReduceLROnPlateau reducing learning rate to 1.0005271035279195e-43.\n",
            "\n",
            "86/86 [==============================] - 83s 964ms/step - loss: 2.1029 - regression_loss: 1.5929 - classification_loss: 0.5100 - lr: 1.0005e-42\n",
            "Epoch 95/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:54 - loss: 1.9126 - regression_loss: 1.4469 - classification_loss: 0.4656\n",
            " 2/86 [..............................] - ETA: 1:24 - loss: 2.0598 - regression_loss: 1.5814 - classification_loss: 0.4785\n",
            " 3/86 [>.............................] - ETA: 1:22 - loss: 2.1128 - regression_loss: 1.5969 - classification_loss: 0.5160\n",
            " 4/86 [>.............................] - ETA: 1:18 - loss: 2.1323 - regression_loss: 1.6129 - classification_loss: 0.5194\n",
            " 5/86 [>.............................] - ETA: 1:23 - loss: 2.1139 - regression_loss: 1.6021 - classification_loss: 0.5118\n",
            " 6/86 [=>............................] - ETA: 1:18 - loss: 2.1116 - regression_loss: 1.5989 - classification_loss: 0.5127\n",
            " 7/86 [=>............................] - ETA: 1:16 - loss: 2.0652 - regression_loss: 1.5587 - classification_loss: 0.5065\n",
            " 8/86 [=>............................] - ETA: 1:18 - loss: 2.0512 - regression_loss: 1.5428 - classification_loss: 0.5084\n",
            " 9/86 [==>...........................] - ETA: 1:14 - loss: 2.0787 - regression_loss: 1.5651 - classification_loss: 0.5137\n",
            "10/86 [==>...........................] - ETA: 1:12 - loss: 2.1096 - regression_loss: 1.5882 - classification_loss: 0.5214\n",
            "11/86 [==>...........................] - ETA: 1:11 - loss: 2.1146 - regression_loss: 1.5914 - classification_loss: 0.5233\n",
            "12/86 [===>..........................] - ETA: 1:11 - loss: 2.0884 - regression_loss: 1.5736 - classification_loss: 0.5148\n",
            "13/86 [===>..........................] - ETA: 1:11 - loss: 2.0966 - regression_loss: 1.5826 - classification_loss: 0.5139\n",
            "14/86 [===>..........................] - ETA: 1:11 - loss: 2.1123 - regression_loss: 1.5952 - classification_loss: 0.5171\n",
            "15/86 [====>.........................] - ETA: 1:11 - loss: 2.1010 - regression_loss: 1.5893 - classification_loss: 0.5117\n",
            "16/86 [====>.........................] - ETA: 1:10 - loss: 2.0880 - regression_loss: 1.5763 - classification_loss: 0.5117\n",
            "17/86 [====>.........................] - ETA: 1:09 - loss: 2.0771 - regression_loss: 1.5689 - classification_loss: 0.5082\n",
            "18/86 [=====>........................] - ETA: 1:07 - loss: 2.0623 - regression_loss: 1.5549 - classification_loss: 0.5074\n",
            "19/86 [=====>........................] - ETA: 1:06 - loss: 2.0642 - regression_loss: 1.5561 - classification_loss: 0.5081\n",
            "20/86 [=====>........................] - ETA: 1:04 - loss: 2.0698 - regression_loss: 1.5626 - classification_loss: 0.5073\n",
            "21/86 [======>.......................] - ETA: 1:03 - loss: 2.0674 - regression_loss: 1.5607 - classification_loss: 0.5066\n",
            "22/86 [======>.......................] - ETA: 1:03 - loss: 2.0627 - regression_loss: 1.5568 - classification_loss: 0.5058\n",
            "23/86 [=======>......................] - ETA: 1:03 - loss: 2.0666 - regression_loss: 1.5568 - classification_loss: 0.5098\n",
            "24/86 [=======>......................] - ETA: 1:02 - loss: 2.0688 - regression_loss: 1.5587 - classification_loss: 0.5101\n",
            "25/86 [=======>......................] - ETA: 1:01 - loss: 2.0619 - regression_loss: 1.5561 - classification_loss: 0.5058\n",
            "26/86 [========>.....................] - ETA: 1:00 - loss: 2.0536 - regression_loss: 1.5483 - classification_loss: 0.5054\n",
            "27/86 [========>.....................] - ETA: 1:00 - loss: 2.0418 - regression_loss: 1.5401 - classification_loss: 0.5017\n",
            "28/86 [========>.....................] - ETA: 58s - loss: 2.0475 - regression_loss: 1.5438 - classification_loss: 0.5037 \n",
            "29/86 [=========>....................] - ETA: 57s - loss: 2.0417 - regression_loss: 1.5379 - classification_loss: 0.5038\n",
            "30/86 [=========>....................] - ETA: 56s - loss: 2.0389 - regression_loss: 1.5364 - classification_loss: 0.5025\n",
            "31/86 [=========>....................] - ETA: 55s - loss: 2.0440 - regression_loss: 1.5409 - classification_loss: 0.5031\n",
            "32/86 [==========>...................] - ETA: 54s - loss: 2.0352 - regression_loss: 1.5349 - classification_loss: 0.5003\n",
            "33/86 [==========>...................] - ETA: 53s - loss: 2.0375 - regression_loss: 1.5372 - classification_loss: 0.5003\n",
            "34/86 [==========>...................] - ETA: 52s - loss: 2.0358 - regression_loss: 1.5364 - classification_loss: 0.4994\n",
            "35/86 [===========>..................] - ETA: 51s - loss: 2.0369 - regression_loss: 1.5382 - classification_loss: 0.4987\n",
            "36/86 [===========>..................] - ETA: 50s - loss: 2.0333 - regression_loss: 1.5367 - classification_loss: 0.4966\n",
            "37/86 [===========>..................] - ETA: 49s - loss: 2.0361 - regression_loss: 1.5397 - classification_loss: 0.4964\n",
            "38/86 [============>.................] - ETA: 48s - loss: 2.0334 - regression_loss: 1.5380 - classification_loss: 0.4955\n",
            "39/86 [============>.................] - ETA: 47s - loss: 2.0207 - regression_loss: 1.5279 - classification_loss: 0.4928\n",
            "40/86 [============>.................] - ETA: 45s - loss: 2.0267 - regression_loss: 1.5325 - classification_loss: 0.4942\n",
            "41/86 [=============>................] - ETA: 45s - loss: 2.0295 - regression_loss: 1.5337 - classification_loss: 0.4958\n",
            "42/86 [=============>................] - ETA: 43s - loss: 2.0331 - regression_loss: 1.5362 - classification_loss: 0.4968\n",
            "43/86 [==============>...............] - ETA: 42s - loss: 2.0325 - regression_loss: 1.5365 - classification_loss: 0.4960\n",
            "44/86 [==============>...............] - ETA: 41s - loss: 2.0321 - regression_loss: 1.5361 - classification_loss: 0.4960\n",
            "45/86 [==============>...............] - ETA: 40s - loss: 2.0379 - regression_loss: 1.5410 - classification_loss: 0.4969\n",
            "46/86 [===============>..............] - ETA: 39s - loss: 2.0395 - regression_loss: 1.5423 - classification_loss: 0.4972\n",
            "47/86 [===============>..............] - ETA: 38s - loss: 2.0483 - regression_loss: 1.5490 - classification_loss: 0.4993\n",
            "48/86 [===============>..............] - ETA: 37s - loss: 2.0482 - regression_loss: 1.5482 - classification_loss: 0.5001\n",
            "49/86 [================>.............] - ETA: 36s - loss: 2.0405 - regression_loss: 1.5425 - classification_loss: 0.4980\n",
            "50/86 [================>.............] - ETA: 35s - loss: 2.0414 - regression_loss: 1.5435 - classification_loss: 0.4979\n",
            "51/86 [================>.............] - ETA: 34s - loss: 2.0399 - regression_loss: 1.5421 - classification_loss: 0.4978\n",
            "52/86 [=================>............] - ETA: 33s - loss: 2.0382 - regression_loss: 1.5411 - classification_loss: 0.4971\n",
            "53/86 [=================>............] - ETA: 32s - loss: 2.0398 - regression_loss: 1.5442 - classification_loss: 0.4957\n",
            "54/86 [=================>............] - ETA: 31s - loss: 2.0413 - regression_loss: 1.5442 - classification_loss: 0.4971\n",
            "55/86 [==================>...........] - ETA: 30s - loss: 2.0469 - regression_loss: 1.5482 - classification_loss: 0.4987\n",
            "56/86 [==================>...........] - ETA: 29s - loss: 2.0446 - regression_loss: 1.5469 - classification_loss: 0.4977\n",
            "57/86 [==================>...........] - ETA: 28s - loss: 2.0444 - regression_loss: 1.5474 - classification_loss: 0.4971\n",
            "58/86 [===================>..........] - ETA: 27s - loss: 2.0434 - regression_loss: 1.5469 - classification_loss: 0.4964\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.0399 - regression_loss: 1.5429 - classification_loss: 0.4970\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.0427 - regression_loss: 1.5447 - classification_loss: 0.4981\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.0443 - regression_loss: 1.5458 - classification_loss: 0.4985\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.0434 - regression_loss: 1.5451 - classification_loss: 0.4983\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.0440 - regression_loss: 1.5453 - classification_loss: 0.4987\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.0423 - regression_loss: 1.5449 - classification_loss: 0.4974\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.0457 - regression_loss: 1.5476 - classification_loss: 0.4982\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0420 - regression_loss: 1.5452 - classification_loss: 0.4968\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0460 - regression_loss: 1.5487 - classification_loss: 0.4974\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0501 - regression_loss: 1.5514 - classification_loss: 0.4987\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0588 - regression_loss: 1.5586 - classification_loss: 0.5002\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0587 - regression_loss: 1.5590 - classification_loss: 0.4997\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0586 - regression_loss: 1.5594 - classification_loss: 0.4992\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0606 - regression_loss: 1.5612 - classification_loss: 0.4994\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0616 - regression_loss: 1.5623 - classification_loss: 0.4993\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0590 - regression_loss: 1.5601 - classification_loss: 0.4989\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0592 - regression_loss: 1.5609 - classification_loss: 0.4983\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0652 - regression_loss: 1.5656 - classification_loss: 0.4996 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0670 - regression_loss: 1.5673 - classification_loss: 0.4997\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0677 - regression_loss: 1.5681 - classification_loss: 0.4997\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0685 - regression_loss: 1.5695 - classification_loss: 0.4990\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0710 - regression_loss: 1.5717 - classification_loss: 0.4993\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0747 - regression_loss: 1.5745 - classification_loss: 0.5002\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0740 - regression_loss: 1.5736 - classification_loss: 0.5005\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0738 - regression_loss: 1.5733 - classification_loss: 0.5006\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0742 - regression_loss: 1.5734 - classification_loss: 0.5008\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0727 - regression_loss: 1.5731 - classification_loss: 0.4995\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0761 - regression_loss: 1.5745 - classification_loss: 0.5016\n",
            "Epoch 95: saving model to ./snapshots\\resnet50_csv_95.h5\n",
            "\n",
            "86/86 [==============================] - 84s 970ms/step - loss: 2.0761 - regression_loss: 1.5745 - classification_loss: 0.5016 - lr: 9.9492e-44\n",
            "Epoch 96/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:52 - loss: 2.0262 - regression_loss: 1.6044 - classification_loss: 0.4218\n",
            " 2/86 [..............................] - ETA: 1:12 - loss: 2.0987 - regression_loss: 1.6212 - classification_loss: 0.4776\n",
            " 3/86 [>.............................] - ETA: 1:15 - loss: 2.1449 - regression_loss: 1.6677 - classification_loss: 0.4772\n",
            " 4/86 [>.............................] - ETA: 1:23 - loss: 2.1350 - regression_loss: 1.6525 - classification_loss: 0.4824\n",
            " 5/86 [>.............................] - ETA: 1:17 - loss: 2.1801 - regression_loss: 1.6777 - classification_loss: 0.5024\n",
            " 6/86 [=>............................] - ETA: 1:15 - loss: 2.1604 - regression_loss: 1.6210 - classification_loss: 0.5394\n",
            " 7/86 [=>............................] - ETA: 1:12 - loss: 2.1582 - regression_loss: 1.6218 - classification_loss: 0.5364\n",
            " 8/86 [=>............................] - ETA: 1:10 - loss: 2.2132 - regression_loss: 1.6696 - classification_loss: 0.5436\n",
            " 9/86 [==>...........................] - ETA: 1:10 - loss: 2.1985 - regression_loss: 1.6531 - classification_loss: 0.5454\n",
            "10/86 [==>...........................] - ETA: 1:08 - loss: 2.1903 - regression_loss: 1.6501 - classification_loss: 0.5401\n",
            "11/86 [==>...........................] - ETA: 1:09 - loss: 2.1975 - regression_loss: 1.6592 - classification_loss: 0.5382\n",
            "12/86 [===>..........................] - ETA: 1:07 - loss: 2.1880 - regression_loss: 1.6481 - classification_loss: 0.5399\n",
            "13/86 [===>..........................] - ETA: 1:08 - loss: 2.1839 - regression_loss: 1.6434 - classification_loss: 0.5406\n",
            "14/86 [===>..........................] - ETA: 1:07 - loss: 2.2018 - regression_loss: 1.6583 - classification_loss: 0.5435\n",
            "15/86 [====>.........................] - ETA: 1:08 - loss: 2.1926 - regression_loss: 1.6523 - classification_loss: 0.5403\n",
            "16/86 [====>.........................] - ETA: 1:06 - loss: 2.1975 - regression_loss: 1.6557 - classification_loss: 0.5418\n",
            "17/86 [====>.........................] - ETA: 1:07 - loss: 2.1932 - regression_loss: 1.6510 - classification_loss: 0.5422\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.1986 - regression_loss: 1.6556 - classification_loss: 0.5430\n",
            "19/86 [=====>........................] - ETA: 1:03 - loss: 2.1916 - regression_loss: 1.6486 - classification_loss: 0.5430\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.1877 - regression_loss: 1.6448 - classification_loss: 0.5430\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.1779 - regression_loss: 1.6402 - classification_loss: 0.5377\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.1623 - regression_loss: 1.6301 - classification_loss: 0.5322\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.1653 - regression_loss: 1.6322 - classification_loss: 0.5331\n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.1495 - regression_loss: 1.6201 - classification_loss: 0.5294 \n",
            "25/86 [=======>......................] - ETA: 59s - loss: 2.1364 - regression_loss: 1.6097 - classification_loss: 0.5267\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.1242 - regression_loss: 1.6007 - classification_loss: 0.5235\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.1214 - regression_loss: 1.5987 - classification_loss: 0.5227\n",
            "28/86 [========>.....................] - ETA: 56s - loss: 2.1299 - regression_loss: 1.6062 - classification_loss: 0.5236\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.1288 - regression_loss: 1.6045 - classification_loss: 0.5243\n",
            "30/86 [=========>....................] - ETA: 54s - loss: 2.1364 - regression_loss: 1.6115 - classification_loss: 0.5249\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.1428 - regression_loss: 1.6168 - classification_loss: 0.5260\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.1340 - regression_loss: 1.6102 - classification_loss: 0.5238\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.1333 - regression_loss: 1.6107 - classification_loss: 0.5226\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.1375 - regression_loss: 1.6142 - classification_loss: 0.5233\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.1317 - regression_loss: 1.6102 - classification_loss: 0.5215\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.1327 - regression_loss: 1.6104 - classification_loss: 0.5223\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.1311 - regression_loss: 1.6096 - classification_loss: 0.5215\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.1429 - regression_loss: 1.6198 - classification_loss: 0.5231\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.1447 - regression_loss: 1.6220 - classification_loss: 0.5227\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.1447 - regression_loss: 1.6198 - classification_loss: 0.5249\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.1346 - regression_loss: 1.6129 - classification_loss: 0.5217\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1392 - regression_loss: 1.6143 - classification_loss: 0.5248\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1406 - regression_loss: 1.6146 - classification_loss: 0.5260\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1356 - regression_loss: 1.6110 - classification_loss: 0.5245\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.1347 - regression_loss: 1.6117 - classification_loss: 0.5230\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1297 - regression_loss: 1.6065 - classification_loss: 0.5232\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1254 - regression_loss: 1.6039 - classification_loss: 0.5216\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1267 - regression_loss: 1.6033 - classification_loss: 0.5234\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1199 - regression_loss: 1.5994 - classification_loss: 0.5205\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1164 - regression_loss: 1.5968 - classification_loss: 0.5196\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1172 - regression_loss: 1.5978 - classification_loss: 0.5194\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1135 - regression_loss: 1.5951 - classification_loss: 0.5184\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1152 - regression_loss: 1.5967 - classification_loss: 0.5185\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1180 - regression_loss: 1.5984 - classification_loss: 0.5196\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1138 - regression_loss: 1.5942 - classification_loss: 0.5196\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1157 - regression_loss: 1.5968 - classification_loss: 0.5189\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1147 - regression_loss: 1.5964 - classification_loss: 0.5184\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1197 - regression_loss: 1.5998 - classification_loss: 0.5199\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1287 - regression_loss: 1.6071 - classification_loss: 0.5216\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1356 - regression_loss: 1.6127 - classification_loss: 0.5229\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1370 - regression_loss: 1.6139 - classification_loss: 0.5231\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1373 - regression_loss: 1.6133 - classification_loss: 0.5240\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1425 - regression_loss: 1.6164 - classification_loss: 0.5261\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1359 - regression_loss: 1.6116 - classification_loss: 0.5243\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1315 - regression_loss: 1.6078 - classification_loss: 0.5237\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1264 - regression_loss: 1.6044 - classification_loss: 0.5220\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1278 - regression_loss: 1.6052 - classification_loss: 0.5226\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1276 - regression_loss: 1.6043 - classification_loss: 0.5233\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1275 - regression_loss: 1.6046 - classification_loss: 0.5229\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1249 - regression_loss: 1.6029 - classification_loss: 0.5220\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1214 - regression_loss: 1.6006 - classification_loss: 0.5208\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1239 - regression_loss: 1.6031 - classification_loss: 0.5209\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1241 - regression_loss: 1.6037 - classification_loss: 0.5204\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1221 - regression_loss: 1.6023 - classification_loss: 0.5198\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1221 - regression_loss: 1.6022 - classification_loss: 0.5200\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1237 - regression_loss: 1.6042 - classification_loss: 0.5195 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1232 - regression_loss: 1.6042 - classification_loss: 0.5190\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1230 - regression_loss: 1.6038 - classification_loss: 0.5193\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1159 - regression_loss: 1.5987 - classification_loss: 0.5172\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1155 - regression_loss: 1.5985 - classification_loss: 0.5170\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1175 - regression_loss: 1.6001 - classification_loss: 0.5174\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1164 - regression_loss: 1.5992 - classification_loss: 0.5173\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1158 - regression_loss: 1.5978 - classification_loss: 0.5179\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1161 - regression_loss: 1.5977 - classification_loss: 0.5183\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1175 - regression_loss: 1.5982 - classification_loss: 0.5194\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1176 - regression_loss: 1.5981 - classification_loss: 0.5194\n",
            "Epoch 96: saving model to ./snapshots\\resnet50_csv_96.h5\n",
            "\n",
            "Epoch 96: ReduceLROnPlateau reducing learning rate to 9.949219096706202e-45.\n",
            "\n",
            "86/86 [==============================] - 83s 958ms/step - loss: 2.1176 - regression_loss: 1.5981 - classification_loss: 0.5194 - lr: 9.9492e-44\n",
            "Epoch 97/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 1:53 - loss: 1.4602 - regression_loss: 1.1193 - classification_loss: 0.3409\n",
            " 2/86 [..............................] - ETA: 1:13 - loss: 1.8382 - regression_loss: 1.4087 - classification_loss: 0.4295\n",
            " 3/86 [>.............................] - ETA: 1:28 - loss: 1.9362 - regression_loss: 1.4722 - classification_loss: 0.4640\n",
            " 4/86 [>.............................] - ETA: 1:28 - loss: 2.0131 - regression_loss: 1.5096 - classification_loss: 0.5035\n",
            " 5/86 [>.............................] - ETA: 1:23 - loss: 1.9687 - regression_loss: 1.4690 - classification_loss: 0.4997\n",
            " 6/86 [=>............................] - ETA: 1:20 - loss: 1.9820 - regression_loss: 1.4895 - classification_loss: 0.4925\n",
            " 7/86 [=>............................] - ETA: 1:17 - loss: 2.0098 - regression_loss: 1.5128 - classification_loss: 0.4970\n",
            " 8/86 [=>............................] - ETA: 1:15 - loss: 1.9944 - regression_loss: 1.5023 - classification_loss: 0.4921\n",
            " 9/86 [==>...........................] - ETA: 1:13 - loss: 2.0082 - regression_loss: 1.5129 - classification_loss: 0.4954\n",
            "10/86 [==>...........................] - ETA: 1:15 - loss: 2.0258 - regression_loss: 1.5287 - classification_loss: 0.4971\n",
            "11/86 [==>...........................] - ETA: 1:14 - loss: 2.0267 - regression_loss: 1.5319 - classification_loss: 0.4947\n",
            "12/86 [===>..........................] - ETA: 1:14 - loss: 2.0201 - regression_loss: 1.5307 - classification_loss: 0.4894\n",
            "13/86 [===>..........................] - ETA: 1:12 - loss: 2.0556 - regression_loss: 1.5489 - classification_loss: 0.5067\n",
            "14/86 [===>..........................] - ETA: 1:12 - loss: 2.0663 - regression_loss: 1.5620 - classification_loss: 0.5043\n",
            "15/86 [====>.........................] - ETA: 1:09 - loss: 2.0717 - regression_loss: 1.5704 - classification_loss: 0.5012\n",
            "16/86 [====>.........................] - ETA: 1:08 - loss: 2.0743 - regression_loss: 1.5715 - classification_loss: 0.5028\n",
            "17/86 [====>.........................] - ETA: 1:06 - loss: 2.0820 - regression_loss: 1.5759 - classification_loss: 0.5061\n",
            "18/86 [=====>........................] - ETA: 1:06 - loss: 2.0812 - regression_loss: 1.5772 - classification_loss: 0.5040\n",
            "19/86 [=====>........................] - ETA: 1:06 - loss: 2.0867 - regression_loss: 1.5813 - classification_loss: 0.5053\n",
            "20/86 [=====>........................] - ETA: 1:04 - loss: 2.0880 - regression_loss: 1.5791 - classification_loss: 0.5089\n",
            "21/86 [======>.......................] - ETA: 1:04 - loss: 2.0982 - regression_loss: 1.5836 - classification_loss: 0.5146\n",
            "22/86 [======>.......................] - ETA: 1:03 - loss: 2.0870 - regression_loss: 1.5751 - classification_loss: 0.5119\n",
            "23/86 [=======>......................] - ETA: 1:03 - loss: 2.0990 - regression_loss: 1.5870 - classification_loss: 0.5120\n",
            "24/86 [=======>......................] - ETA: 1:01 - loss: 2.0956 - regression_loss: 1.5870 - classification_loss: 0.5086\n",
            "25/86 [=======>......................] - ETA: 1:00 - loss: 2.0931 - regression_loss: 1.5848 - classification_loss: 0.5083\n",
            "26/86 [========>.....................] - ETA: 59s - loss: 2.0901 - regression_loss: 1.5840 - classification_loss: 0.5060 \n",
            "27/86 [========>.....................] - ETA: 57s - loss: 2.0801 - regression_loss: 1.5769 - classification_loss: 0.5031\n",
            "28/86 [========>.....................] - ETA: 56s - loss: 2.0760 - regression_loss: 1.5730 - classification_loss: 0.5030\n",
            "29/86 [=========>....................] - ETA: 55s - loss: 2.0742 - regression_loss: 1.5718 - classification_loss: 0.5025\n",
            "30/86 [=========>....................] - ETA: 54s - loss: 2.0645 - regression_loss: 1.5647 - classification_loss: 0.4998\n",
            "31/86 [=========>....................] - ETA: 53s - loss: 2.0750 - regression_loss: 1.5719 - classification_loss: 0.5031\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.0776 - regression_loss: 1.5725 - classification_loss: 0.5051\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.0744 - regression_loss: 1.5697 - classification_loss: 0.5047\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.0678 - regression_loss: 1.5635 - classification_loss: 0.5043\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.0674 - regression_loss: 1.5651 - classification_loss: 0.5023\n",
            "36/86 [===========>..................] - ETA: 48s - loss: 2.0644 - regression_loss: 1.5617 - classification_loss: 0.5027\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.0764 - regression_loss: 1.5714 - classification_loss: 0.5050\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.0776 - regression_loss: 1.5719 - classification_loss: 0.5057\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.0813 - regression_loss: 1.5763 - classification_loss: 0.5050\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.0789 - regression_loss: 1.5735 - classification_loss: 0.5054\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.0774 - regression_loss: 1.5730 - classification_loss: 0.5044\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.0826 - regression_loss: 1.5764 - classification_loss: 0.5062\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.0859 - regression_loss: 1.5794 - classification_loss: 0.5065\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.0849 - regression_loss: 1.5780 - classification_loss: 0.5069\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.0849 - regression_loss: 1.5771 - classification_loss: 0.5078\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.0765 - regression_loss: 1.5715 - classification_loss: 0.5050\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.0826 - regression_loss: 1.5769 - classification_loss: 0.5057\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.0909 - regression_loss: 1.5831 - classification_loss: 0.5078\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.0860 - regression_loss: 1.5782 - classification_loss: 0.5078\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.0915 - regression_loss: 1.5820 - classification_loss: 0.5094\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.0925 - regression_loss: 1.5827 - classification_loss: 0.5098\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.0945 - regression_loss: 1.5845 - classification_loss: 0.5100\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0930 - regression_loss: 1.5843 - classification_loss: 0.5087\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.0954 - regression_loss: 1.5860 - classification_loss: 0.5094\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.0944 - regression_loss: 1.5845 - classification_loss: 0.5100\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.0878 - regression_loss: 1.5801 - classification_loss: 0.5077\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.0850 - regression_loss: 1.5776 - classification_loss: 0.5073\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.0840 - regression_loss: 1.5762 - classification_loss: 0.5078\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.0868 - regression_loss: 1.5779 - classification_loss: 0.5089\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.0854 - regression_loss: 1.5766 - classification_loss: 0.5088\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.0865 - regression_loss: 1.5776 - classification_loss: 0.5088\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.0869 - regression_loss: 1.5784 - classification_loss: 0.5085\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.0901 - regression_loss: 1.5809 - classification_loss: 0.5092\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.0923 - regression_loss: 1.5832 - classification_loss: 0.5091\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.0881 - regression_loss: 1.5807 - classification_loss: 0.5073\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.0846 - regression_loss: 1.5780 - classification_loss: 0.5065\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.0840 - regression_loss: 1.5774 - classification_loss: 0.5066\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.0861 - regression_loss: 1.5792 - classification_loss: 0.5069\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.0865 - regression_loss: 1.5795 - classification_loss: 0.5070\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.0826 - regression_loss: 1.5766 - classification_loss: 0.5060\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.0885 - regression_loss: 1.5813 - classification_loss: 0.5072\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.0863 - regression_loss: 1.5787 - classification_loss: 0.5076\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.0871 - regression_loss: 1.5798 - classification_loss: 0.5073\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0839 - regression_loss: 1.5773 - classification_loss: 0.5066\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0832 - regression_loss: 1.5759 - classification_loss: 0.5073\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0870 - regression_loss: 1.5790 - classification_loss: 0.5080 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0811 - regression_loss: 1.5748 - classification_loss: 0.5064\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0818 - regression_loss: 1.5757 - classification_loss: 0.5061\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0835 - regression_loss: 1.5778 - classification_loss: 0.5057\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0842 - regression_loss: 1.5783 - classification_loss: 0.5060\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0850 - regression_loss: 1.5798 - classification_loss: 0.5052\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0785 - regression_loss: 1.5748 - classification_loss: 0.5037\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0789 - regression_loss: 1.5746 - classification_loss: 0.5043\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0820 - regression_loss: 1.5772 - classification_loss: 0.5048\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0828 - regression_loss: 1.5776 - classification_loss: 0.5052\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0813 - regression_loss: 1.5760 - classification_loss: 0.5053\n",
            "Epoch 97: saving model to ./snapshots\\resnet50_csv_97.h5\n",
            "\n",
            "86/86 [==============================] - 83s 957ms/step - loss: 2.0813 - regression_loss: 1.5760 - classification_loss: 0.5053 - lr: 9.8091e-45\n",
            "Epoch 98/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:23 - loss: 2.0733 - regression_loss: 1.5477 - classification_loss: 0.5256\n",
            " 2/86 [..............................] - ETA: 1:18 - loss: 2.3546 - regression_loss: 1.7822 - classification_loss: 0.5723\n",
            " 3/86 [>.............................] - ETA: 1:13 - loss: 2.3068 - regression_loss: 1.7409 - classification_loss: 0.5659\n",
            " 4/86 [>.............................] - ETA: 1:11 - loss: 2.2659 - regression_loss: 1.7107 - classification_loss: 0.5551\n",
            " 5/86 [>.............................] - ETA: 1:12 - loss: 2.1803 - regression_loss: 1.6341 - classification_loss: 0.5461\n",
            " 6/86 [=>............................] - ETA: 1:16 - loss: 2.1598 - regression_loss: 1.6297 - classification_loss: 0.5302\n",
            " 7/86 [=>............................] - ETA: 1:13 - loss: 2.1462 - regression_loss: 1.6223 - classification_loss: 0.5238\n",
            " 8/86 [=>............................] - ETA: 1:10 - loss: 2.1287 - regression_loss: 1.6087 - classification_loss: 0.5200\n",
            " 9/86 [==>...........................] - ETA: 1:09 - loss: 2.1433 - regression_loss: 1.6303 - classification_loss: 0.5130\n",
            "10/86 [==>...........................] - ETA: 1:09 - loss: 2.1500 - regression_loss: 1.6314 - classification_loss: 0.5186\n",
            "11/86 [==>...........................] - ETA: 1:11 - loss: 2.1510 - regression_loss: 1.6342 - classification_loss: 0.5168\n",
            "12/86 [===>..........................] - ETA: 1:10 - loss: 2.1495 - regression_loss: 1.6351 - classification_loss: 0.5144\n",
            "13/86 [===>..........................] - ETA: 1:09 - loss: 2.1465 - regression_loss: 1.6381 - classification_loss: 0.5084\n",
            "14/86 [===>..........................] - ETA: 1:07 - loss: 2.1521 - regression_loss: 1.6384 - classification_loss: 0.5137\n",
            "15/86 [====>.........................] - ETA: 1:05 - loss: 2.1592 - regression_loss: 1.6416 - classification_loss: 0.5176\n",
            "16/86 [====>.........................] - ETA: 1:06 - loss: 2.1559 - regression_loss: 1.6415 - classification_loss: 0.5145\n",
            "17/86 [====>.........................] - ETA: 1:06 - loss: 2.1600 - regression_loss: 1.6402 - classification_loss: 0.5198\n",
            "18/86 [=====>........................] - ETA: 1:04 - loss: 2.1561 - regression_loss: 1.6355 - classification_loss: 0.5205\n",
            "19/86 [=====>........................] - ETA: 1:03 - loss: 2.1758 - regression_loss: 1.6515 - classification_loss: 0.5244\n",
            "20/86 [=====>........................] - ETA: 1:03 - loss: 2.1679 - regression_loss: 1.6456 - classification_loss: 0.5223\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.1665 - regression_loss: 1.6446 - classification_loss: 0.5219\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.1697 - regression_loss: 1.6449 - classification_loss: 0.5247\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.1744 - regression_loss: 1.6481 - classification_loss: 0.5263\n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.1800 - regression_loss: 1.6509 - classification_loss: 0.5291 \n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.1743 - regression_loss: 1.6453 - classification_loss: 0.5289\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.1862 - regression_loss: 1.6521 - classification_loss: 0.5342\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.1738 - regression_loss: 1.6434 - classification_loss: 0.5304\n",
            "28/86 [========>.....................] - ETA: 55s - loss: 2.1750 - regression_loss: 1.6458 - classification_loss: 0.5292\n",
            "29/86 [=========>....................] - ETA: 54s - loss: 2.1731 - regression_loss: 1.6452 - classification_loss: 0.5279\n",
            "30/86 [=========>....................] - ETA: 53s - loss: 2.1697 - regression_loss: 1.6419 - classification_loss: 0.5278\n",
            "31/86 [=========>....................] - ETA: 52s - loss: 2.1745 - regression_loss: 1.6434 - classification_loss: 0.5311\n",
            "32/86 [==========>...................] - ETA: 51s - loss: 2.1731 - regression_loss: 1.6428 - classification_loss: 0.5304\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.1679 - regression_loss: 1.6384 - classification_loss: 0.5295\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.1661 - regression_loss: 1.6375 - classification_loss: 0.5286\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.1652 - regression_loss: 1.6359 - classification_loss: 0.5292\n",
            "36/86 [===========>..................] - ETA: 48s - loss: 2.1723 - regression_loss: 1.6416 - classification_loss: 0.5308\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.1649 - regression_loss: 1.6342 - classification_loss: 0.5308\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.1662 - regression_loss: 1.6333 - classification_loss: 0.5329\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.1719 - regression_loss: 1.6382 - classification_loss: 0.5336\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.1704 - regression_loss: 1.6373 - classification_loss: 0.5331\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.1747 - regression_loss: 1.6414 - classification_loss: 0.5333\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1712 - regression_loss: 1.6395 - classification_loss: 0.5317\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1663 - regression_loss: 1.6378 - classification_loss: 0.5285\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1658 - regression_loss: 1.6359 - classification_loss: 0.5298\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.1662 - regression_loss: 1.6362 - classification_loss: 0.5299\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1651 - regression_loss: 1.6364 - classification_loss: 0.5287\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1631 - regression_loss: 1.6367 - classification_loss: 0.5263\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.1631 - regression_loss: 1.6371 - classification_loss: 0.5259\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1625 - regression_loss: 1.6379 - classification_loss: 0.5246\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1645 - regression_loss: 1.6384 - classification_loss: 0.5261\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1602 - regression_loss: 1.6347 - classification_loss: 0.5255\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1577 - regression_loss: 1.6336 - classification_loss: 0.5240\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1519 - regression_loss: 1.6297 - classification_loss: 0.5221\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1548 - regression_loss: 1.6318 - classification_loss: 0.5230\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1521 - regression_loss: 1.6289 - classification_loss: 0.5232\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1462 - regression_loss: 1.6238 - classification_loss: 0.5224\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1479 - regression_loss: 1.6249 - classification_loss: 0.5230\n",
            "58/86 [===================>..........] - ETA: 27s - loss: 2.1449 - regression_loss: 1.6226 - classification_loss: 0.5224\n",
            "59/86 [===================>..........] - ETA: 26s - loss: 2.1429 - regression_loss: 1.6212 - classification_loss: 0.5216\n",
            "60/86 [===================>..........] - ETA: 25s - loss: 2.1448 - regression_loss: 1.6227 - classification_loss: 0.5221\n",
            "61/86 [====================>.........] - ETA: 24s - loss: 2.1466 - regression_loss: 1.6242 - classification_loss: 0.5224\n",
            "62/86 [====================>.........] - ETA: 23s - loss: 2.1444 - regression_loss: 1.6225 - classification_loss: 0.5219\n",
            "63/86 [====================>.........] - ETA: 22s - loss: 2.1432 - regression_loss: 1.6214 - classification_loss: 0.5219\n",
            "64/86 [=====================>........] - ETA: 21s - loss: 2.1342 - regression_loss: 1.6147 - classification_loss: 0.5195\n",
            "65/86 [=====================>........] - ETA: 20s - loss: 2.1347 - regression_loss: 1.6156 - classification_loss: 0.5191\n",
            "66/86 [======================>.......] - ETA: 19s - loss: 2.1328 - regression_loss: 1.6140 - classification_loss: 0.5187\n",
            "67/86 [======================>.......] - ETA: 18s - loss: 2.1320 - regression_loss: 1.6132 - classification_loss: 0.5189\n",
            "68/86 [======================>.......] - ETA: 17s - loss: 2.1347 - regression_loss: 1.6153 - classification_loss: 0.5193\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1314 - regression_loss: 1.6131 - classification_loss: 0.5183\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1218 - regression_loss: 1.6061 - classification_loss: 0.5157\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1196 - regression_loss: 1.6045 - classification_loss: 0.5151\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1188 - regression_loss: 1.6042 - classification_loss: 0.5146\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1184 - regression_loss: 1.6043 - classification_loss: 0.5141\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1188 - regression_loss: 1.6048 - classification_loss: 0.5139\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1181 - regression_loss: 1.6043 - classification_loss: 0.5139\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.1220 - regression_loss: 1.6077 - classification_loss: 0.5143 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.1213 - regression_loss: 1.6070 - classification_loss: 0.5143\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1244 - regression_loss: 1.6096 - classification_loss: 0.5147\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.1255 - regression_loss: 1.6113 - classification_loss: 0.5143\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.1303 - regression_loss: 1.6151 - classification_loss: 0.5152\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.1331 - regression_loss: 1.6161 - classification_loss: 0.5170\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.1330 - regression_loss: 1.6157 - classification_loss: 0.5173\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.1353 - regression_loss: 1.6172 - classification_loss: 0.5181\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.1356 - regression_loss: 1.6176 - classification_loss: 0.5180\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1360 - regression_loss: 1.6182 - classification_loss: 0.5178\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1393 - regression_loss: 1.6207 - classification_loss: 0.5186\n",
            "Epoch 98: saving model to ./snapshots\\resnet50_csv_98.h5\n",
            "\n",
            "Epoch 98: ReduceLROnPlateau reducing learning rate to 9.80908925027372e-46.\n",
            "\n",
            "86/86 [==============================] - 83s 960ms/step - loss: 2.1393 - regression_loss: 1.6207 - classification_loss: 0.5186 - lr: 9.8091e-45\n",
            "Epoch 99/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:09 - loss: 2.2462 - regression_loss: 1.6929 - classification_loss: 0.5533\n",
            " 2/86 [..............................] - ETA: 1:42 - loss: 2.1935 - regression_loss: 1.6476 - classification_loss: 0.5460\n",
            " 3/86 [>.............................] - ETA: 1:39 - loss: 2.1558 - regression_loss: 1.6121 - classification_loss: 0.5437\n",
            " 4/86 [>.............................] - ETA: 1:25 - loss: 2.2299 - regression_loss: 1.6707 - classification_loss: 0.5592\n",
            " 5/86 [>.............................] - ETA: 1:24 - loss: 2.2505 - regression_loss: 1.6772 - classification_loss: 0.5733\n",
            " 6/86 [=>............................] - ETA: 1:18 - loss: 2.2556 - regression_loss: 1.6838 - classification_loss: 0.5718\n",
            " 7/86 [=>............................] - ETA: 1:16 - loss: 2.2565 - regression_loss: 1.6904 - classification_loss: 0.5661\n",
            " 8/86 [=>............................] - ETA: 1:15 - loss: 2.2491 - regression_loss: 1.6868 - classification_loss: 0.5623\n",
            " 9/86 [==>...........................] - ETA: 1:14 - loss: 2.2554 - regression_loss: 1.6935 - classification_loss: 0.5619\n",
            "10/86 [==>...........................] - ETA: 1:14 - loss: 2.2241 - regression_loss: 1.6756 - classification_loss: 0.5485\n",
            "11/86 [==>...........................] - ETA: 1:11 - loss: 2.2048 - regression_loss: 1.6568 - classification_loss: 0.5480\n",
            "12/86 [===>..........................] - ETA: 1:10 - loss: 2.1985 - regression_loss: 1.6511 - classification_loss: 0.5474\n",
            "13/86 [===>..........................] - ETA: 1:08 - loss: 2.1908 - regression_loss: 1.6467 - classification_loss: 0.5441\n",
            "14/86 [===>..........................] - ETA: 1:07 - loss: 2.1680 - regression_loss: 1.6270 - classification_loss: 0.5411\n",
            "15/86 [====>.........................] - ETA: 1:06 - loss: 2.1562 - regression_loss: 1.6205 - classification_loss: 0.5357\n",
            "16/86 [====>.........................] - ETA: 1:07 - loss: 2.1467 - regression_loss: 1.6150 - classification_loss: 0.5317\n",
            "17/86 [====>.........................] - ETA: 1:06 - loss: 2.1512 - regression_loss: 1.6192 - classification_loss: 0.5320\n",
            "18/86 [=====>........................] - ETA: 1:05 - loss: 2.1518 - regression_loss: 1.6191 - classification_loss: 0.5327\n",
            "19/86 [=====>........................] - ETA: 1:03 - loss: 2.1607 - regression_loss: 1.6216 - classification_loss: 0.5391\n",
            "20/86 [=====>........................] - ETA: 1:02 - loss: 2.1647 - regression_loss: 1.6234 - classification_loss: 0.5413\n",
            "21/86 [======>.......................] - ETA: 1:02 - loss: 2.1615 - regression_loss: 1.6203 - classification_loss: 0.5412\n",
            "22/86 [======>.......................] - ETA: 1:01 - loss: 2.1532 - regression_loss: 1.6171 - classification_loss: 0.5360\n",
            "23/86 [=======>......................] - ETA: 1:00 - loss: 2.1514 - regression_loss: 1.6178 - classification_loss: 0.5336\n",
            "24/86 [=======>......................] - ETA: 59s - loss: 2.1458 - regression_loss: 1.6134 - classification_loss: 0.5324 \n",
            "25/86 [=======>......................] - ETA: 58s - loss: 2.1433 - regression_loss: 1.6132 - classification_loss: 0.5301\n",
            "26/86 [========>.....................] - ETA: 57s - loss: 2.1439 - regression_loss: 1.6137 - classification_loss: 0.5302\n",
            "27/86 [========>.....................] - ETA: 56s - loss: 2.1364 - regression_loss: 1.6087 - classification_loss: 0.5277\n",
            "28/86 [========>.....................] - ETA: 54s - loss: 2.1367 - regression_loss: 1.6095 - classification_loss: 0.5272\n",
            "29/86 [=========>....................] - ETA: 53s - loss: 2.1336 - regression_loss: 1.6085 - classification_loss: 0.5252\n",
            "30/86 [=========>....................] - ETA: 52s - loss: 2.1315 - regression_loss: 1.6053 - classification_loss: 0.5261\n",
            "31/86 [=========>....................] - ETA: 51s - loss: 2.1386 - regression_loss: 1.6097 - classification_loss: 0.5289\n",
            "32/86 [==========>...................] - ETA: 50s - loss: 2.1514 - regression_loss: 1.6183 - classification_loss: 0.5331\n",
            "33/86 [==========>...................] - ETA: 50s - loss: 2.1452 - regression_loss: 1.6152 - classification_loss: 0.5300\n",
            "34/86 [==========>...................] - ETA: 49s - loss: 2.1374 - regression_loss: 1.6102 - classification_loss: 0.5271\n",
            "35/86 [===========>..................] - ETA: 48s - loss: 2.1422 - regression_loss: 1.6134 - classification_loss: 0.5288\n",
            "36/86 [===========>..................] - ETA: 47s - loss: 2.1345 - regression_loss: 1.6090 - classification_loss: 0.5255\n",
            "37/86 [===========>..................] - ETA: 46s - loss: 2.1342 - regression_loss: 1.6092 - classification_loss: 0.5250\n",
            "38/86 [============>.................] - ETA: 45s - loss: 2.1356 - regression_loss: 1.6100 - classification_loss: 0.5256\n",
            "39/86 [============>.................] - ETA: 44s - loss: 2.1280 - regression_loss: 1.6031 - classification_loss: 0.5249\n",
            "40/86 [============>.................] - ETA: 43s - loss: 2.1180 - regression_loss: 1.5958 - classification_loss: 0.5221\n",
            "41/86 [=============>................] - ETA: 42s - loss: 2.1202 - regression_loss: 1.5979 - classification_loss: 0.5222\n",
            "42/86 [=============>................] - ETA: 41s - loss: 2.1190 - regression_loss: 1.5959 - classification_loss: 0.5231\n",
            "43/86 [==============>...............] - ETA: 40s - loss: 2.1176 - regression_loss: 1.5941 - classification_loss: 0.5235\n",
            "44/86 [==============>...............] - ETA: 39s - loss: 2.1219 - regression_loss: 1.5974 - classification_loss: 0.5245\n",
            "45/86 [==============>...............] - ETA: 38s - loss: 2.1255 - regression_loss: 1.6011 - classification_loss: 0.5243\n",
            "46/86 [===============>..............] - ETA: 37s - loss: 2.1269 - regression_loss: 1.6029 - classification_loss: 0.5240\n",
            "47/86 [===============>..............] - ETA: 36s - loss: 2.1286 - regression_loss: 1.6043 - classification_loss: 0.5243\n",
            "48/86 [===============>..............] - ETA: 35s - loss: 2.1259 - regression_loss: 1.6027 - classification_loss: 0.5232\n",
            "49/86 [================>.............] - ETA: 34s - loss: 2.1185 - regression_loss: 1.5981 - classification_loss: 0.5204\n",
            "50/86 [================>.............] - ETA: 33s - loss: 2.1059 - regression_loss: 1.5889 - classification_loss: 0.5170\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.1055 - regression_loss: 1.5883 - classification_loss: 0.5173\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1114 - regression_loss: 1.5929 - classification_loss: 0.5184\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.1101 - regression_loss: 1.5921 - classification_loss: 0.5179\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1115 - regression_loss: 1.5937 - classification_loss: 0.5178\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1116 - regression_loss: 1.5945 - classification_loss: 0.5171\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1085 - regression_loss: 1.5917 - classification_loss: 0.5169\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1096 - regression_loss: 1.5915 - classification_loss: 0.5181\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1060 - regression_loss: 1.5886 - classification_loss: 0.5174\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1047 - regression_loss: 1.5893 - classification_loss: 0.5154\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1058 - regression_loss: 1.5905 - classification_loss: 0.5153\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1059 - regression_loss: 1.5896 - classification_loss: 0.5163\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1015 - regression_loss: 1.5867 - classification_loss: 0.5148\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1057 - regression_loss: 1.5897 - classification_loss: 0.5161\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1024 - regression_loss: 1.5864 - classification_loss: 0.5160\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1018 - regression_loss: 1.5868 - classification_loss: 0.5150\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.0987 - regression_loss: 1.5841 - classification_loss: 0.5146\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.0998 - regression_loss: 1.5850 - classification_loss: 0.5148\n",
            "68/86 [======================>.......] - ETA: 16s - loss: 2.1022 - regression_loss: 1.5877 - classification_loss: 0.5144\n",
            "69/86 [=======================>......] - ETA: 15s - loss: 2.0986 - regression_loss: 1.5851 - classification_loss: 0.5135\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1029 - regression_loss: 1.5882 - classification_loss: 0.5148\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1014 - regression_loss: 1.5865 - classification_loss: 0.5149\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1020 - regression_loss: 1.5871 - classification_loss: 0.5149\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1005 - regression_loss: 1.5860 - classification_loss: 0.5145\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.0991 - regression_loss: 1.5850 - classification_loss: 0.5141\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.0986 - regression_loss: 1.5847 - classification_loss: 0.5139\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0951 - regression_loss: 1.5822 - classification_loss: 0.5129 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0983 - regression_loss: 1.5836 - classification_loss: 0.5147\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.0932 - regression_loss: 1.5800 - classification_loss: 0.5132\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0922 - regression_loss: 1.5793 - classification_loss: 0.5129\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0934 - regression_loss: 1.5803 - classification_loss: 0.5131\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0936 - regression_loss: 1.5807 - classification_loss: 0.5129\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0974 - regression_loss: 1.5829 - classification_loss: 0.5145\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0991 - regression_loss: 1.5837 - classification_loss: 0.5154\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0996 - regression_loss: 1.5840 - classification_loss: 0.5155\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.1037 - regression_loss: 1.5860 - classification_loss: 0.5177\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1024 - regression_loss: 1.5849 - classification_loss: 0.5175\n",
            "Epoch 99: saving model to ./snapshots\\resnet50_csv_99.h5\n",
            "\n",
            "86/86 [==============================] - 82s 944ms/step - loss: 2.1024 - regression_loss: 1.5849 - classification_loss: 0.5175 - lr: 1.4013e-45\n",
            "Epoch 100/100\n",
            "\n",
            " 1/86 [..............................] - ETA: 2:23 - loss: 2.0564 - regression_loss: 1.5696 - classification_loss: 0.4868\n",
            " 2/86 [..............................] - ETA: 1:53 - loss: 2.1050 - regression_loss: 1.6235 - classification_loss: 0.4815\n",
            " 3/86 [>.............................] - ETA: 1:39 - loss: 2.2314 - regression_loss: 1.7130 - classification_loss: 0.5184\n",
            " 4/86 [>.............................] - ETA: 1:24 - loss: 2.1861 - regression_loss: 1.6811 - classification_loss: 0.5050\n",
            " 5/86 [>.............................] - ETA: 1:21 - loss: 2.1783 - regression_loss: 1.6762 - classification_loss: 0.5021\n",
            " 6/86 [=>............................] - ETA: 1:20 - loss: 2.1890 - regression_loss: 1.6852 - classification_loss: 0.5039\n",
            " 7/86 [=>............................] - ETA: 1:19 - loss: 2.2208 - regression_loss: 1.7003 - classification_loss: 0.5205\n",
            " 8/86 [=>............................] - ETA: 1:19 - loss: 2.2035 - regression_loss: 1.6810 - classification_loss: 0.5225\n",
            " 9/86 [==>...........................] - ETA: 1:16 - loss: 2.1856 - regression_loss: 1.6657 - classification_loss: 0.5199\n",
            "10/86 [==>...........................] - ETA: 1:14 - loss: 2.2024 - regression_loss: 1.6676 - classification_loss: 0.5348\n",
            "11/86 [==>...........................] - ETA: 1:12 - loss: 2.1781 - regression_loss: 1.6466 - classification_loss: 0.5315\n",
            "12/86 [===>..........................] - ETA: 1:10 - loss: 2.1466 - regression_loss: 1.6248 - classification_loss: 0.5218\n",
            "13/86 [===>..........................] - ETA: 1:09 - loss: 2.1301 - regression_loss: 1.6130 - classification_loss: 0.5171\n",
            "14/86 [===>..........................] - ETA: 1:10 - loss: 2.1323 - regression_loss: 1.6158 - classification_loss: 0.5166\n",
            "15/86 [====>.........................] - ETA: 1:10 - loss: 2.1314 - regression_loss: 1.6155 - classification_loss: 0.5160\n",
            "16/86 [====>.........................] - ETA: 1:10 - loss: 2.1431 - regression_loss: 1.6252 - classification_loss: 0.5178\n",
            "17/86 [====>.........................] - ETA: 1:08 - loss: 2.1452 - regression_loss: 1.6232 - classification_loss: 0.5220\n",
            "18/86 [=====>........................] - ETA: 1:07 - loss: 2.1504 - regression_loss: 1.6257 - classification_loss: 0.5247\n",
            "19/86 [=====>........................] - ETA: 1:05 - loss: 2.1434 - regression_loss: 1.6191 - classification_loss: 0.5244\n",
            "20/86 [=====>........................] - ETA: 1:05 - loss: 2.1409 - regression_loss: 1.6157 - classification_loss: 0.5252\n",
            "21/86 [======>.......................] - ETA: 1:04 - loss: 2.1443 - regression_loss: 1.6181 - classification_loss: 0.5262\n",
            "22/86 [======>.......................] - ETA: 1:03 - loss: 2.1389 - regression_loss: 1.6089 - classification_loss: 0.5300\n",
            "23/86 [=======>......................] - ETA: 1:01 - loss: 2.1376 - regression_loss: 1.6085 - classification_loss: 0.5291\n",
            "24/86 [=======>......................] - ETA: 1:00 - loss: 2.1334 - regression_loss: 1.6046 - classification_loss: 0.5288\n",
            "25/86 [=======>......................] - ETA: 59s - loss: 2.1363 - regression_loss: 1.6074 - classification_loss: 0.5289 \n",
            "26/86 [========>.....................] - ETA: 58s - loss: 2.1373 - regression_loss: 1.6076 - classification_loss: 0.5298\n",
            "27/86 [========>.....................] - ETA: 57s - loss: 2.1329 - regression_loss: 1.6060 - classification_loss: 0.5269\n",
            "28/86 [========>.....................] - ETA: 56s - loss: 2.1288 - regression_loss: 1.6037 - classification_loss: 0.5250\n",
            "29/86 [=========>....................] - ETA: 55s - loss: 2.1252 - regression_loss: 1.5994 - classification_loss: 0.5257\n",
            "30/86 [=========>....................] - ETA: 54s - loss: 2.1269 - regression_loss: 1.6005 - classification_loss: 0.5264\n",
            "31/86 [=========>....................] - ETA: 53s - loss: 2.1196 - regression_loss: 1.5966 - classification_loss: 0.5230\n",
            "32/86 [==========>...................] - ETA: 52s - loss: 2.1274 - regression_loss: 1.6015 - classification_loss: 0.5260\n",
            "33/86 [==========>...................] - ETA: 51s - loss: 2.1258 - regression_loss: 1.6002 - classification_loss: 0.5256\n",
            "34/86 [==========>...................] - ETA: 50s - loss: 2.1162 - regression_loss: 1.5918 - classification_loss: 0.5244\n",
            "35/86 [===========>..................] - ETA: 49s - loss: 2.1196 - regression_loss: 1.5954 - classification_loss: 0.5241\n",
            "36/86 [===========>..................] - ETA: 48s - loss: 2.1188 - regression_loss: 1.5948 - classification_loss: 0.5240\n",
            "37/86 [===========>..................] - ETA: 47s - loss: 2.1217 - regression_loss: 1.5946 - classification_loss: 0.5272\n",
            "38/86 [============>.................] - ETA: 46s - loss: 2.1205 - regression_loss: 1.5929 - classification_loss: 0.5276\n",
            "39/86 [============>.................] - ETA: 45s - loss: 2.1233 - regression_loss: 1.5961 - classification_loss: 0.5272\n",
            "40/86 [============>.................] - ETA: 44s - loss: 2.1278 - regression_loss: 1.5977 - classification_loss: 0.5302\n",
            "41/86 [=============>................] - ETA: 43s - loss: 2.1280 - regression_loss: 1.5987 - classification_loss: 0.5292\n",
            "42/86 [=============>................] - ETA: 42s - loss: 2.1306 - regression_loss: 1.6000 - classification_loss: 0.5306\n",
            "43/86 [==============>...............] - ETA: 41s - loss: 2.1255 - regression_loss: 1.5968 - classification_loss: 0.5287\n",
            "44/86 [==============>...............] - ETA: 40s - loss: 2.1222 - regression_loss: 1.5942 - classification_loss: 0.5280\n",
            "45/86 [==============>...............] - ETA: 39s - loss: 2.1191 - regression_loss: 1.5914 - classification_loss: 0.5277\n",
            "46/86 [===============>..............] - ETA: 38s - loss: 2.1137 - regression_loss: 1.5882 - classification_loss: 0.5255\n",
            "47/86 [===============>..............] - ETA: 37s - loss: 2.1123 - regression_loss: 1.5858 - classification_loss: 0.5265\n",
            "48/86 [===============>..............] - ETA: 36s - loss: 2.0993 - regression_loss: 1.5760 - classification_loss: 0.5233\n",
            "49/86 [================>.............] - ETA: 35s - loss: 2.1031 - regression_loss: 1.5786 - classification_loss: 0.5245\n",
            "50/86 [================>.............] - ETA: 34s - loss: 2.1051 - regression_loss: 1.5812 - classification_loss: 0.5239\n",
            "51/86 [================>.............] - ETA: 33s - loss: 2.0988 - regression_loss: 1.5766 - classification_loss: 0.5223\n",
            "52/86 [=================>............] - ETA: 32s - loss: 2.1010 - regression_loss: 1.5787 - classification_loss: 0.5223\n",
            "53/86 [=================>............] - ETA: 31s - loss: 2.0997 - regression_loss: 1.5795 - classification_loss: 0.5202\n",
            "54/86 [=================>............] - ETA: 30s - loss: 2.1001 - regression_loss: 1.5791 - classification_loss: 0.5211\n",
            "55/86 [==================>...........] - ETA: 29s - loss: 2.1033 - regression_loss: 1.5821 - classification_loss: 0.5212\n",
            "56/86 [==================>...........] - ETA: 28s - loss: 2.1078 - regression_loss: 1.5837 - classification_loss: 0.5241\n",
            "57/86 [==================>...........] - ETA: 27s - loss: 2.1096 - regression_loss: 1.5845 - classification_loss: 0.5252\n",
            "58/86 [===================>..........] - ETA: 26s - loss: 2.1114 - regression_loss: 1.5860 - classification_loss: 0.5254\n",
            "59/86 [===================>..........] - ETA: 25s - loss: 2.1085 - regression_loss: 1.5841 - classification_loss: 0.5244\n",
            "60/86 [===================>..........] - ETA: 24s - loss: 2.1060 - regression_loss: 1.5823 - classification_loss: 0.5237\n",
            "61/86 [====================>.........] - ETA: 23s - loss: 2.1074 - regression_loss: 1.5841 - classification_loss: 0.5233\n",
            "62/86 [====================>.........] - ETA: 22s - loss: 2.1055 - regression_loss: 1.5828 - classification_loss: 0.5228\n",
            "63/86 [====================>.........] - ETA: 21s - loss: 2.1090 - regression_loss: 1.5855 - classification_loss: 0.5235\n",
            "64/86 [=====================>........] - ETA: 20s - loss: 2.1079 - regression_loss: 1.5849 - classification_loss: 0.5229\n",
            "65/86 [=====================>........] - ETA: 19s - loss: 2.1132 - regression_loss: 1.5892 - classification_loss: 0.5240\n",
            "66/86 [======================>.......] - ETA: 18s - loss: 2.1081 - regression_loss: 1.5849 - classification_loss: 0.5232\n",
            "67/86 [======================>.......] - ETA: 17s - loss: 2.1078 - regression_loss: 1.5843 - classification_loss: 0.5235\n",
            "68/86 [======================>.......] - ETA: 16s - loss: 2.1058 - regression_loss: 1.5835 - classification_loss: 0.5223\n",
            "69/86 [=======================>......] - ETA: 16s - loss: 2.1064 - regression_loss: 1.5840 - classification_loss: 0.5224\n",
            "70/86 [=======================>......] - ETA: 15s - loss: 2.1073 - regression_loss: 1.5844 - classification_loss: 0.5229\n",
            "71/86 [=======================>......] - ETA: 14s - loss: 2.1064 - regression_loss: 1.5845 - classification_loss: 0.5219\n",
            "72/86 [========================>.....] - ETA: 13s - loss: 2.1073 - regression_loss: 1.5852 - classification_loss: 0.5221\n",
            "73/86 [========================>.....] - ETA: 12s - loss: 2.1089 - regression_loss: 1.5872 - classification_loss: 0.5217\n",
            "74/86 [========================>.....] - ETA: 11s - loss: 2.1017 - regression_loss: 1.5821 - classification_loss: 0.5197\n",
            "75/86 [=========================>....] - ETA: 10s - loss: 2.1054 - regression_loss: 1.5851 - classification_loss: 0.5202\n",
            "76/86 [=========================>....] - ETA: 9s - loss: 2.0994 - regression_loss: 1.5809 - classification_loss: 0.5185 \n",
            "77/86 [=========================>....] - ETA: 8s - loss: 2.0976 - regression_loss: 1.5796 - classification_loss: 0.5179\n",
            "78/86 [==========================>...] - ETA: 7s - loss: 2.1015 - regression_loss: 1.5827 - classification_loss: 0.5188\n",
            "79/86 [==========================>...] - ETA: 6s - loss: 2.0967 - regression_loss: 1.5796 - classification_loss: 0.5171\n",
            "80/86 [==========================>...] - ETA: 5s - loss: 2.0947 - regression_loss: 1.5776 - classification_loss: 0.5170\n",
            "81/86 [===========================>..] - ETA: 4s - loss: 2.0903 - regression_loss: 1.5747 - classification_loss: 0.5156\n",
            "82/86 [===========================>..] - ETA: 3s - loss: 2.0908 - regression_loss: 1.5751 - classification_loss: 0.5156\n",
            "83/86 [===========================>..] - ETA: 2s - loss: 2.0939 - regression_loss: 1.5780 - classification_loss: 0.5159\n",
            "84/86 [============================>.] - ETA: 1s - loss: 2.0946 - regression_loss: 1.5786 - classification_loss: 0.5160\n",
            "85/86 [============================>.] - ETA: 0s - loss: 2.0966 - regression_loss: 1.5802 - classification_loss: 0.5165\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0967 - regression_loss: 1.5799 - classification_loss: 0.5168\n",
            "Epoch 100: saving model to ./snapshots\\resnet50_csv_100.h5\n",
            "\n",
            "Epoch 100: ReduceLROnPlateau reducing learning rate to 1.4012984643248171e-46.\n",
            "\n",
            "86/86 [==============================] - 82s 951ms/step - loss: 2.0967 - regression_loss: 1.5799 - classification_loss: 0.5168 - lr: 1.4013e-45\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-30 20:39:05.908752: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-30 20:39:06.253832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9458 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
            "c:\\Users\\User\\anaconda3\\envs\\retinanet\\lib\\site-packages\\keras\\initializers\\initializers_v2.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
            "  warnings.warn(\n",
            "WARNING:tensorflow:Skipping loading weights for layer #212 (named classification_submodel) due to mismatch in shape for weight pyramid_classification/kernel:0. Weight expects shape (3, 3, 256, 144). Received saved weight with shape (720, 256, 3, 3)\n",
            "WARNING:tensorflow:Skipping loading weights for layer #212 (named classification_submodel) due to mismatch in shape for weight pyramid_classification/bias:0. Weight expects shape (144,). Received saved weight with shape (720,)\n",
            "c:\\Users\\User\\anaconda3\\envs\\retinanet\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
            "c:\\Users\\User\\Downloads\\RetinaNet\\keras-retinanet\\keras_retinanet\\bin\\train.py:538: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  return training_model.fit_generator(\n",
            "2023-12-30 20:39:15.067248: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
            "2023-12-30 20:39:16.089695: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
          ]
        }
      ],
      "source": [
        "!python keras_retinanet/bin/train.py \\\n",
        "        --freeze-backbone --random-transform --weights {pretrained_model_path} \\\n",
        "         --batch-size 8 --steps 86 --epochs 100 --tensorboard-dir tensorboard csv {annotations_path} {classes_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f25f2677",
      "metadata": {
        "id": "f25f2677"
      },
      "source": [
        "**<h3>Carrying out Inference on the model.</h3>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2e6f21d6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of ../filesToReplace/modified_visualization.py copied and replaced in keras_retinanet/utils/visualization.py successfully.\n"
          ]
        }
      ],
      "source": [
        "# For prediction (To properly visualize the bounding boxes)\n",
        "visualise_path_1 = \"../filesToReplace/modified_visualization.py\"\n",
        "visualise_copyTo_path= \"keras_retinanet/utils/visualization.py\"\n",
        "\n",
        "copy_and_replace(visualise_path_1, visualise_copyTo_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6d511993",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-16T11:24:16.929994Z",
          "start_time": "2023-01-16T11:23:35.549355Z"
        },
        "code_folding": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "6d511993",
        "outputId": "54300612-2ef0-4a0d-ff6e-f2c3d7616fb7",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "snapshots\\resnet50_csv_99.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The initializer RandomNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n"
          ]
        }
      ],
      "source": [
        "model_path = os.path.join('snapshots', sorted(os.listdir('snapshots'), reverse=True)[0])\n",
        "print(model_path)\n",
        "\n",
        "img_save_path = os.path.join(HOME, 'inference_images')\n",
        "\n",
        "if not os.path.exists(img_save_path):\n",
        "    os.makedirs(img_save_path)\n",
        "\n",
        "# load retinanet model\n",
        "model = models.load_model(model_path, backbone_name='resnet50')\n",
        "model = models.convert_model(model)\n",
        "\n",
        "# load label to names mapping for visualization purposes\n",
        "labels_to_names = pandas.read_csv(classes_path,header=None).T.loc[0].to_dict()\n",
        "\n",
        "\n",
        "def img_inference(img_path, threshold):\n",
        "    image = read_image_bgr(img_path)\n",
        "\n",
        "    # copy to draw on\n",
        "    draw = image.copy()\n",
        "    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # preprocess image for network\n",
        "    image = preprocess_image(image)\n",
        "    image, scale = resize_image(image)\n",
        "\n",
        "    # process image\n",
        "    start = time.time()\n",
        "    boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
        "    print(\"Processing Time: \", time.time() - start)\n",
        "\n",
        "    # correct for image scale\n",
        "    boxes /= scale\n",
        "\n",
        "    # visualize detections\n",
        "    for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
        "        # scores are sorted so we can break\n",
        "        if score < threshold:\n",
        "            break\n",
        "\n",
        "        color = label_color(label)\n",
        "\n",
        "        b = box.astype(int)\n",
        "        draw_box(draw, b, color=color)\n",
        "\n",
        "        caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
        "        draw_caption(draw, b, caption)\n",
        "\n",
        "    height, width, _ = draw.shape  # Get the height and width of the image\n",
        "    plt.figure(figsize=(width / 100, height / 100))  # Set the figure size based on the image size\n",
        "    plt.axis('off')\n",
        "    plt.imshow(draw)\n",
        "    plt.savefig(os.path.join(img_save_path, os.path.basename(img_path) + \".jpg\"), bbox_inches='tight')\n",
        "    plt.close()\n",
        "    # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a46bce78",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-16T11:26:32.301337Z",
          "start_time": "2023-01-16T11:26:04.661138Z"
        },
        "id": "a46bce78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Time:  2.7901651859283447\n",
            "Processing Time:  1.3851959705352783\n",
            "Processing Time:  0.2917187213897705\n",
            "Processing Time:  0.3416762351989746\n",
            "Processing Time:  0.28693628311157227\n",
            "Processing Time:  0.0555117130279541\n",
            "Processing Time:  0.05820798873901367\n",
            "Processing Time:  0.3191666603088379\n",
            "Processing Time:  0.04650735855102539\n",
            "Processing Time:  0.04697275161743164\n",
            "Processing Time:  0.3316824436187744\n",
            "Processing Time:  0.25351738929748535\n",
            "Processing Time:  0.05292630195617676\n",
            "Processing Time:  0.0614163875579834\n",
            "Processing Time:  0.051138877868652344\n",
            "Processing Time:  0.04951333999633789\n",
            "Processing Time:  0.25437116622924805\n",
            "Processing Time:  0.0559992790222168\n",
            "Processing Time:  0.05800127983093262\n",
            "Processing Time:  0.29778289794921875\n",
            "Processing Time:  0.0575871467590332\n",
            "Processing Time:  0.07150936126708984\n",
            "Processing Time:  0.04999971389770508\n",
            "Processing Time:  0.050000667572021484\n",
            "Processing Time:  0.0469970703125\n",
            "Processing Time:  0.05235099792480469\n",
            "Processing Time:  0.33552074432373047\n",
            "Processing Time:  0.04700040817260742\n",
            "Processing Time:  0.04899883270263672\n",
            "Processing Time:  0.056497812271118164\n",
            "Processing Time:  0.2902224063873291\n",
            "Processing Time:  0.047509193420410156\n",
            "Processing Time:  0.05047154426574707\n",
            "Processing Time:  0.04950976371765137\n",
            "Processing Time:  0.0523989200592041\n",
            "Processing Time:  0.050002336502075195\n",
            "Processing Time:  0.050506591796875\n",
            "Processing Time:  0.048510074615478516\n",
            "Processing Time:  0.046030521392822266\n",
            "Processing Time:  0.046999216079711914\n",
            "Processing Time:  0.057366132736206055\n",
            "Processing Time:  0.06499648094177246\n",
            "Processing Time:  0.2860853672027588\n",
            "Processing Time:  0.04751086235046387\n",
            "Processing Time:  0.05650782585144043\n",
            "Processing Time:  0.0655064582824707\n",
            "Processing Time:  0.06083226203918457\n",
            "Processing Time:  0.050362586975097656\n",
            "Processing Time:  0.3604614734649658\n",
            "Processing Time:  0.06440353393554688\n",
            "Processing Time:  0.051085710525512695\n",
            "Processing Time:  0.052514076232910156\n",
            "Processing Time:  0.07300424575805664\n",
            "Processing Time:  0.05952286720275879\n",
            "Processing Time:  0.05816364288330078\n",
            "Processing Time:  0.051216840744018555\n",
            "Processing Time:  0.049034833908081055\n",
            "Processing Time:  0.0485072135925293\n",
            "Processing Time:  0.3569636344909668\n",
            "Processing Time:  0.05108141899108887\n",
            "Processing Time:  0.04691886901855469\n",
            "Processing Time:  0.06604671478271484\n",
            "Processing Time:  0.05413031578063965\n",
            "Processing Time:  0.048512935638427734\n",
            "Processing Time:  0.05241203308105469\n",
            "Processing Time:  0.06133580207824707\n",
            "Processing Time:  0.04850649833679199\n",
            "Processing Time:  0.05351066589355469\n",
            "Processing Time:  0.05821490287780762\n",
            "Processing Time:  0.08550500869750977\n",
            "Processing Time:  0.0795738697052002\n",
            "Processing Time:  0.07825183868408203\n",
            "Processing Time:  0.07851505279541016\n",
            "Processing Time:  0.07222437858581543\n",
            "Processing Time:  0.09091806411743164\n",
            "Processing Time:  0.2611401081085205\n",
            "Processing Time:  0.04550623893737793\n",
            "Processing Time:  0.05000019073486328\n",
            "Processing Time:  0.0517117977142334\n",
            "Processing Time:  0.07151246070861816\n",
            "Processing Time:  0.2621283531188965\n",
            "Processing Time:  0.2271721363067627\n",
            "Processing Time:  0.30972862243652344\n",
            "Processing Time:  0.05321955680847168\n",
            "Processing Time:  0.15811443328857422\n",
            "Processing Time:  0.0560002326965332\n",
            "Processing Time:  0.04699993133544922\n",
            "Processing Time:  0.30583858489990234\n",
            "Processing Time:  0.04817652702331543\n",
            "Processing Time:  0.15229463577270508\n",
            "Processing Time:  0.23356151580810547\n",
            "Processing Time:  0.05880331993103027\n",
            "Processing Time:  0.0609898567199707\n",
            "Processing Time:  0.047508955001831055\n",
            "Processing Time:  0.05851292610168457\n",
            "Processing Time:  0.059508323669433594\n",
            "Processing Time:  0.05300331115722656\n",
            "Processing Time:  0.0650026798248291\n",
            "Processing Time:  0.23885083198547363\n",
            "Processing Time:  0.05150747299194336\n",
            "Processing Time:  0.0655064582824707\n",
            "Processing Time:  0.05150771141052246\n",
            "Processing Time:  0.0534515380859375\n",
            "Processing Time:  0.049359798431396484\n",
            "Processing Time:  0.051000356674194336\n",
            "Processing Time:  0.3020017147064209\n",
            "Processing Time:  0.05585670471191406\n",
            "Processing Time:  0.05300116539001465\n",
            "Processing Time:  0.05600237846374512\n",
            "Processing Time:  0.048319101333618164\n",
            "Processing Time:  0.04605817794799805\n",
            "Processing Time:  0.2801671028137207\n",
            "Processing Time:  0.04750704765319824\n",
            "Processing Time:  0.0525059700012207\n",
            "Processing Time:  0.060692548751831055\n",
            "Processing Time:  0.058901309967041016\n",
            "Processing Time:  0.19071173667907715\n",
            "Processing Time:  0.06371307373046875\n",
            "Processing Time:  0.04900002479553223\n",
            "Processing Time:  0.046701669692993164\n",
            "Processing Time:  0.06750988960266113\n",
            "Processing Time:  0.06550931930541992\n",
            "Processing Time:  0.0655064582824707\n",
            "Processing Time:  0.04999899864196777\n",
            "Processing Time:  0.06008505821228027\n",
            "Processing Time:  0.04857492446899414\n",
            "Processing Time:  0.0690007209777832\n",
            "Processing Time:  0.06400275230407715\n",
            "Processing Time:  0.07361125946044922\n",
            "Processing Time:  0.06408524513244629\n",
            "Processing Time:  0.08005404472351074\n",
            "Processing Time:  0.09700608253479004\n",
            "Processing Time:  0.09425783157348633\n",
            "Processing Time:  0.07674455642700195\n",
            "Processing Time:  0.0761101245880127\n",
            "Processing Time:  0.0915074348449707\n",
            "Processing Time:  0.08850789070129395\n",
            "Processing Time:  0.07306885719299316\n",
            "Processing Time:  0.07497453689575195\n",
            "Processing Time:  0.06699943542480469\n",
            "Processing Time:  0.08751702308654785\n",
            "Processing Time:  0.3482644557952881\n",
            "Processing Time:  0.05400276184082031\n",
            "Processing Time:  0.05700278282165527\n",
            "Processing Time:  0.051337480545043945\n",
            "Processing Time:  0.05402660369873047\n",
            "Processing Time:  0.06115531921386719\n",
            "Processing Time:  0.05000019073486328\n",
            "Processing Time:  0.05050539970397949\n",
            "Processing Time:  0.06550788879394531\n",
            "Processing Time:  0.06706523895263672\n",
            "Processing Time:  0.07050776481628418\n",
            "Processing Time:  0.07400274276733398\n",
            "Processing Time:  0.07309436798095703\n",
            "Processing Time:  0.0500025749206543\n",
            "Processing Time:  0.0591888427734375\n",
            "Processing Time:  0.050513505935668945\n",
            "Processing Time:  0.22953510284423828\n",
            "Processing Time:  0.051511526107788086\n",
            "Processing Time:  0.05050849914550781\n",
            "Processing Time:  0.06319499015808105\n",
            "Processing Time:  0.0508575439453125\n",
            "Processing Time:  0.04937100410461426\n",
            "Processing Time:  0.048003435134887695\n",
            "Processing Time:  0.05212044715881348\n",
            "Processing Time:  0.05299854278564453\n",
            "Processing Time:  0.06600236892700195\n",
            "Processing Time:  0.06900835037231445\n",
            "Processing Time:  0.06999945640563965\n",
            "Processing Time:  0.07317161560058594\n",
            "Processing Time:  0.07944488525390625\n",
            "Processing Time:  0.06951260566711426\n",
            "Processing Time:  0.06856584548950195\n",
            "Processing Time:  0.10216164588928223\n",
            "Processing Time:  0.06568741798400879\n",
            "Processing Time:  0.091094970703125\n",
            "Processing Time:  0.08576393127441406\n",
            "Processing Time:  0.09550833702087402\n",
            "Processing Time:  0.06902647018432617\n",
            "Processing Time:  0.07105541229248047\n",
            "Processing Time:  0.07410025596618652\n",
            "Processing Time:  0.0976262092590332\n",
            "Processing Time:  0.07796835899353027\n",
            "Processing Time:  0.0850982666015625\n",
            "Processing Time:  0.08476138114929199\n",
            "Processing Time:  0.08701586723327637\n",
            "Processing Time:  0.07601690292358398\n",
            "Processing Time:  0.09869098663330078\n",
            "Processing Time:  0.07662367820739746\n",
            "Processing Time:  0.08850836753845215\n",
            "Processing Time:  0.07650589942932129\n",
            "Processing Time:  0.36716437339782715\n",
            "Processing Time:  0.05099940299987793\n",
            "Processing Time:  0.0620732307434082\n",
            "Processing Time:  0.17013001441955566\n",
            "Processing Time:  0.27434802055358887\n",
            "Processing Time:  0.05291152000427246\n",
            "Processing Time:  0.05100250244140625\n",
            "Processing Time:  0.32828617095947266\n",
            "Processing Time:  0.06133222579956055\n",
            "Processing Time:  0.04860854148864746\n",
            "Processing Time:  0.05803823471069336\n",
            "Processing Time:  0.06450867652893066\n",
            "Processing Time:  0.0525050163269043\n",
            "Processing Time:  0.049002647399902344\n",
            "Processing Time:  0.05059981346130371\n",
            "Processing Time:  0.051000118255615234\n",
            "Processing Time:  0.24701476097106934\n",
            "Processing Time:  0.05166435241699219\n",
            "Processing Time:  0.07240819931030273\n",
            "Processing Time:  0.060504913330078125\n",
            "Processing Time:  0.17050933837890625\n",
            "Processing Time:  0.06661534309387207\n",
            "Processing Time:  0.06412696838378906\n",
            "Processing Time:  0.08022379875183105\n",
            "Processing Time:  0.052228689193725586\n",
            "Processing Time:  0.32391858100891113\n",
            "Processing Time:  0.06504940986633301\n",
            "Processing Time:  0.04818606376647949\n",
            "Processing Time:  0.06325697898864746\n",
            "Processing Time:  0.049568891525268555\n",
            "Processing Time:  0.06729602813720703\n",
            "Processing Time:  0.06050539016723633\n",
            "Processing Time:  0.0630805492401123\n",
            "Processing Time:  0.16648221015930176\n",
            "Processing Time:  0.22153306007385254\n",
            "Processing Time:  0.26853156089782715\n",
            "Processing Time:  0.04896402359008789\n",
            "Processing Time:  0.05282020568847656\n",
            "Processing Time:  0.05149388313293457\n",
            "Processing Time:  0.0782318115234375\n",
            "Processing Time:  0.06050753593444824\n",
            "Processing Time:  0.05150890350341797\n",
            "Processing Time:  0.06350922584533691\n",
            "Processing Time:  0.26204586029052734\n",
            "Processing Time:  0.0630805492401123\n",
            "Processing Time:  0.08050799369812012\n",
            "Processing Time:  0.057581424713134766\n",
            "Processing Time:  0.052896738052368164\n",
            "Processing Time:  0.22315621376037598\n",
            "Processing Time:  0.18594145774841309\n",
            "Processing Time:  0.06609749794006348\n",
            "Processing Time:  0.3189823627471924\n",
            "Processing Time:  0.058798789978027344\n",
            "Processing Time:  0.06131434440612793\n",
            "Processing Time:  0.051500797271728516\n",
            "Processing Time:  0.06914877891540527\n",
            "Processing Time:  0.05351614952087402\n",
            "Processing Time:  0.06937742233276367\n",
            "Processing Time:  0.05116915702819824\n",
            "Processing Time:  0.23423027992248535\n",
            "Processing Time:  0.054003000259399414\n",
            "Processing Time:  0.06850814819335938\n",
            "Processing Time:  0.05261850357055664\n",
            "Processing Time:  0.05300259590148926\n",
            "Processing Time:  0.061116695404052734\n",
            "Processing Time:  0.05039358139038086\n",
            "Processing Time:  0.05910944938659668\n",
            "Processing Time:  0.05950522422790527\n",
            "Processing Time:  0.054506540298461914\n",
            "Processing Time:  0.06300020217895508\n",
            "Processing Time:  0.1025080680847168\n",
            "Processing Time:  0.06732535362243652\n",
            "Processing Time:  0.07400226593017578\n",
            "Processing Time:  0.06517529487609863\n",
            "Processing Time:  0.21995806694030762\n",
            "Processing Time:  0.053278446197509766\n",
            "Processing Time:  0.049999237060546875\n",
            "Processing Time:  0.05566263198852539\n",
            "Processing Time:  0.06251239776611328\n",
            "Processing Time:  0.04899954795837402\n",
            "Processing Time:  0.04706549644470215\n",
            "Processing Time:  0.06463885307312012\n",
            "Processing Time:  0.05450749397277832\n",
            "Processing Time:  0.04823160171508789\n",
            "Processing Time:  0.06615614891052246\n",
            "Processing Time:  0.06810307502746582\n",
            "Processing Time:  0.0754854679107666\n",
            "Processing Time:  0.0709993839263916\n",
            "Processing Time:  0.08452820777893066\n",
            "Processing Time:  0.07550597190856934\n",
            "Processing Time:  0.08349871635437012\n",
            "Processing Time:  0.08111763000488281\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "test_path = os.path.join('Pizza-Object-Detector-7','test')\n",
        "\n",
        "images = [os.path.join(test_path,img) for img in os.listdir(test_path) if '.jpg' in img]\n",
        "\n",
        "for img in images:\n",
        "    img_inference(img, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c44ecff",
      "metadata": {
        "id": "9c44ecff"
      },
      "source": [
        "**<h3>Testing the RetinaNet model on the Testing subset.</h3>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8605f293",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-16T12:56:52.868443Z",
          "start_time": "2023-01-16T12:56:52.853353Z"
        },
        "id": "8605f293"
      },
      "outputs": [],
      "source": [
        "test_annotations_path = os.path.join(HOME, os.path.join('Pizza-Object-Detector-7','test','_annotations.csv'))\n",
        "model_path            = os.path.join(HOME, os.path.join('snapshots', sorted(os.listdir('snapshots'), reverse=True)[0]))\n",
        "inference_model_path  = os.path.join(HOME, os.path.join('snapshots','inference_resnet50_csv.h5'))\n",
        "save_path             = os.path.join(HOME, os.path.join('evaluation'))\n",
        "classes_path          = os.path.join(HOME, os.path.join('_classes.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "14c6fdd3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-16T12:54:13.719598Z",
          "start_time": "2023-01-16T12:54:13.715602Z"
        },
        "id": "14c6fdd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python keras_retinanet/bin/convert_model.py c:\\Users\\User\\Downloads\\RetinaNet\\keras-retinanet\\snapshots\\resnet50_csv_99.h5 c:\\Users\\User\\Downloads\\RetinaNet\\keras-retinanet\\snapshots\\inference_resnet50_csv.h5\n",
            "python keras_retinanet/bin/evaluate.py csv c:\\Users\\User\\Downloads\\RetinaNet\\keras-retinanet\\Pizza-Object-Detector-7\\test\\_annotations.csv c:\\Users\\User\\Downloads\\RetinaNet\\keras-retinanet\\_classes.csv c:\\Users\\User\\Downloads\\RetinaNet\\keras-retinanet\\snapshots\\inference_resnet50_csv.h5\n"
          ]
        }
      ],
      "source": [
        "#ALTERNATIVE: Run commands in terminal instead\n",
        "print(f'python keras_retinanet/bin/convert_model.py {model_path} {inference_model_path}')\n",
        "print(f'python keras_retinanet/bin/evaluate.py csv {test_annotations_path} {classes_path} {inference_model_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "00c99c0b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-16T13:06:27.282773Z",
          "start_time": "2023-01-16T13:05:57.626612Z"
        },
        "collapsed": true,
        "id": "00c99c0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of ../filesToReplace/visualization.py copied and replaced in keras_retinanet/utils/visualization.py successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-30 23:04:04.983113: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "c:\\Users\\User\\anaconda3\\envs\\retinanet\\lib\\site-packages\\keras\\initializers\\initializers_v2.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
            "  warnings.warn(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n",
            "Loading model, this may take a second...\n",
            "209 instances of class Arugula with average precision: 0.0050\n",
            "49 instances of class Bacon with average precision: 0.0000\n",
            "492 instances of class Basil with average precision: 0.2318\n",
            "31 instances of class Broccoli with average precision: 0.0538\n",
            "620 instances of class Cheese with average precision: 0.0615\n",
            "184 instances of class Chicken with average precision: 0.0002\n",
            "184 instances of class Corn with average precision: 0.0000\n",
            "277 instances of class Ham with average precision: 0.0066\n",
            "456 instances of class Mushroom with average precision: 0.0273\n",
            "580 instances of class Olives with average precision: 0.2463\n",
            "204 instances of class Onion with average precision: 0.0000\n",
            "849 instances of class Pepperoni with average precision: 0.4181\n",
            "555 instances of class Peppers with average precision: 0.0474\n",
            "98 instances of class Pineapple with average precision: 0.0000\n",
            "313 instances of class Pizza with average precision: 0.9186\n",
            "745 instances of class Tomatoes with average precision: 0.1479\n",
            "Precision:  (array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.04761905, 0.09090909, 0.08695652, 0.08333333, 0.08      ,\n",
            "       0.07692308, 0.07407407, 0.07142857, 0.06896552, 0.06666667,\n",
            "       0.06451613, 0.0625    , 0.06060606, 0.05882353, 0.05714286,\n",
            "       0.05555556, 0.05405405, 0.05263158, 0.05128205, 0.05      ,\n",
            "       0.04878049, 0.04761905, 0.04651163, 0.04545455, 0.04444444,\n",
            "       0.04347826, 0.04255319, 0.04166667, 0.04081633, 0.04      ,\n",
            "       0.03921569, 0.03846154, 0.05660377, 0.05555556, 0.05454545,\n",
            "       0.05357143, 0.05263158, 0.05172414, 0.05084746, 0.05      ,\n",
            "       0.04918033, 0.0483871 , 0.04761905, 0.046875  , 0.04615385,\n",
            "       0.04545455, 0.04477612, 0.04411765, 0.04347826, 0.04285714,\n",
            "       0.04225352, 0.04166667, 0.04109589, 0.04054054, 0.04      ,\n",
            "       0.03947368, 0.03896104, 0.03846154, 0.03797468, 0.0375    ,\n",
            "       0.03703704, 0.03658537, 0.03614458, 0.03571429, 0.03529412,\n",
            "       0.03488372, 0.03448276, 0.03409091, 0.03370787, 0.03333333,\n",
            "       0.03296703, 0.0326087 , 0.03225806, 0.03191489, 0.03157895,\n",
            "       0.03125   , 0.03092784, 0.03061224, 0.03030303, 0.03      ,\n",
            "       0.02970297, 0.02941176, 0.03883495, 0.03846154, 0.03809524,\n",
            "       0.03773585, 0.03738318, 0.03703704, 0.03669725, 0.03636364,\n",
            "       0.03603604, 0.03571429, 0.04424779, 0.04385965, 0.04347826,\n",
            "       0.04310345, 0.04273504, 0.04237288, 0.04201681, 0.04166667,\n",
            "       0.04132231, 0.04098361, 0.04065041, 0.0483871 , 0.048     ,\n",
            "       0.04761905, 0.04724409, 0.046875  , 0.05426357, 0.05384615,\n",
            "       0.05343511, 0.0530303 , 0.05263158, 0.05223881, 0.05185185,\n",
            "       0.05147059, 0.05109489, 0.05072464, 0.05035971, 0.05      ,\n",
            "       0.04964539, 0.04929577, 0.04895105, 0.04861111, 0.04827586,\n",
            "       0.04794521, 0.04761905, 0.0472973 , 0.04697987, 0.05333333,\n",
            "       0.05298013, 0.05263158, 0.05228758, 0.05194805, 0.0516129 ,\n",
            "       0.05128205, 0.05095541, 0.05063291, 0.05031447, 0.05      ,\n",
            "       0.04968944, 0.04938272, 0.04907975, 0.04878049, 0.04848485,\n",
            "       0.05421687, 0.05389222, 0.05357143, 0.05325444, 0.05294118,\n",
            "       0.05263158, 0.05232558, 0.05202312, 0.05172414, 0.05142857,\n",
            "       0.05113636, 0.05084746, 0.0505618 , 0.05027933, 0.05      ,\n",
            "       0.04972376, 0.04945055, 0.04918033, 0.05434783, 0.05405405,\n",
            "       0.05913978, 0.05882353, 0.05851064, 0.05820106, 0.05789474,\n",
            "       0.05759162, 0.05729167, 0.05699482, 0.05670103, 0.05641026,\n",
            "       0.05612245, 0.05583756, 0.05555556, 0.05527638, 0.06      ,\n",
            "       0.05970149, 0.05940594, 0.0591133 , 0.05882353, 0.05853659,\n",
            "       0.05825243, 0.05797101, 0.05769231, 0.05741627, 0.05714286,\n",
            "       0.06161137, 0.06132075, 0.06103286, 0.06074766, 0.06046512,\n",
            "       0.06018519, 0.05990783, 0.05963303, 0.05936073, 0.05909091,\n",
            "       0.05882353, 0.05855856, 0.05829596, 0.0625    , 0.06666667,\n",
            "       0.06637168, 0.0660793 , 0.06578947, 0.06550218, 0.06521739,\n",
            "       0.06493506, 0.06465517, 0.06437768, 0.06410256, 0.06382979,\n",
            "       0.06355932, 0.06329114, 0.06302521, 0.06276151, 0.0625    ,\n",
            "       0.06224066, 0.06198347, 0.0617284 , 0.06147541, 0.06122449]), array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.00478469, 0.00956938, 0.00956938, 0.00956938, 0.00956938,\n",
            "       0.00956938, 0.00956938, 0.00956938, 0.00956938, 0.00956938,\n",
            "       0.00956938, 0.00956938, 0.00956938, 0.00956938, 0.00956938,\n",
            "       0.00956938, 0.00956938, 0.00956938, 0.00956938, 0.00956938,\n",
            "       0.00956938, 0.00956938, 0.00956938, 0.00956938, 0.00956938,\n",
            "       0.00956938, 0.00956938, 0.00956938, 0.00956938, 0.00956938,\n",
            "       0.00956938, 0.00956938, 0.01435407, 0.01435407, 0.01435407,\n",
            "       0.01435407, 0.01435407, 0.01435407, 0.01435407, 0.01435407,\n",
            "       0.01435407, 0.01435407, 0.01435407, 0.01435407, 0.01435407,\n",
            "       0.01435407, 0.01435407, 0.01435407, 0.01435407, 0.01435407,\n",
            "       0.01435407, 0.01435407, 0.01435407, 0.01435407, 0.01435407,\n",
            "       0.01435407, 0.01435407, 0.01435407, 0.01435407, 0.01435407,\n",
            "       0.01435407, 0.01435407, 0.01435407, 0.01435407, 0.01435407,\n",
            "       0.01435407, 0.01435407, 0.01435407, 0.01435407, 0.01435407,\n",
            "       0.01435407, 0.01435407, 0.01435407, 0.01435407, 0.01435407,\n",
            "       0.01435407, 0.01435407, 0.01435407, 0.01435407, 0.01435407,\n",
            "       0.01435407, 0.01435407, 0.01913876, 0.01913876, 0.01913876,\n",
            "       0.01913876, 0.01913876, 0.01913876, 0.01913876, 0.01913876,\n",
            "       0.01913876, 0.01913876, 0.02392344, 0.02392344, 0.02392344,\n",
            "       0.02392344, 0.02392344, 0.02392344, 0.02392344, 0.02392344,\n",
            "       0.02392344, 0.02392344, 0.02392344, 0.02870813, 0.02870813,\n",
            "       0.02870813, 0.02870813, 0.02870813, 0.03349282, 0.03349282,\n",
            "       0.03349282, 0.03349282, 0.03349282, 0.03349282, 0.03349282,\n",
            "       0.03349282, 0.03349282, 0.03349282, 0.03349282, 0.03349282,\n",
            "       0.03349282, 0.03349282, 0.03349282, 0.03349282, 0.03349282,\n",
            "       0.03349282, 0.03349282, 0.03349282, 0.03349282, 0.03827751,\n",
            "       0.03827751, 0.03827751, 0.03827751, 0.03827751, 0.03827751,\n",
            "       0.03827751, 0.03827751, 0.03827751, 0.03827751, 0.03827751,\n",
            "       0.03827751, 0.03827751, 0.03827751, 0.03827751, 0.03827751,\n",
            "       0.0430622 , 0.0430622 , 0.0430622 , 0.0430622 , 0.0430622 ,\n",
            "       0.0430622 , 0.0430622 , 0.0430622 , 0.0430622 , 0.0430622 ,\n",
            "       0.0430622 , 0.0430622 , 0.0430622 , 0.0430622 , 0.0430622 ,\n",
            "       0.0430622 , 0.0430622 , 0.0430622 , 0.04784689, 0.04784689,\n",
            "       0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158,\n",
            "       0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158,\n",
            "       0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05741627,\n",
            "       0.05741627, 0.05741627, 0.05741627, 0.05741627, 0.05741627,\n",
            "       0.05741627, 0.05741627, 0.05741627, 0.05741627, 0.05741627,\n",
            "       0.06220096, 0.06220096, 0.06220096, 0.06220096, 0.06220096,\n",
            "       0.06220096, 0.06220096, 0.06220096, 0.06220096, 0.06220096,\n",
            "       0.06220096, 0.06220096, 0.06220096, 0.06698565, 0.07177033,\n",
            "       0.07177033, 0.07177033, 0.07177033, 0.07177033, 0.07177033,\n",
            "       0.07177033, 0.07177033, 0.07177033, 0.07177033, 0.07177033,\n",
            "       0.07177033, 0.07177033, 0.07177033, 0.07177033, 0.07177033,\n",
            "       0.07177033, 0.07177033, 0.07177033, 0.07177033, 0.07177033]))\n",
            "Recall:  (array([], dtype=float64), array([], dtype=float64))\n",
            "Inference time for 283 images: 0.1729\n",
            "mAP using the weighted average of precisions among classes: 0.1866\n",
            "mAP: 0.1353\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-30 23:04:08.981142: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-30 23:04:09.368254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1797 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
            "c:\\Users\\User\\anaconda3\\envs\\retinanet\\lib\\site-packages\\keras\\initializers\\initializers_v2.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
            "  warnings.warn(\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "Running network:   0% (0 of 283) |       | Elapsed Time: 0:00:00 ETA:  --:--:--\n",
            "2023-12-30 23:04:15.557430: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
            "2023-12-30 23:04:18.200975: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
            "2023-12-30 23:04:22.472884: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2023-12-30 23:04:22.473170: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "Running network:   0% (1 of 283) |       | Elapsed Time: 0:00:09 ETA:   0:46:21\n",
            "2023-12-30 23:04:23.777558: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2023-12-30 23:04:23.777841: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "Running network:   0% (2 of 283) |       | Elapsed Time: 0:00:11 ETA:   0:25:51\n",
            "2023-12-30 23:04:24.069839: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2023-12-30 23:04:24.070159: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2023-12-30 23:04:24.267581: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.63GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2023-12-30 23:04:24.267874: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.63GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "Running network:   1% (3 of 283) |       | Elapsed Time: 0:00:11 ETA:   0:17:56\n",
            "2023-12-30 23:04:24.547895: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2023-12-30 23:04:24.548171: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "Running network:   1% (4 of 283) |       | Elapsed Time: 0:00:11 ETA:   0:03:16\n",
            "Running network:   1% (5 of 283) |       | Elapsed Time: 0:00:12 ETA:   0:02:42\n",
            "Running network:   2% (6 of 283) |       | Elapsed Time: 0:00:12 ETA:   0:02:20\n",
            "Running network:   2% (7 of 283) |       | Elapsed Time: 0:00:12 ETA:   0:02:03\n",
            "Running network:   2% (8 of 283) |       | Elapsed Time: 0:00:12 ETA:   0:01:49\n",
            "Running network:   3% (9 of 283) |       | Elapsed Time: 0:00:12 ETA:   0:01:40\n",
            "Running network:   3% (10 of 283) |      | Elapsed Time: 0:00:12 ETA:   0:01:34\n",
            "Running network:   3% (11 of 283) |      | Elapsed Time: 0:00:13 ETA:   0:01:03\n",
            "Running network:   4% (12 of 283) |      | Elapsed Time: 0:00:13 ETA:   0:01:00\n",
            "Running network:   4% (13 of 283) |      | Elapsed Time: 0:00:13 ETA:   0:00:55\n",
            "Running network:   4% (14 of 283) |      | Elapsed Time: 0:00:13 ETA:   0:00:53\n",
            "Running network:   5% (15 of 283) |      | Elapsed Time: 0:00:14 ETA:   0:00:50\n",
            "Running network:   5% (16 of 283) |      | Elapsed Time: 0:00:14 ETA:   0:00:52\n",
            "Running network:   6% (17 of 283) |      | Elapsed Time: 0:00:14 ETA:   0:00:51\n",
            "Running network:   6% (18 of 283) |      | Elapsed Time: 0:00:14 ETA:   0:00:53\n",
            "Running network:   6% (19 of 283) |      | Elapsed Time: 0:00:14 ETA:   0:00:54\n",
            "Running network:   7% (20 of 283) |      | Elapsed Time: 0:00:15 ETA:   0:00:53\n",
            "Running network:   7% (21 of 283) |      | Elapsed Time: 0:00:15 ETA:   0:00:58\n",
            "Running network:   7% (22 of 283) |      | Elapsed Time: 0:00:15 ETA:   0:00:56\n",
            "Running network:   8% (23 of 283) |      | Elapsed Time: 0:00:15 ETA:   0:00:53\n",
            "Running network:   8% (24 of 283) |      | Elapsed Time: 0:00:15 ETA:   0:00:55\n",
            "2023-12-30 23:04:28.993681: W tensorflow/core/common_runtime/bfc_allocator.cc:360] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n",
            "Running network:   8% (25 of 283) |      | Elapsed Time: 0:00:17 ETA:   0:01:50\n",
            "Running network:   9% (26 of 283) |      | Elapsed Time: 0:00:17 ETA:   0:01:35\n",
            "Running network:   9% (27 of 283) |      | Elapsed Time: 0:00:17 ETA:   0:02:10\n",
            "Running network:   9% (28 of 283) |      | Elapsed Time: 0:00:17 ETA:   0:02:15\n",
            "Running network:  10% (29 of 283) |      | Elapsed Time: 0:00:18 ETA:   0:01:55\n",
            "Running network:  10% (30 of 283) |      | Elapsed Time: 0:00:18 ETA:   0:01:41\n",
            "Running network:  10% (31 of 283) |      | Elapsed Time: 0:00:18 ETA:   0:01:34\n",
            "Running network:  11% (32 of 283) |      | Elapsed Time: 0:00:18 ETA:   0:01:28\n",
            "Running network:  11% (33 of 283) |      | Elapsed Time: 0:00:18 ETA:   0:01:22\n",
            "Running network:  12% (34 of 283) |      | Elapsed Time: 0:00:18 ETA:   0:01:17\n",
            "Running network:  12% (35 of 283) |      | Elapsed Time: 0:00:19 ETA:   0:01:13\n",
            "Running network:  12% (36 of 283) |      | Elapsed Time: 0:00:19 ETA:   0:00:50\n",
            "Running network:  13% (37 of 283) |      | Elapsed Time: 0:00:19 ETA:   0:00:48\n",
            "Running network:  13% (38 of 283) |      | Elapsed Time: 0:00:20 ETA:   0:00:55\n",
            "Running network:  13% (39 of 283) |      | Elapsed Time: 0:00:20 ETA:   0:00:59\n",
            "Running network:  14% (40 of 283) |      | Elapsed Time: 0:00:20 ETA:   0:00:57\n",
            "Running network:  14% (41 of 283) |      | Elapsed Time: 0:00:20 ETA:   0:00:55\n",
            "Running network:  14% (42 of 283) |      | Elapsed Time: 0:00:20 ETA:   0:00:54\n",
            "Running network:  15% (43 of 283) |      | Elapsed Time: 0:00:21 ETA:   0:01:01\n",
            "Running network:  15% (44 of 283) |      | Elapsed Time: 0:00:21 ETA:   0:01:10\n",
            "Running network:  15% (45 of 283) |      | Elapsed Time: 0:00:21 ETA:   0:01:05\n",
            "Running network:  16% (46 of 283) |      | Elapsed Time: 0:00:21 ETA:   0:01:01\n",
            "Running network:  16% (47 of 283) |      | Elapsed Time: 0:00:21 ETA:   0:00:58\n",
            "Running network:  16% (48 of 283) |#     | Elapsed Time: 0:00:22 ETA:   0:00:58\n",
            "Running network:  17% (49 of 283) |#     | Elapsed Time: 0:00:22 ETA:   0:00:45\n",
            "Running network:  17% (50 of 283) |#     | Elapsed Time: 0:00:22 ETA:   0:00:42\n",
            "Running network:  18% (51 of 283) |#     | Elapsed Time: 0:00:22 ETA:   0:00:43\n",
            "Running network:  18% (52 of 283) |#     | Elapsed Time: 0:00:22 ETA:   0:00:43\n",
            "Running network:  18% (53 of 283) |#     | Elapsed Time: 0:00:22 ETA:   0:00:44\n",
            "Running network:  19% (54 of 283) |#     | Elapsed Time: 0:00:23 ETA:   0:00:45\n",
            "Running network:  19% (55 of 283) |#     | Elapsed Time: 0:00:23 ETA:   0:00:43\n",
            "Running network:  19% (56 of 283) |#     | Elapsed Time: 0:00:23 ETA:   0:00:40\n",
            "Running network:  20% (57 of 283) |#     | Elapsed Time: 0:00:23 ETA:   0:00:39\n",
            "Running network:  20% (58 of 283) |#     | Elapsed Time: 0:00:24 ETA:   0:00:46\n",
            "Running network:  20% (59 of 283) |#     | Elapsed Time: 0:00:24 ETA:   0:00:51\n",
            "Running network:  21% (60 of 283) |#     | Elapsed Time: 0:00:24 ETA:   0:00:49\n",
            "Running network:  21% (61 of 283) |#     | Elapsed Time: 0:00:24 ETA:   0:00:47\n",
            "Running network:  21% (62 of 283) |#     | Elapsed Time: 0:00:24 ETA:   0:00:47\n",
            "Running network:  22% (63 of 283) |#     | Elapsed Time: 0:00:25 ETA:   0:00:46\n",
            "Running network:  22% (64 of 283) |#     | Elapsed Time: 0:00:25 ETA:   0:00:44\n",
            "Running network:  22% (65 of 283) |#     | Elapsed Time: 0:00:25 ETA:   0:00:43\n",
            "Running network:  23% (66 of 283) |#     | Elapsed Time: 0:00:25 ETA:   0:00:41\n",
            "Running network:  23% (67 of 283) |#     | Elapsed Time: 0:00:25 ETA:   0:00:44\n",
            "Running network:  24% (68 of 283) |#     | Elapsed Time: 0:00:25 ETA:   0:00:43\n",
            "Running network:  24% (69 of 283) |#     | Elapsed Time: 0:00:26 ETA:   0:00:43\n",
            "Running network:  24% (70 of 283) |#     | Elapsed Time: 0:00:26 ETA:   0:00:35\n",
            "Running network:  25% (71 of 283) |#     | Elapsed Time: 0:00:26 ETA:   0:00:35\n",
            "Running network:  25% (72 of 283) |#     | Elapsed Time: 0:00:26 ETA:   0:00:32\n",
            "Running network:  25% (73 of 283) |#     | Elapsed Time: 0:00:26 ETA:   0:00:35\n",
            "Running network:  26% (74 of 283) |#     | Elapsed Time: 0:00:26 ETA:   0:00:34\n",
            "Running network:  26% (75 of 283) |#     | Elapsed Time: 0:00:27 ETA:   0:00:38\n",
            "Running network:  26% (76 of 283) |#     | Elapsed Time: 0:00:27 ETA:   0:00:38\n",
            "Running network:  27% (77 of 283) |#     | Elapsed Time: 0:00:27 ETA:   0:00:41\n",
            "Running network:  27% (78 of 283) |#     | Elapsed Time: 0:00:27 ETA:   0:00:41\n",
            "Running network:  27% (79 of 283) |#     | Elapsed Time: 0:00:28 ETA:   0:00:45\n",
            "Running network:  28% (80 of 283) |#     | Elapsed Time: 0:00:28 ETA:   0:00:46\n",
            "Running network:  28% (81 of 283) |#     | Elapsed Time: 0:00:28 ETA:   0:00:48\n",
            "Running network:  28% (82 of 283) |#     | Elapsed Time: 0:00:28 ETA:   0:00:45\n",
            "Running network:  29% (83 of 283) |#     | Elapsed Time: 0:00:28 ETA:   0:00:45\n",
            "Running network:  29% (84 of 283) |#     | Elapsed Time: 0:00:29 ETA:   0:00:44\n",
            "Running network:  30% (85 of 283) |#     | Elapsed Time: 0:00:29 ETA:   0:00:42\n",
            "Running network:  30% (86 of 283) |#     | Elapsed Time: 0:00:29 ETA:   0:00:40\n",
            "Running network:  30% (87 of 283) |#     | Elapsed Time: 0:00:29 ETA:   0:00:41\n",
            "Running network:  31% (88 of 283) |#     | Elapsed Time: 0:00:30 ETA:   0:00:42\n",
            "Running network:  31% (89 of 283) |#     | Elapsed Time: 0:00:30 ETA:   0:00:40\n",
            "Running network:  31% (90 of 283) |#     | Elapsed Time: 0:00:30 ETA:   0:00:39\n",
            "Running network:  32% (91 of 283) |#     | Elapsed Time: 0:00:30 ETA:   0:00:38\n",
            "Running network:  32% (92 of 283) |#     | Elapsed Time: 0:00:30 ETA:   0:00:36\n",
            "Running network:  32% (93 of 283) |#     | Elapsed Time: 0:00:30 ETA:   0:00:35\n",
            "Running network:  33% (94 of 283) |#     | Elapsed Time: 0:00:31 ETA:   0:00:36\n",
            "Running network:  33% (95 of 283) |##    | Elapsed Time: 0:00:31 ETA:   0:00:37\n",
            "Running network:  33% (96 of 283) |##    | Elapsed Time: 0:00:31 ETA:   0:00:37\n",
            "Running network:  34% (97 of 283) |##    | Elapsed Time: 0:00:31 ETA:   0:00:37\n",
            "Running network:  34% (98 of 283) |##    | Elapsed Time: 0:00:32 ETA:   0:00:35\n",
            "Running network:  34% (99 of 283) |##    | Elapsed Time: 0:00:32 ETA:   0:00:38\n",
            "Running network:  35% (100 of 283) |#    | Elapsed Time: 0:00:32 ETA:   0:00:46\n",
            "Running network:  35% (101 of 283) |#    | Elapsed Time: 0:00:32 ETA:   0:00:46\n",
            "Running network:  36% (102 of 283) |#    | Elapsed Time: 0:00:33 ETA:   0:00:46\n",
            "Running network:  36% (103 of 283) |#    | Elapsed Time: 0:00:33 ETA:   0:00:44\n",
            "Running network:  36% (104 of 283) |#    | Elapsed Time: 0:00:33 ETA:   0:00:41\n",
            "Running network:  37% (105 of 283) |#    | Elapsed Time: 0:00:33 ETA:   0:00:41\n",
            "Running network:  37% (106 of 283) |#    | Elapsed Time: 0:00:34 ETA:   0:00:46\n",
            "Running network:  37% (107 of 283) |#    | Elapsed Time: 0:00:34 ETA:   0:00:43\n",
            "Running network:  38% (108 of 283) |#    | Elapsed Time: 0:00:34 ETA:   0:00:42\n",
            "Running network:  38% (110 of 283) |#    | Elapsed Time: 0:00:34 ETA:   0:00:35\n",
            "Running network:  39% (111 of 283) |#    | Elapsed Time: 0:00:35 ETA:   0:00:36\n",
            "Running network:  39% (112 of 283) |#    | Elapsed Time: 0:00:35 ETA:   0:00:38\n",
            "Running network:  39% (113 of 283) |#    | Elapsed Time: 0:00:35 ETA:   0:00:36\n",
            "Running network:  40% (114 of 283) |##   | Elapsed Time: 0:00:35 ETA:   0:00:36\n",
            "Running network:  40% (115 of 283) |##   | Elapsed Time: 0:00:36 ETA:   0:00:39\n",
            "Running network:  40% (116 of 283) |##   | Elapsed Time: 0:00:36 ETA:   0:00:33\n",
            "Running network:  41% (117 of 283) |##   | Elapsed Time: 0:00:36 ETA:   0:00:33\n",
            "Running network:  41% (118 of 283) |##   | Elapsed Time: 0:00:36 ETA:   0:00:34\n",
            "Running network:  42% (119 of 283) |##   | Elapsed Time: 0:00:36 ETA:   0:00:33\n",
            "Running network:  42% (120 of 283) |##   | Elapsed Time: 0:00:36 ETA:   0:00:33\n",
            "Running network:  42% (121 of 283) |##   | Elapsed Time: 0:00:36 ETA:   0:00:30\n",
            "Running network:  43% (122 of 283) |##   | Elapsed Time: 0:00:37 ETA:   0:00:32\n",
            "Running network:  43% (123 of 283) |##   | Elapsed Time: 0:00:37 ETA:   0:00:33\n",
            "Running network:  43% (124 of 283) |##   | Elapsed Time: 0:00:37 ETA:   0:00:33\n",
            "Running network:  44% (125 of 283) |##   | Elapsed Time: 0:00:37 ETA:   0:00:32\n",
            "Running network:  44% (126 of 283) |##   | Elapsed Time: 0:00:38 ETA:   0:00:35\n",
            "Running network:  44% (127 of 283) |##   | Elapsed Time: 0:00:38 ETA:   0:00:33\n",
            "Running network:  45% (128 of 283) |##   | Elapsed Time: 0:00:38 ETA:   0:00:32\n",
            "Running network:  45% (129 of 283) |##   | Elapsed Time: 0:00:38 ETA:   0:00:32\n",
            "Running network:  45% (130 of 283) |##   | Elapsed Time: 0:00:39 ETA:   0:00:37\n",
            "Running network:  46% (131 of 283) |##   | Elapsed Time: 0:00:39 ETA:   0:00:34\n",
            "Running network:  46% (132 of 283) |##   | Elapsed Time: 0:00:39 ETA:   0:00:33\n",
            "Running network:  46% (133 of 283) |##   | Elapsed Time: 0:00:39 ETA:   0:00:35\n",
            "Running network:  47% (134 of 283) |##   | Elapsed Time: 0:00:40 ETA:   0:00:35\n",
            "Running network:  47% (135 of 283) |##   | Elapsed Time: 0:00:40 ETA:   0:00:33\n",
            "Running network:  48% (136 of 283) |##   | Elapsed Time: 0:00:40 ETA:   0:00:32\n",
            "Running network:  48% (137 of 283) |##   | Elapsed Time: 0:00:40 ETA:   0:00:29\n",
            "Running network:  48% (138 of 283) |##   | Elapsed Time: 0:00:40 ETA:   0:00:29\n",
            "Running network:  49% (139 of 283) |##   | Elapsed Time: 0:00:40 ETA:   0:00:28\n",
            "Running network:  49% (140 of 283) |##   | Elapsed Time: 0:00:41 ETA:   0:00:29\n",
            "Running network:  49% (141 of 283) |##   | Elapsed Time: 0:00:41 ETA:   0:00:26\n",
            "Running network:  50% (142 of 283) |##   | Elapsed Time: 0:00:41 ETA:   0:00:29\n",
            "Running network:  50% (143 of 283) |##   | Elapsed Time: 0:00:41 ETA:   0:00:29\n",
            "Running network:  50% (144 of 283) |##   | Elapsed Time: 0:00:42 ETA:   0:00:31\n",
            "Running network:  51% (145 of 283) |##   | Elapsed Time: 0:00:42 ETA:   0:00:31\n",
            "Running network:  51% (146 of 283) |##   | Elapsed Time: 0:00:42 ETA:   0:00:32\n",
            "Running network:  51% (147 of 283) |##   | Elapsed Time: 0:00:42 ETA:   0:00:31\n",
            "Running network:  52% (148 of 283) |##   | Elapsed Time: 0:00:42 ETA:   0:00:31\n",
            "Running network:  52% (149 of 283) |##   | Elapsed Time: 0:00:42 ETA:   0:00:29\n",
            "Running network:  53% (150 of 283) |##   | Elapsed Time: 0:00:43 ETA:   0:00:26\n",
            "Running network:  53% (151 of 283) |##   | Elapsed Time: 0:00:43 ETA:   0:00:26\n",
            "Running network:  53% (152 of 283) |##   | Elapsed Time: 0:00:43 ETA:   0:00:25\n",
            "Running network:  54% (153 of 283) |##   | Elapsed Time: 0:00:43 ETA:   0:00:24\n",
            "Running network:  54% (154 of 283) |##   | Elapsed Time: 0:00:43 ETA:   0:00:22\n",
            "Running network:  54% (155 of 283) |##   | Elapsed Time: 0:00:43 ETA:   0:00:22\n",
            "Running network:  55% (156 of 283) |##   | Elapsed Time: 0:00:44 ETA:   0:00:21\n",
            "Running network:  55% (157 of 283) |##   | Elapsed Time: 0:00:44 ETA:   0:00:20\n",
            "Running network:  55% (158 of 283) |##   | Elapsed Time: 0:00:44 ETA:   0:00:20\n",
            "Running network:  56% (159 of 283) |##   | Elapsed Time: 0:00:44 ETA:   0:00:19\n",
            "Running network:  56% (160 of 283) |##   | Elapsed Time: 0:00:44 ETA:   0:00:19\n",
            "Running network:  56% (161 of 283) |##   | Elapsed Time: 0:00:45 ETA:   0:00:21\n",
            "Running network:  57% (162 of 283) |##   | Elapsed Time: 0:00:45 ETA:   0:00:21\n",
            "Running network:  57% (163 of 283) |##   | Elapsed Time: 0:00:45 ETA:   0:00:22\n",
            "Running network:  57% (164 of 283) |##   | Elapsed Time: 0:00:45 ETA:   0:00:25\n",
            "Running network:  58% (165 of 283) |##   | Elapsed Time: 0:00:45 ETA:   0:00:25\n",
            "Running network:  58% (166 of 283) |##   | Elapsed Time: 0:00:46 ETA:   0:00:24\n",
            "Running network:  59% (167 of 283) |##   | Elapsed Time: 0:00:46 ETA:   0:00:23\n",
            "Running network:  59% (168 of 283) |##   | Elapsed Time: 0:00:46 ETA:   0:00:23\n",
            "Running network:  59% (169 of 283) |##   | Elapsed Time: 0:00:46 ETA:   0:00:23\n",
            "Running network:  60% (170 of 283) |###  | Elapsed Time: 0:00:46 ETA:   0:00:23\n",
            "Running network:  60% (171 of 283) |###  | Elapsed Time: 0:00:46 ETA:   0:00:22\n",
            "Running network:  60% (172 of 283) |###  | Elapsed Time: 0:00:47 ETA:   0:00:20\n",
            "Running network:  61% (173 of 283) |###  | Elapsed Time: 0:00:47 ETA:   0:00:20\n",
            "Running network:  61% (174 of 283) |###  | Elapsed Time: 0:00:47 ETA:   0:00:20\n",
            "Running network:  61% (175 of 283) |###  | Elapsed Time: 0:00:47 ETA:   0:00:19\n",
            "Running network:  62% (176 of 283) |###  | Elapsed Time: 0:00:47 ETA:   0:00:19\n",
            "Running network:  62% (177 of 283) |###  | Elapsed Time: 0:00:47 ETA:   0:00:17\n",
            "Running network:  62% (178 of 283) |###  | Elapsed Time: 0:00:48 ETA:   0:00:18\n",
            "Running network:  63% (179 of 283) |###  | Elapsed Time: 0:00:48 ETA:   0:00:18\n",
            "Running network:  63% (180 of 283) |###  | Elapsed Time: 0:00:48 ETA:   0:00:19\n",
            "Running network:  63% (181 of 283) |###  | Elapsed Time: 0:00:48 ETA:   0:00:18\n",
            "Running network:  64% (182 of 283) |###  | Elapsed Time: 0:00:49 ETA:   0:00:20\n",
            "Running network:  64% (183 of 283) |###  | Elapsed Time: 0:00:49 ETA:   0:00:20\n",
            "Running network:  65% (184 of 283) |###  | Elapsed Time: 0:00:49 ETA:   0:00:20\n",
            "Running network:  65% (185 of 283) |###  | Elapsed Time: 0:00:49 ETA:   0:00:20\n",
            "Running network:  66% (187 of 283) |###  | Elapsed Time: 0:00:50 ETA:   0:00:20\n",
            "Running network:  66% (188 of 283) |###  | Elapsed Time: 0:00:50 ETA:   0:00:19\n",
            "Running network:  66% (189 of 283) |###  | Elapsed Time: 0:00:50 ETA:   0:00:18\n",
            "Running network:  67% (190 of 283) |###  | Elapsed Time: 0:00:50 ETA:   0:00:17\n",
            "Running network:  67% (191 of 283) |###  | Elapsed Time: 0:00:50 ETA:   0:00:17\n",
            "Running network:  67% (192 of 283) |###  | Elapsed Time: 0:00:50 ETA:   0:00:17\n",
            "Running network:  68% (193 of 283) |###  | Elapsed Time: 0:00:51 ETA:   0:00:16\n",
            "Running network:  68% (194 of 283) |###  | Elapsed Time: 0:00:51 ETA:   0:00:16\n",
            "Running network:  68% (195 of 283) |###  | Elapsed Time: 0:00:51 ETA:   0:00:15\n",
            "Running network:  69% (196 of 283) |###  | Elapsed Time: 0:00:51 ETA:   0:00:15\n",
            "Running network:  69% (197 of 283) |###  | Elapsed Time: 0:00:51 ETA:   0:00:15\n",
            "Running network:  69% (198 of 283) |###  | Elapsed Time: 0:00:52 ETA:   0:00:15\n",
            "Running network:  70% (200 of 283) |###  | Elapsed Time: 0:00:52 ETA:   0:00:14\n",
            "Running network:  71% (201 of 283) |###  | Elapsed Time: 0:00:52 ETA:   0:00:13\n",
            "Running network:  71% (202 of 283) |###  | Elapsed Time: 0:00:52 ETA:   0:00:13\n",
            "Running network:  72% (204 of 283) |###  | Elapsed Time: 0:00:52 ETA:   0:00:12\n",
            "Running network:  72% (205 of 283) |###  | Elapsed Time: 0:00:52 ETA:   0:00:11\n",
            "Running network:  72% (206 of 283) |###  | Elapsed Time: 0:00:53 ETA:   0:00:11\n",
            "Running network:  73% (207 of 283) |###  | Elapsed Time: 0:00:53 ETA:   0:00:11\n",
            "Running network:  73% (208 of 283) |###  | Elapsed Time: 0:00:53 ETA:   0:00:11\n",
            "Running network:  73% (209 of 283) |###  | Elapsed Time: 0:00:53 ETA:   0:00:10\n",
            "Running network:  74% (210 of 283) |###  | Elapsed Time: 0:00:53 ETA:   0:00:10\n",
            "Running network:  74% (211 of 283) |###  | Elapsed Time: 0:00:53 ETA:   0:00:10\n",
            "Running network:  74% (212 of 283) |###  | Elapsed Time: 0:00:54 ETA:   0:00:10\n",
            "Running network:  75% (213 of 283) |###  | Elapsed Time: 0:00:54 ETA:   0:00:09\n",
            "Running network:  75% (214 of 283) |###  | Elapsed Time: 0:00:54 ETA:   0:00:09\n",
            "Running network:  75% (215 of 283) |###  | Elapsed Time: 0:00:54 ETA:   0:00:10\n",
            "Running network:  76% (216 of 283) |###  | Elapsed Time: 0:00:54 ETA:   0:00:11\n",
            "Running network:  76% (217 of 283) |###  | Elapsed Time: 0:00:54 ETA:   0:00:11\n",
            "Running network:  77% (218 of 283) |###  | Elapsed Time: 0:00:55 ETA:   0:00:11\n",
            "Running network:  77% (219 of 283) |###  | Elapsed Time: 0:00:55 ETA:   0:00:11\n",
            "Running network:  77% (220 of 283) |###  | Elapsed Time: 0:00:55 ETA:   0:00:12\n",
            "Running network:  78% (221 of 283) |###  | Elapsed Time: 0:00:55 ETA:   0:00:11\n",
            "Running network:  78% (222 of 283) |###  | Elapsed Time: 0:00:55 ETA:   0:00:10\n",
            "Running network:  78% (223 of 283) |###  | Elapsed Time: 0:00:55 ETA:   0:00:10\n",
            "Running network:  79% (225 of 283) |###  | Elapsed Time: 0:00:56 ETA:   0:00:09\n",
            "Running network:  79% (226 of 283) |###  | Elapsed Time: 0:00:56 ETA:   0:00:09\n",
            "Running network:  80% (227 of 283) |#### | Elapsed Time: 0:00:56 ETA:   0:00:09\n",
            "Running network:  80% (228 of 283) |#### | Elapsed Time: 0:00:56 ETA:   0:00:08\n",
            "Running network:  80% (229 of 283) |#### | Elapsed Time: 0:00:56 ETA:   0:00:08\n",
            "Running network:  81% (230 of 283) |#### | Elapsed Time: 0:00:56 ETA:   0:00:08\n",
            "Running network:  81% (232 of 283) |#### | Elapsed Time: 0:00:57 ETA:   0:00:07\n",
            "Running network:  82% (233 of 283) |#### | Elapsed Time: 0:00:57 ETA:   0:00:07\n",
            "Running network:  82% (234 of 283) |#### | Elapsed Time: 0:00:57 ETA:   0:00:06\n",
            "Running network:  83% (235 of 283) |#### | Elapsed Time: 0:00:57 ETA:   0:00:06\n",
            "Running network:  83% (236 of 283) |#### | Elapsed Time: 0:00:57 ETA:   0:00:06\n",
            "Running network:  83% (237 of 283) |#### | Elapsed Time: 0:00:57 ETA:   0:00:06\n",
            "Running network:  84% (238 of 283) |#### | Elapsed Time: 0:00:57 ETA:   0:00:06\n",
            "Running network:  84% (239 of 283) |#### | Elapsed Time: 0:00:58 ETA:   0:00:05\n",
            "Running network:  84% (240 of 283) |#### | Elapsed Time: 0:00:58 ETA:   0:00:06\n",
            "Running network:  85% (241 of 283) |#### | Elapsed Time: 0:00:58 ETA:   0:00:06\n",
            "Running network:  85% (242 of 283) |#### | Elapsed Time: 0:00:58 ETA:   0:00:05\n",
            "Running network:  85% (243 of 283) |#### | Elapsed Time: 0:00:58 ETA:   0:00:05\n",
            "Running network:  86% (244 of 283) |#### | Elapsed Time: 0:00:58 ETA:   0:00:05\n",
            "Running network:  86% (245 of 283) |#### | Elapsed Time: 0:00:59 ETA:   0:00:05\n",
            "Running network:  86% (246 of 283) |#### | Elapsed Time: 0:00:59 ETA:   0:00:05\n",
            "Running network:  87% (247 of 283) |#### | Elapsed Time: 0:00:59 ETA:   0:00:05\n",
            "Running network:  87% (248 of 283) |#### | Elapsed Time: 0:00:59 ETA:   0:00:05\n",
            "Running network:  87% (249 of 283) |#### | Elapsed Time: 0:00:59 ETA:   0:00:04\n",
            "Running network:  88% (250 of 283) |#### | Elapsed Time: 0:00:59 ETA:   0:00:04\n",
            "Running network:  88% (251 of 283) |#### | Elapsed Time: 0:01:00 ETA:   0:00:05\n",
            "Running network:  89% (252 of 283) |#### | Elapsed Time: 0:01:00 ETA:   0:00:04\n",
            "Running network:  89% (253 of 283) |#### | Elapsed Time: 0:01:00 ETA:   0:00:04\n",
            "Running network:  89% (254 of 283) |#### | Elapsed Time: 0:01:00 ETA:   0:00:04\n",
            "Running network:  90% (255 of 283) |#### | Elapsed Time: 0:01:00 ETA:   0:00:05\n",
            "Running network:  90% (256 of 283) |#### | Elapsed Time: 0:01:01 ETA:   0:00:05\n",
            "Running network:  90% (257 of 283) |#### | Elapsed Time: 0:01:01 ETA:   0:00:04\n",
            "Running network:  91% (258 of 283) |#### | Elapsed Time: 0:01:01 ETA:   0:00:05\n",
            "Running network:  91% (259 of 283) |#### | Elapsed Time: 0:01:01 ETA:   0:00:04\n",
            "Running network:  91% (260 of 283) |#### | Elapsed Time: 0:01:01 ETA:   0:00:04\n",
            "Running network:  92% (261 of 283) |#### | Elapsed Time: 0:01:01 ETA:   0:00:04\n",
            "Running network:  92% (262 of 283) |#### | Elapsed Time: 0:01:01 ETA:   0:00:03\n",
            "Running network:  92% (263 of 283) |#### | Elapsed Time: 0:01:02 ETA:   0:00:03\n",
            "Running network:  93% (264 of 283) |#### | Elapsed Time: 0:01:02 ETA:   0:00:03\n",
            "Running network:  93% (265 of 283) |#### | Elapsed Time: 0:01:02 ETA:   0:00:03\n",
            "Running network:  93% (266 of 283) |#### | Elapsed Time: 0:01:02 ETA:   0:00:03\n",
            "Running network:  94% (267 of 283) |#### | Elapsed Time: 0:01:02 ETA:   0:00:02\n",
            "Running network:  94% (268 of 283) |#### | Elapsed Time: 0:01:03 ETA:   0:00:02\n",
            "Running network:  95% (269 of 283) |#### | Elapsed Time: 0:01:03 ETA:   0:00:02\n",
            "Running network:  95% (270 of 283) |#### | Elapsed Time: 0:01:03 ETA:   0:00:02\n",
            "Running network:  95% (271 of 283) |#### | Elapsed Time: 0:01:03 ETA:   0:00:01\n",
            "Running network:  96% (272 of 283) |#### | Elapsed Time: 0:01:03 ETA:   0:00:01\n",
            "Running network:  96% (273 of 283) |#### | Elapsed Time: 0:01:03 ETA:   0:00:01\n",
            "Running network:  96% (274 of 283) |#### | Elapsed Time: 0:01:03 ETA:   0:00:01\n",
            "Running network:  97% (275 of 283) |#### | Elapsed Time: 0:01:03 ETA:   0:00:01\n",
            "Running network:  97% (276 of 283) |#### | Elapsed Time: 0:01:04 ETA:   0:00:01\n",
            "Running network:  97% (277 of 283) |#### | Elapsed Time: 0:01:04 ETA:   0:00:00\n",
            "Running network:  98% (278 of 283) |#### | Elapsed Time: 0:01:04 ETA:   0:00:00\n",
            "Running network:  98% (279 of 283) |#### | Elapsed Time: 0:01:04 ETA:   0:00:00\n",
            "Running network:  98% (280 of 283) |#### | Elapsed Time: 0:01:04 ETA:   0:00:00\n",
            "Running network:  99% (281 of 283) |#### | Elapsed Time: 0:01:05 ETA:   0:00:00\n",
            "Running network:  99% (282 of 283) |#### | Elapsed Time: 0:01:05 ETA:   0:00:00\n",
            "Running network: 100% (283 of 283) |#####| Elapsed Time: 0:01:05 Time:  0:01:05\n",
            "Parsing annotations:   0% (0 of 283) |   | Elapsed Time: 0:00:00 ETA:  --:--:--\n",
            "Parsing annotations: 100% (283 of 283) |#| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ]
        }
      ],
      "source": [
        "#Convert training model to inference model\n",
        "!python keras_retinanet/bin/convert_model.py {model_path} {inference_model_path}\n",
        "\n",
        "# For cv::Mat error\n",
        "visualise_path = \"../filesToReplace/visualization.py\"\n",
        "visualise_copyTo_path= \"keras_retinanet/utils/visualization.py\"\n",
        "\n",
        "copy_and_replace(visualise_path, visualise_copyTo_path)\n",
        "\n",
        "#Evaluate\n",
        "!python keras_retinanet/bin/evaluate.py --gpu 0 --score-threshold 0.0 --save-path {save_path} --no-resize\\\n",
        "        csv {test_annotations_path} {classes_path} {inference_model_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "c5a31cea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading [average_precisions, inference_time, precision_recall] from numpy evaluation_results.npy file\n",
        "evaluation_results = np.load(os.path.join(HOME, 'evaluation_results.npy'), allow_pickle=True)\n",
        "\n",
        "average_precisions = evaluation_results[0]\n",
        "inference_time     = evaluation_results[1]\n",
        "precision_recall   = evaluation_results[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "6d1d1687",
      "metadata": {},
      "outputs": [],
      "source": [
        "# average_precisions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f06dc928",
      "metadata": {},
      "outputs": [],
      "source": [
        "# precision_recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d6e7c563",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[35mAverage Precisions for each class:\u001b[0m\n",
            "Arugula        : 0.005016673916195447\n",
            "Bacon          : 0.0\n",
            "Basil          : 0.23183565521104982\n",
            "Broccoli       : 0.05376344086021505\n",
            "Cheese         : 0.061468365656653756\n",
            "Chicken        : 0.00018422991893883567\n",
            "Corn           : 0.0\n",
            "Ham            : 0.0065836776025293055\n",
            "Mushroom       : 0.027279908060418075\n",
            "Olives         : 0.2463227237639064\n",
            "Onion          : 0.0\n",
            "Pepperoni      : 0.41809744435160456\n",
            "Peppers        : 0.047394101739777386\n",
            "Pineapple      : 0.0\n",
            "Pizza          : 0.9186188208854265\n",
            "Tomatoes       : 0.14792537137538855\n",
            "\n",
            "\u001b[37mMean Average Precision (mAP):\u001b[0m\n",
            "0.1352806508338815\n"
          ]
        }
      ],
      "source": [
        "# Retrieving the average precisions for each class from the average_precisions dictionary first element in the average precisions dictionary items\n",
        "aps = [average_precision[0] for label, average_precision in average_precisions.items()]\n",
        "\n",
        "print('\\033[35m' + 'Average Precisions for each class:' + '\\033[0m')\n",
        "for idx, ap in enumerate(aps):\n",
        "    print(f'{classes[idx]:15}: {ap}')\n",
        "\n",
        "# Mean Average Precision\n",
        "mean_average_precision = np.mean(aps)\n",
        "print(f'\\n\\033[37m' + 'Mean Average Precision (mAP):' + '\\033[0m')\n",
        "print(f'{mean_average_precision}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27c44280",
      "metadata": {},
      "source": [
        "**<h3>Testing Results.</h3>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "38fb61d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Construct the logdir path\n",
        "logdir = os.path.join(HOME, 'tensorboard')\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir={logdir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "3f4ed726",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "plt.bar(list(average_precisions.keys()), list(aps), color='blue')\n",
        "\n",
        "# Putting text on the bar chart\n",
        "for idx, val in enumerate(aps):\n",
        "    plt.text(idx, val, round(val, 3), horizontalalignment='center', verticalalignment='bottom', fontdict={'fontweight': 500, 'size': 12})\n",
        "\n",
        "# Set the x-ticks to correspond to all labels\n",
        "plt.gca().set_xticks(range(len(classes)))\n",
        "\n",
        "plt.xticks(rotation=45, fontsize=12)\n",
        "# Set the labels to the actual class labels\n",
        "plt.gca().set_xticklabels(classes)\n",
        "plt.xlabel(\"Labels\", fontsize=14)\n",
        "plt.ylabel(\"Mean Average Precision\", fontsize=14)\n",
        "plt.title(\"Mean Average Precision vs Labels (Keras RetinaNet)\", fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "a5747007",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plotting precision and recall curve for each class\n",
        "plt.figure(figsize=(20, 10))\n",
        "for label, (precision_list, recall_list) in precision_recall.items():\n",
        "    plt.plot(recall_list, precision_list, label=classes[label] + \" (AP: {:.4f})\".format(aps[label]))\n",
        "\n",
        "plt.xlabel(\"Recall\", fontsize=14)\n",
        "plt.ylabel(\"Precision\", fontsize=14)\n",
        "plt.title(\"Precision vs Recall Curve (Keras RetinaNet)\", fontsize=20)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "1b24161f71c97a6e1337e708242a5db52c0e6b5491719b2b3b399069e13d6b51"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
